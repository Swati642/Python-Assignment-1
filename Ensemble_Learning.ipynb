{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWuBKjPSx9+w4kEicn9eBC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems\n",
        "Yes, Bagging can be used for regression problems. It trains multiple models on bootstrapped subsets of the data and combines their predictions, usually by averaging them. It's effective in reducing variance and preventing overfitting, especially with high-variance models like decision trees.\n",
        "\n",
        "Example: BaggingRegressor in scikit-learn with decision trees as base models."
      ],
      "metadata": {
        "id": "ShxgM9I-eOFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between multiple model training and single model training\n",
        "**Multiple Model Training** involves training several models, either of the same or different types, and combining their predictions to improve performance (e.g., through methods like Bagging, Boosting, or Stacking).\n",
        "\n",
        "**Single Model Training** involves training one model on the entire dataset and using it for prediction.\n",
        "\n",
        "**Key Differences**:\n",
        "- **Multiple Models**: Reduces overfitting, improves generalization, and increases robustness.\n",
        "- **Single Model**: Simpler, faster, but may overfit or underfit depending on the model complexity."
      ],
      "metadata": {
        "id": "u154p5DnipA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest\n",
        "Feature Randomness in Random Forest refers to the random selection of a subset of features at each split while building individual decision trees. This helps to create diverse trees, reducing the correlation between them and improving the model’s overall performance by preventing overfitting. It also enhances the model's ability to generalize."
      ],
      "metadata": {
        "id": "CsEJgq7tiyQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score\n",
        "**OOB (Out-of-Bag) Score** is an internal cross-validation method used in Random Forest. It evaluates the model’s performance without the need for a separate validation set.\n",
        "\n",
        "it's working:\n",
        "- For each tree in the forest, the data points not selected for training (out-of-bag samples) are used to test the tree.\n",
        "- The OOB score is the average accuracy (or error) across all trees, based on their corresponding out-of-bag samples.\n",
        "\n",
        "It provides a way to estimate the model's performance without the need for cross-validation or a test set."
      ],
      "metadata": {
        "id": "xjmR3ewVjACh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can you measure the importance of features in a Random Forest model\n",
        "Feature importance in a Random Forest model can be measured using the following methods:\n",
        "\n",
        "1. **Gini Importance (or Mean Decrease Impurity)**:\n",
        "   - It measures the total reduction in impurity (like Gini index or entropy) contributed by a feature when it is used in a decision tree node.\n",
        "   - Features that result in larger reductions in impurity are considered more important.\n",
        "\n",
        "2. **Permutation Importance**:\n",
        "   - It measures the change in model performance (e.g., accuracy or MSE) when the values of a feature are randomly shuffled.\n",
        "   - A large decrease in performance after shuffling indicates a high importance of that feature.\n",
        "\n",
        "Both methods can be used in Random Forest, and scikit-learn provides built-in functions like `feature_importances_` to easily access the importance scores of features."
      ],
      "metadata": {
        "id": "DIilEmHHjPkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier\n",
        "A **Bagging Classifier** works by combining the predictions of multiple base models (usually decision trees) to improve overall accuracy and reduce variance. Here's how it works:\n",
        "\n",
        "1. **Data Sampling**: Random subsets of the training data are created by bootstrapping (sampling with replacement). Each subset has the same number of samples as the original data but some samples may appear multiple times, and others may not appear at all.\n",
        "\n",
        "2. **Model Training**: A separate classifier is trained on each subset of data. Each model might overfit to its particular subset, but the aggregation will reduce this.\n",
        "\n",
        "3. **Prediction**: For classification tasks, the final prediction is made by **voting** (majority class). For regression tasks, the final prediction is made by averaging the predictions of all models.\n",
        "\n",
        "Bagging helps reduce overfitting by averaging out errors, and it works especially well with high-variance models like decision trees."
      ],
      "metadata": {
        "id": "bytBcp5pjXLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifier’s performance\n",
        "To evaluate a **Bagging Classifier's** performance, you can use several metrics and techniques:\n",
        "\n",
        "1. **Accuracy**: Calculate the proportion of correctly classified instances.\n",
        "\n",
        "2. **Confusion Matrix**: Helps evaluate the true positives, true negatives, false positives, and false negatives, giving insight into classification performance.\n",
        "\n",
        "3. **Precision, Recall, F1-Score**: Measure the classifier's ability to correctly identify positive and negative instances, especially useful for imbalanced datasets.\n",
        "\n",
        "4. **ROC-AUC Curve**: For binary classification, it shows the trade-off between sensitivity (recall) and specificity, with the area under the curve indicating model performance.\n",
        "\n",
        "5. **Cross-Validation**: Use k-fold cross-validation to estimate the model's performance on unseen data. This helps reduce the variance caused by a single train-test split.\n",
        "\n",
        "6. **Out-of-Bag (OOB) Score**: For Bagging, the OOB score gives an unbiased estimate of the model's accuracy without needing a separate validation set. It uses the samples that were not selected during bootstrapping for testing.\n",
        "\n",
        "7. **Feature Importance**: Evaluate the importance of each feature in making predictions, especially when dealing with complex datasets."
      ],
      "metadata": {
        "id": "8CFW3_UXjmTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does a Bagging Regressor work\n",
        "A **Bagging Regressor** works by combining multiple regression models (often decision trees) to improve the overall performance and reduce overfitting. Here's how it works:\n",
        "\n",
        "1. **Bootstrapping**: The dataset is randomly sampled with replacement to create multiple subsets (called bootstrap samples).\n",
        "\n",
        "2. **Model Training**: For each bootstrap sample, a separate regression model (e.g., decision tree regressor) is trained independently.\n",
        "\n",
        "3. **Predictions**: When making predictions, each individual model provides a prediction, and the final output is obtained by averaging the predictions from all models.\n",
        "\n",
        "4. **Variance Reduction**: Since Bagging uses multiple models, it reduces the variance and overfitting that might occur in a single model, leading to better generalization on unseen data.\n",
        "\n",
        "Key Benefit: It helps in making the model more robust by reducing the effects of outliers and noise, especially in complex regression problems."
      ],
      "metadata": {
        "id": "rm5uwr-7jsQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques\n",
        "The main advantage of ensemble techniques is **improved accuracy and robustness**. By combining multiple models, ensemble methods reduce the likelihood of errors from individual models, leading to better generalization and higher performance. They also help in minimizing overfitting and variance, making the model more stable and reliable across different datasets."
      ],
      "metadata": {
        "id": "MXhfNdOakFC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods\n",
        "The main challenge of ensemble methods is **increased computational complexity**. They require more resources, as multiple models need to be trained, stored, and evaluated. This can lead to longer training times and higher memory usage. Additionally, combining models effectively and tuning hyperparameters can be more complex."
      ],
      "metadata": {
        "id": "D5DREgHtkN6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Explain the key idea behind ensemble techniques\n",
        "The key idea behind ensemble techniques is to **combine predictions from multiple models** to produce a more accurate and robust final prediction than any single model alone. This reduces variance (bagging), bias (boosting), or improves predictions (stacking)."
      ],
      "metadata": {
        "id": "62BP1vj4mRk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier\n",
        "A Random Forest Classifier is an **ensemble model** that builds multiple **decision trees** on random subsets of data and features, and predicts the **majority vote** (classification) from all trees. It improves accuracy and reduces overfitting."
      ],
      "metadata": {
        "id": "RDBRWLJGmYIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques\n",
        "A Random Forest Classifier is an **ensemble model** that builds multiple **decision trees** on random subsets of data and features, and predicts the **majority vote** (classification) from all trees. It improves accuracy and reduces overfitting."
      ],
      "metadata": {
        "id": "1Hh0jbPJmjSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning\n",
        "Ensemble learning is a machine learning technique where **multiple models (usually weak learners like decision trees)** are trained and combined to solve the same problem. The main goal is to **boost prediction accuracy, reduce overfitting, and improve model robustness**. Common ensemble methods include **Bagging (e.g., Random Forest), Boosting (e.g., AdaBoost, XGBoost)**, and **Stacking**. These techniques help leverage the strengths of individual models while minimizing their weaknesses.q"
      ],
      "metadata": {
        "id": "RUxyMEKomrK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods\n",
        "Avoid using ensemble methods when:\n",
        "\n",
        "1. **Simple models perform well** – No need to overcomplicate.\n",
        "2. **Interpretability is crucial** – Ensembles are like black boxes.\n",
        "3. **Limited resources** – They need more time and compute.\n",
        "4. **Small datasets** – Risk of overfitting increases.\n",
        "5. **Real-time prediction is required** – They’re slower to infer."
      ],
      "metadata": {
        "id": "b1ku4gcSm3TX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting\n",
        "Bagging reduces overfitting by:\n",
        "\n",
        "1. **Training on different subsets** (with replacement) — adds variety.\n",
        "2. **Reduces variance** — averages out the noise.\n",
        "3. **Prevents a single model from dominating** — ensemble smooths predictions."
      ],
      "metadata": {
        "id": "POza-uuInDUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree\n",
        "Random Forest is better than a single Decision Tree because:\n",
        "\n",
        "1. **Reduces overfitting** – combines many trees to generalize better.  \n",
        "2. **More accurate** – averaging reduces variance.  \n",
        "3. **Robust to noise/outliers** – less sensitive than a single tree.  \n",
        "4. **Handles missing data and imbalance** better."
      ],
      "metadata": {
        "id": "c2IW3rkanLJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Bootstrap sampling plays a key role in Bagging by:\n",
        "\n",
        "- **Creating diverse datasets**: Each model is trained on a random sample (with replacement) of the original data.  \n",
        "- **Reducing variance**: This diversity among models helps in averaging out errors.  \n",
        "- **Improving generalization**: Models see slightly different data, making the ensemble more robust."
      ],
      "metadata": {
        "id": "BhiCGOUuprdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What are some real-world applications of ensemble techniques\n",
        "Real-world applications of ensemble techniques:\n",
        "\n",
        "- **Finance**: Credit scoring, fraud detection  \n",
        "- **Healthcare**: Disease prediction, medical image analysis  \n",
        "- **Marketing**: Customer segmentation, churn prediction  \n",
        "- **E-commerce**: Recommendation systems, product ranking  \n",
        "- **Cybersecurity**: Intrusion detection, spam filtering  \n",
        "- **Weather**: Forecasting using multiple models for accuracy"
      ],
      "metadata": {
        "id": "_7Lxr-bOp1K4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the difference between Bagging and Boosting?\n",
        "Bagging trains multiple models in parallel on random subsets of data to reduce variance and improve stability. Boosting trains models sequentially, each focusing more on previous errors, to reduce bias and improve accuracy. Bagging gives equal weight to models; Boosting adjusts weights based on performance. Bagging (like Random Forest) is less prone to overfitting, while Boosting (like AdaBoost) can overfit if not tuned well."
      ],
      "metadata": {
        "id": "qeupqTwKqHHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n"
      ],
      "metadata": {
        "id": "e9gkhSI3qSBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "U1R46ur_qcBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. 2 Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "h5CvwahHqckp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "jrkmTAXyqkYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "scquU9Esqlcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "importances = model.feature_importances_\n",
        "for name, importance in zip(data.feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "OufUyv1_qpII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "RIt_hqt3qvVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "# Compare MSE\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Decision Tree MSE:\", dt_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "id": "F8NRoORbqzg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "u8jAGKqzq5w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest with OOB score\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB score\n",
        "print(\"OOB Score:\", rf.oob_score_)\n"
      ],
      "metadata": {
        "id": "-WpfzSrBq-hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "dpBleY2drCXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging with SVM\n",
        "model = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "bXneyp7XrFB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "8kgVXkberJjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of different numbers of trees to evaluate\n",
        "n_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n in n_trees:\n",
        "    # Train Random Forest Classifier with different numbers of trees\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Print accuracy for the current number of trees\n",
        "    print(f\"Accuracy with {n} trees: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "hLdKtaNcrMzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score2"
      ],
      "metadata": {
        "id": "9-BNojn5rQ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Use only two classes for binary classification\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model as base estimator\n",
        "base_estimator = LogisticRegression()\n",
        "\n",
        "# Train Bagging Classifier with Logistic Regression as base estimator\n",
        "bagging_model = BaggingClassifier(base_estimator=base_estimator, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = bagging_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print the AUC score\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(f\"AUC score: {auc_score}\")\n"
      ],
      "metadata": {
        "id": "4JV16xD-rTnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "X5xJPozdrhif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Boston housing dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf_regressor.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importance scores\n",
        "features = load_boston().feature_names\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance scores\n",
        "print(importance_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "gsPbh4qIrkKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy"
      ],
      "metadata": {
        "id": "5uhMh-2wrpyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train Bagging Classifier with Decision Trees as base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Initialize and train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Compare accuracy\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f'Bagging Classifier Accuracy: {bagging_accuracy:.4f}')\n",
        "print(f'Random Forest Classifier Accuracy: {rf_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "5fj5DCTurthR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.  Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "bL2aQJAkr0bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Best Hyperparameters: {best_params}')\n",
        "print(f'Accuracy of the tuned Random Forest model: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "nGVDLdMZr6JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance="
      ],
      "metadata": {
        "id": "p-Z1ieUPr_Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston housing dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize a list of different numbers of base estimators (n_estimators)\n",
        "n_estimators_list = [10, 50, 100]\n",
        "\n",
        "# Initialize a DecisionTreeRegressor as the base estimator for Bagging\n",
        "base_estimator = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Train Bagging Regressors with different numbers of base estimators\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=base_estimator, n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate using Mean Squared Error (MSE)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Store the result\n",
        "    results[n_estimators] = mse\n",
        "\n",
        "# Print the performance results for different numbers of base estimators\n",
        "for n_estimators, mse in results.items():\n",
        "    print(f'Number of Base Estimators: {n_estimators}, Mean Squared Error: {mse:.4f}')\n"
      ],
      "metadata": {
        "id": "8dBcmn4ysBdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_samples = X_test[y_test != y_pred]\n",
        "\n",
        "# Print the number of misclassified samples\n",
        "print(f\"Number of Misclassified Samples: {len(misclassified_samples)}\")\n",
        "\n",
        "# Optionally, you can also print some of the misclassified samples\n",
        "print(\"Misclassified Samples (First 5):\")\n",
        "print(misclassified_samples[:5])\n"
      ],
      "metadata": {
        "id": "h2Yqeya4sGDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tHadd2dCsOqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "aGAUsDf6sQZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier and Bagging Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=42)\n",
        "\n",
        "# Train the Decision Tree model\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the test set\n",
        "dt_pred = dt_classifier.predict(X_test)\n",
        "bagging_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of both models\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Print the accuracy of both models\n",
        "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "cpEB4Kc6sXY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "-b6vqsAnsc6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Malignant', 'Benign'], yticklabels=['Malignant', 'Benign'])\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "beCl3bZEshCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. = Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "uLzm6ll_sl0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize base models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(random_state=42)\n",
        "lr = LogisticRegression(random_state=42)\n",
        "\n",
        "# Initialize Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('dt', dt), ('svm', svm), ('lr', lr)],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_clf.fit(X_train\n"
      ],
      "metadata": {
        "id": "wj7o_CN1spyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "T9G--nPGsuup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display the features and their importance scores\n",
        "features = load_breast_cancer().feature_names\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the features by importance and print top 5 features\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(importance_df.head(5))\n"
      ],
      "metadata": {
        "id": "y5Ef05hjsyjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "H1yOtGM_s2Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Bagging Classifier with Decision Tree as base estimator\n",
        "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "# Evaluate the performance using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "fTtGoCkOs4qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "4z7yawrUs_nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of max_depth values to test\n",
        "max_depth_values = [None, 5, 10, 15, 20, 25, 30]\n",
        "accuracies = []\n",
        "\n",
        "# Train Random Forest Classifier with different max_depth values\n",
        "for max_depth in max_depth_values:\n",
        "    rf = RandomForestClassifier(max_depth=max_depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Plot the effect of max_depth on accuracy\n",
        "plt.plot(max_depth_values, accuracies, marker='o')\n",
        "plt.title('Effect of max_depth on Random Forest Accuracy')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print results\n",
        "for max_depth, accuracy in zip(max_depth_values, accuracies):\n",
        "    print(f\"max_depth = {max_depth}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "uHnuxJiUtCWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance="
      ],
      "metadata": {
        "id": "js0geyBktH93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "\n",
        "# Train Bagging Regressor using Decision Tree as base estimator\n",
        "bagging_dt = BaggingRegressor(base_estimator=dt_regressor, n_estimators=50, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Train Bagging Regressor using KNeighbors as base estimator\n",
        "bagging_knn = BaggingRegressor(base_estimator=knn_regressor, n_estimators=50, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error (Decision Tree): {mse_dt:.4f}\")\n",
        "print(f\"Mean Squared Error (KNeighbors): {mse_knn:.4f}\")\n",
        "\n",
        "# Plot comparison of MSE for both models\n",
        "models = ['Decision Tree', 'KNeighbors']\n",
        "mse_values = [mse_dt, mse_knn]\n",
        "\n",
        "plt.bar(models, mse_values, color=['blue', 'green'])\n",
        "plt.title('Bagging Regressor Performance Comparison')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "88kL2rmTtKnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "lUzl6cC5tRtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "\n",
        "# Train Bagging Regressor using Decision Tree as base estimator\n",
        "bagging_dt = BaggingRegressor(base_estimator=dt_regressor, n_estimators=50, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Train Bagging Regressor using KNeighbors as base estimator\n",
        "bagging_knn = BaggingRegressor(base_estimator=knn_regressor, n_estimators=50, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error (Decision Tree): {mse_dt:.4f}\")\n",
        "print(f\"Mean Squared Error (KNeighbors): {mse_knn:.4f}\")\n",
        "\n",
        "# Plot comparison of MSE for both models\n",
        "models = ['Decision Tree', 'KNeighbors']\n",
        "mse_values = [mse_dt, mse_knn]\n",
        "\n",
        "plt.bar(models, mse_values, color=['blue', 'green'])\n",
        "plt.title('Bagging Regressor Performance Comparison')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i7wVQOI7tZug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42.  Train a Bagging Classifier and evaluate its performance using cross-validatio."
      ],
      "metadata": {
        "id": "okv2uukZtd2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Initialize a Decision Tree Classifier as base estimator\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=dt_classifier, n_estimators=50, random_state=42)\n",
        "\n",
        "# Evaluate performance using cross-validation (5-fold)\n",
        "cv_scores = cross_val_score(bagging_classifier, X, y, cv=5)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(f\"Cross-validation scores: {cv_scores}\")\n",
        "print(f\"Mean cross-validation score: {np.mean(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "0tyEoPY1tm9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "vsHwjRLhtn9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Convert to binary classification for simplicity (class 0 vs. others)\n",
        "y_binary = label_binarize(y, classes=[0, 1, 2])[:, 0]  # Considering class 0 vs others\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = rf_classifier.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "\n",
        "# Compute precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oqIrlM5Dtspx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44.  Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "U-80JsjItyM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=200, random_state=42))\n",
        "]\n",
        "\n",
        "# Define stacking classifier with Logistic Regression as final estimator\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the trained model\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Train Random Forest and Logistic Regression separately\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "lr = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "RfEm-oWht4Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "-bsBmrmAt_KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston dataset\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Bagging Regressor model using Decision Tree as the base estimator\n",
        "base_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Initialize a list to store MSE scores for different bootstrap sample sizes\n",
        "bootstrap_sizes = [50, 100, 200, 300]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different levels of bootstrap samples\n",
        "for n_estimators in bootstrap_sizes:\n",
        "    bagging_regressor = BaggingRegressor(base_estimator=base_regressor, n_estimators=n_estimators, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate MSE\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"Bootstrap Size: {n_estimators}, MSE: {mse:.4f}\")\n",
        "\n",
        "# Compare the performance\n",
        "best_mse = min(mse_scores)\n",
        "best_n_estimators = bootstrap_sizes[mse_scores.index(best_mse)]\n",
        "print(f\"Best performance with {best_n_estimators} bootstrap samples, MSE: {best_mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "iyoziV_suDkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}