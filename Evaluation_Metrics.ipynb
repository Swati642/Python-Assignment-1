{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/Q647k945mIDfs4lXfe+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/Evaluation_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What does R-squared represent in a regression model\n",
        "R-squared (R²), also known as the coefficient of determination, is a statistical measure that tells you how well the independent variables explain the variance in the dependent variable in a regression model."
      ],
      "metadata": {
        "id": "4L5F7P4afbRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the assumptions of linear regression:\n",
        "a. Linearity\n",
        "b. Independence of Errors (No autocorrelation)\n",
        "c. Homoscedasticity (Constant variance of errors)\n",
        "d. No Multicollinearity\n",
        "e. Normality of Residuals\n",
        "f. No Endogeneity (Exogeneity of independent variables)\n"
      ],
      "metadata": {
        "id": "iB5LLHA6gjLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between R-squared and Adjusted R-squared\n",
        "a. R-squared shows the proportion of variance in the dependent variable that is explained by the model. It gives you a sense of how well your model fits the data overall.\n",
        "\n",
        "b. Adjusted R-squared does the same, but it goes one step further: it adjusts for the number of independent variables in your model. This makes it more reliable when you're working with multiple predictors.\n",
        "\n",
        "c. A major drawback of R-squared is that it never decreases when you add more variables to the model—even if those variables are useless. So, you could be tricked into thinking your model is better just because it has more features.\n",
        "\n",
        "d. Adjusted R-squared fixes this by introducing a penalty for adding predictors. If a new variable doesn’t actually improve the model, adjusted R-squared can go down.\n",
        "\n",
        "e. Because of this, R-squared is fine for a quick look at model performance, but adjusted R-squared is better when you're comparing models with different numbers of features or checking for overfitting."
      ],
      "metadata": {
        "id": "kTT6mh0hhCAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why do we use Mean Squared Error (MSE):\n",
        "We use Mean Squared Error (MSE) in regression tasks as a standard way to measure the accuracy of a model's predictions. It tells us how far off our predicted values are from the actual values — and it does this in a mathematically convenient way."
      ],
      "metadata": {
        "id": "MfRbrvdTip04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What does an Adjusted R-squared value of 0.85 indicate\n",
        "An Adjusted R-squared value of 0.85 means that 85% of the variability in the dependent variable is explained by your regression model, after adjusting for the number of predictors used."
      ],
      "metadata": {
        "id": "tMRUJuD6jCql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we check for normality of residuals in linear regression:\n",
        "Histogram of Residuals\n",
        "Plot the residuals using a histogram.\n",
        "\n",
        "If they look like a bell-shaped curve, that’s a good sign.\n",
        "\n",
        "Not perfect, but a quick visual check."
      ],
      "metadata": {
        "id": "3WL728i_lY_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(residuals, bins=30)\n",
        "plt.title(\"Histogram of Residuals\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iLQNMelflo6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is multicollinearity, and how does it impact regression:\n",
        "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other.\n",
        "\n",
        "impact on regreesion-\n",
        "Doesn’t hurt prediction accuracy as much, but:\n",
        "\n",
        "Makes coefficients unreliable\n",
        "\n",
        "Increases uncertainty\n",
        "\n",
        "Makes statistical testing misleading\n",
        "\n",
        "Reduces interpretability"
      ],
      "metadata": {
        "id": "NM8ah-UTlrd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Mean Absolute Error (MAE):\n",
        "Mean Absolute Error (MAE) is a metric used to measure the average magnitude of errors in a regression model's predictions. It tells you how far off your predictions are from the actual values, but unlike Mean Squared Error (MSE), it doesn’t square the errors, making it less sensitive to large deviations."
      ],
      "metadata": {
        "id": "hXPgEgkCmOIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the benefits of using an ML pipeline\n",
        "Benefits:\n",
        "Efficiency: Automates tasks, reduces errors, and speeds up experimentation.\n",
        "\n",
        "Reproducibility: Ensures consistency and transparency in your work.\n",
        "\n",
        "Scalability: Adapts easily to growing datasets and computational needs.\n",
        "\n",
        "Maintainability: Simplifies updates and adjustments.\n",
        "\n",
        "Collaboration: Makes it easier for teams to work together on the same process.\n",
        "\n"
      ],
      "metadata": {
        "id": "ETWMJPTZmea-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Why is RMSE considered more interpretable than MSE:\n",
        "Root Mean Squared Error (RMSE) is often considered more interpretable than Mean Squared Error (MSE) because it provides a metric in the same units as the target variable, making it easier to understand in the context of the problem you're solving.\n",
        "RMSE gives you a more interpretable measure because it’s in the same units as the target variable, while MSE gives you squared units, which are harder to interpret in real-world terms.\n",
        "\n",
        "RMSE provides an intuitive sense of how far off your predictions are from actual values, making it more practical for understanding model performance in everyday terms."
      ],
      "metadata": {
        "id": "dGF0iu07mr_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is pickling in Python, and how is it useful in ML:\n",
        "Pickling in Python means saving an object (like a trained ML model) into a file so you can reuse it later without retraining. It's useful in ML for storing models, sharing them, or deploying them easily."
      ],
      "metadata": {
        "id": "pn2FDjJgnIOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What does a high R-squared value mean:\n",
        "A high R-squared value means that the model explains a large portion of the variation in the target variable. It suggests a good fit between the model and the data."
      ],
      "metadata": {
        "id": "G5sYmfvBmVCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What happens if linear regression assumptions are violated:\n",
        "If assumptions are violated, the model’s predictions, coefficients, and statistical tests may become unreliable, leading to incorrect conclusions or poor performance."
      ],
      "metadata": {
        "id": "eicj2qepmeTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can we address multicollinearity in regression:\n",
        "We can address multicollinearity by removing highly correlated variables, combining them, or using regularization techniques like Ridge or Lasso regression."
      ],
      "metadata": {
        "id": "rCrlD9oEmj83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can feature selection improve model performance in regression analysis:\n",
        "Feature selection removes irrelevant or redundant variables, which helps reduce overfitting, improves model accuracy, and makes the model easier to interpret."
      ],
      "metadata": {
        "id": "hiR_js2Omqgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How is Adjusted R-squared calculated:\n",
        "Adjusted R-squared adjusts the regular R-squared by considering the number of predictors. It increases only if the new variable improves the model more than by chance, making it more reliable for comparing models."
      ],
      "metadata": {
        "id": "kL6C-S9MmxfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is MSE sensitive to outliers:\n",
        "MSE is sensitive to outliers because it squares the errors, so large errors (outliers) have a much bigger impact on the final value."
      ],
      "metadata": {
        "id": "jB79dmmWm5NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  What is the role of homoscedasticity in linear regression:\n",
        "Homoscedasticity means the variance of errors is constant across all levels of the independent variables. It ensures the model’s predictions and statistical tests are reliable."
      ],
      "metadata": {
        "id": "fzm99w2jm-wZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is Root Mean Squared Error (RMSE):\n",
        "RMSE is the square root of the average squared errors. It measures the average magnitude of prediction errors, in the same units as the target variable, making it easier to interpret."
      ],
      "metadata": {
        "id": "ecmXn6MdnIZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Why is pickling considered risky:\n",
        "Pickling is risky because it can execute arbitrary code when loading objects, making it vulnerable to code injection attacks if the pickle file comes from an untrusted source."
      ],
      "metadata": {
        "id": "rcY7150HnNgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. , What alternatives exist to pickling for saving ML models:\n",
        "Alternatives to pickling for saving ML models include joblib (faster for large models), ONNX (open standard for model interchange), and TensorFlow SavedModel (for TensorFlow models). These methods can be more efficient and secure."
      ],
      "metadata": {
        "id": "Nq2RkrRCnT-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  What is heteroscedasticity, and why is it a problem:\n",
        "Heteroscedasticity occurs when the variance of errors is not constant across all levels of the independent variables. It’s a problem because it can lead to inefficient estimates and unreliable statistical tests in regression models."
      ],
      "metadata": {
        "id": "dMEPPFhPndiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. How can interaction terms enhance a regression model's predictive power?\n",
        "Interaction terms allow the model to capture the combined effect of two or more predictors, which can improve its predictive power by revealing relationships that individual predictors may not show."
      ],
      "metadata": {
        "id": "TN3fuSBknjVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 1. Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model\n",
        "using Seaborn's \"diamonds\" dataset."
      ],
      "metadata": {
        "id": "TWwjRYOlnqti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Preprocessing: Select relevant features and target\n",
        "diamonds = diamonds.dropna(subset=['price', 'carat', 'depth', 'table'])\n",
        "X = diamonds[['carat', 'depth', 'table']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Add constant to the model for intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Get the residuals\n",
        "residuals = model.resid\n",
        "\n",
        "# Visualize the distribution of residuals\n",
        "sns.histplot(residuals, kde=True, color='blue', bins=30)\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SkOu46ElntnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root\n",
        "Mean Squared Error (RMSE) for a linear regression model"
      ],
      "metadata": {
        "id": "Rjt2AVCBnxJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston dataset (replace with your own dataset if needed)\n",
        "data = load_boston()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate MSE, MAE, and RMSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
      ],
      "metadata": {
        "id": "7JV2aok3nztz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check\n",
        "linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity"
      ],
      "metadata": {
        "id": "DL7SHSFDn3zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Load the Boston dataset (replace with your own dataset if needed)\n",
        "data = load_boston()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Check for linearity (scatter plot between each feature and target variable)\n",
        "sns.pairplot(X_train, height=2.5)\n",
        "plt.suptitle('Feature vs Target Relationships', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Check for homoscedasticity (residual plot)\n",
        "residuals = y_test - y_pred\n",
        "sns.residplot(x=y_pred, y=residuals, lowess=True, color='blue', line_kws={'color': 'red'})\n",
        "plt.title('Residuals vs Predicted Values (Homoscedasticity Check)')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()\n",
        "\n",
        "# Check for multicollinearity (correlation matrix)\n",
        "corr_matrix = X_train.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix (Multicollinearity Check)')\n",
        "plt.show()\n",
        "\n",
        "# Calculate Variance Inflation Factor (VIF) to check multicollinearity\n",
        "X_const = add_constant(X_train)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Variable'] = X_train.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
        "\n",
        "print(\"\\nVariance Inflation Factors (VIF):\")\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "AHI50QINobdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the\n",
        "performance of different regression models"
      ],
      "metadata": {
        "id": "Ny99vUUIogsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston dataset (replace with your own dataset if needed)\n",
        "data = load_boston()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with feature scaling and regression models\n",
        "def create_pipeline(model):\n",
        "    return Pipeline([\n",
        "        ('scaler', StandardScaler()),        # Feature scaling\n",
        "        ('regressor', model)                 # Regression model\n",
        "    ])\n",
        "\n",
        "# List of models to evaluate\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Evaluate each model using cross-validation\n",
        "for name, model in models.items():\n",
        "    pipeline = create_pipeline(model)\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    mse_scores = -scores  # Convert negative MSE to positive\n",
        "    rmse_scores = np.sqrt(mse_scores)\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Mean MSE: {np.mean(mse_scores):.2f}\")\n",
        "    print(f\"  Mean RMSE: {np.mean(rmse_scores):.2f}\")\n",
        "    print(f\"  Standard Deviation of RMSE: {np.std(rmse_scores):.2f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Train and evaluate the best model on the test set (Random Forest in this case)\n",
        "best_model = RandomForestRegressor(random_state=42)\n",
        "pipeline = create_pipeline(best_model)\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate performance on the test set\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "print(f\"Test Set MSE: {mse_test:.2f}\")\n",
        "print(f\"Test Set RMSE: {rmse_test:.2f}\")"
      ],
      "metadata": {
        "id": "OICofnJTokkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and\n",
        "R-squared score."
      ],
      "metadata": {
        "id": "iqRt9-N0ope4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the Boston dataset (replace with your own dataset if needed)\n",
        "data = load_boston()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# For simplicity, use only one feature (for simple linear regression)\n",
        "X_simple = X[['CRIM']]  # Using 'CRIM' (Crime rate) as the feature for simplicity\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model coefficients, intercept, and R-squared score\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared Score: {r2_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "id": "LDlOy_tOos0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using\n",
        "simple linear regression and visualizes the results"
      ],
      "metadata": {
        "id": "PhOSu0dnp9z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the 'tips' dataset from seaborn\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Select features and target variable\n",
        "X = tips[['total_bill']]  # Feature: Total bill\n",
        "y = tips['tip']  # Target: Tip\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse**0.5\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's coefficients, intercept, and performance metrics\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R-squared Score: {r2:.2f}\")\n",
        "\n",
        "# Visualize the relationship between total bill and tip\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=tips, x='total_bill', y='tip', color='blue', label='Data Points')\n",
        "plt.plot(X_test, y_pred, color='red', label='Regression Line')\n",
        "plt.title('Total Bill vs Tip (Linear Regression)')\n",
        "plt.xlabel('Total Bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8VS4jnuFqA7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the\n",
        "model to predict new values and plot the data points along with the regression line."
      ],
      "metadata": {
        "id": "p_M_Rhi4qF0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate a synthetic dataset with one feature (X) and a target variable (y)\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
        "y = 3 * X + 7 + np.random.randn(100, 1) * 2  # Linear relation with some noise\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values using the trained model\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Print the model's coefficients and intercept\n",
        "print(f\"Model Coefficients: {model.coef_[0]}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line', linewidth=2)\n",
        "plt.title('Synthetic Dataset and Linear Regression Model')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SU_53gPVqJLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python script that pickles a trained linear regression model and saves it to a file"
      ],
      "metadata": {
        "id": "2_egJVtVqPAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
        "y = 3 * X + 7 + np.random.randn(100, 1) * 2  # Linear relation with some noise\n",
        "\n",
        "# Train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Pickle the trained model and save it to a file\n",
        "with open('linear_regression_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(\"Model has been pickled and saved to 'linear_regression_model.pkl'\")"
      ],
      "metadata": {
        "id": "HqoijokYqWAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the\n",
        "regression curve"
      ],
      "metadata": {
        "id": "bp2kwo-wqXVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
        "y = 2 * (X**2) + 3 * X + 5 + np.random.randn(100, 1) * 10  # Quadratic relation with noise\n",
        "\n",
        "# Transform the features to include polynomial terms (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Create and train the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict values using the trained model\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the original data and the polynomial regression curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X, y_pred, color='red', label='Polynomial Regression Curve', linewidth=2)\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "qv64i5HSqePG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. . Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear\n",
        "regression model to the data. Print the model's coefficient and intercept."
      ],
      "metadata": {
        "id": "ITpwyaFjqfXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = np.random.rand(100, 1) * 10  # 100 random data points between 0 and 10\n",
        "y = 3 * X + 7 + np.random.randn(100, 1) * 2  # Linear relation with some noise (y = 3 * X + 7)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the model's coefficient and intercept\n",
        "print(f\"Model Coefficient: {model.coef_[0][0]}\")\n",
        "print(f\"Model Intercept: {model.intercept_[0]}\")"
      ],
      "metadata": {
        "id": "9Is5XXTIqjhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and\n",
        "compares their performance."
      ],
      "metadata": {
        "id": "-rGIclaWqn_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
        "y = 2 * (X**2) + 3 * X + 5 + np.random.randn(100, 1) * 10  # Quadratic relation with noise\n",
        "\n",
        "# Initialize plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Loop over different polynomial degrees and fit models\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "for degree in degrees:\n",
        "    # Transform the features to include polynomial terms\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    # Create and train the linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_poly)\n",
        "\n",
        "    # Calculate R-squared score\n",
        "    r2 = r2_score(y, y_pred)\n",
        "\n",
        "    # Plot the regression curve for the current degree\n",
        "    plt.plot(np.sort(X, axis=0), model.predict(poly.fit_transform(np.sort(X, axis=0))), label=f'Degree {degree} (R² = {r2:.2f})')\n",
        "\n",
        "# Plot the original data points\n",
        "plt.scatter(X, y, color='black', alpha=0.5, label='Data Points')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Polynomial Regression Models (Different Degrees)')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-X8If-eGqx8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  Write a Python script that fits a simple linear regression model with two features and prints the model's\n",
        "coefficients, intercept, and R-squared score"
      ],
      "metadata": {
        "id": "02ABonoxqy6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "\n",
        "# 100 random data points for two features\n",
        "X = np.random.rand(100, 2) * 10  # 100 rows, 2 features (between 0 and 10)\n",
        "y = 3 * X[:, 0] + 5 * X[:, 1] + 7 + np.random.randn(100) * 5  # Linear relation with noise\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Print the model's coefficients, intercept, and R-squared score\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared Score: {r2_score(y, y_pred)}\")"
      ],
      "metadata": {
        "id": "4MRK9WoVq2Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the\n",
        "regression line along with the data points."
      ],
      "metadata": {
        "id": "7hxBbUv4q6k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 random data points between 0 and 10\n",
        "y = 3 * X + 7 + np.random.randn(100, 1) * 2  # Linear relation with some noise\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line', linewidth=2)\n",
        "plt.title('Linear Regression (Synthetic Data)')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the model's coefficients and intercept\n",
        "print(f\"Model Coefficients: {model.coef_[0][0]}\")\n",
        "print(f\"Model Intercept: {model.intercept_[0]}\")"
      ],
      "metadata": {
        "id": "K-gwKXHAq_Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset\n",
        "with multiple features."
      ],
      "metadata": {
        "id": "1f_WTs2vrFWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Generate synthetic data with multiple features\n",
        "np.random.seed(42)\n",
        "X1 = np.random.rand(100) * 10  # Feature 1\n",
        "X2 = 3 * X1 + np.random.randn(100) * 2  # Feature 2 (high correlation with X1)\n",
        "X3 = np.random.rand(100) * 10  # Feature 3 (uncorrelated with X1 and X2)\n",
        "y = 2 * X1 + 5 * X2 + 3 * X3 + np.random.randn(100) * 5  # Target variable\n",
        "\n",
        "# Create a DataFrame with the features\n",
        "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3})\n",
        "\n",
        "# Add a constant column for intercept in VIF calculation\n",
        "df_with_constant = add_constant(df)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = df_with_constant.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df_with_constant.values, i) for i in range(df_with_constant.shape[1])]\n",
        "\n",
        "# Display the VIF values\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "J9YwMeoHrI8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a\n",
        "polynomial regression model, and plots the regression curve."
      ],
      "metadata": {
        "id": "fRJtYBS1rgN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data for a polynomial relationship of degree 4\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 random data points between 0 and 10\n",
        "y = 2 * X**4 - 5 * X**3 + 3 * X**2 + 10 * X + 7 + np.random.randn(100, 1) * 100  # Polynomial relationship with noise\n",
        "\n",
        "# Transform the features to include polynomial terms (degree 4)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data points and the polynomial regression curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(np.sort(X, axis=0), model.predict(poly.fit_transform(np.sort(X, axis=0))), color='red', label='Polynomial Regression Line', linewidth=2)\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the model's coefficients and intercept\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "Akvy7BbKrkD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Write a Python script that creates a machine learning pipeline with data standardization and a multiple\n",
        "linear regression model, and prints the R-squared score."
      ],
      "metadata": {
        "id": "zKp8PIQ_rp1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10  # 100 data points with 3 features\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] + 5 * X[:, 2] + 7 + np.random.randn(100) * 2  # Target variable with noise\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with standardization and linear regression\n",
        "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(f\"R-squared score: {r2}\")"
      ],
      "metadata": {
        "id": "2VVNUnIqrupi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the\n",
        "regression curve."
      ],
      "metadata": {
        "id": "iJMcw1l6r8IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data for a polynomial relationship of degree 3\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 random data points between 0 and 10\n",
        "y = 2 * X**3 - 5 * X**2 + 3 * X + 7 + np.random.randn(100, 1) * 100  # Polynomial relationship with noise\n",
        "\n",
        "# Transform the features to include polynomial terms (degree 3)\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data points and the polynomial regression curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(np.sort(X, axis=0), model.predict(poly.fit_transform(np.sort(X, axis=0))), color='red', label='Polynomial Regression Curve', linewidth=2)\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the model's coefficients and intercept\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "R5lpWmMNr_9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print\n",
        "the R-squared score and model coefficients"
      ],
      "metadata": {
        "id": "QjZvOLT6sF-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data with 5 features\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5) * 10  # 100 samples, 5 features (values between 0 and 10)\n",
        "y = 3 * X[:, 0] + 5 * X[:, 1] - 2 * X[:, 2] + 7 * X[:, 3] + 1 * X[:, 4] + 10 + np.random.randn(100) * 2  # Linear relationship with noise\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2}\")\n",
        "\n",
        "# Print the model's coefficients and intercept\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "0OVBrpGlsNOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the\n",
        "data points along with the regression line."
      ],
      "metadata": {
        "id": "hNV9pr7UsTXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data for linear regression\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 random data points between 0 and 10\n",
        "y = 2 * X + 5 + np.random.randn(100, 1) * 2  # Linear relationship with some noise\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions using the fitted model\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Visualize the data points and the regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points')  # Plot the data points\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line', linewidth=2)  # Plot the regression line\n",
        "plt.title('Linear Regression: Data Points and Regression Line')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the model's coefficient and intercept\n",
        "print(f\"Model Coefficient: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "MUZihCDrsXPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's Rsquared score and coefficients."
      ],
      "metadata": {
        "id": "KI2AK0e_sdoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data with 3 features\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10  # 100 samples, 3 features (values between 0 and 10)\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + 5 + np.random.randn(100) * 2  # Linear relationship with noise\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2}\")\n",
        "\n",
        "# Print the model's coefficients and intercept\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "HRyGEcxPsjr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python script that demonstrates how to serialize and deserialize machine learning models using\n",
        "joblib instead of pickling."
      ],
      "metadata": {
        "id": "ZFhX_awVsqAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import joblib\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 random data points between 0 and 10\n",
        "y = 2 * X + 5 + np.random.randn(100, 1) * 2  # Linear relationship with noise\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib_file = \"linear_regression_model.pkl\"\n",
        "joblib.dump(model, joblib_file)\n",
        "print(f\"Model saved to {joblib_file}\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load(joblib_file)\n",
        "print(f\"Model loaded from {joblib_file}\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "X_new = np.array([[5]])  # New data point for prediction\n",
        "y_pred = loaded_model.predict(X_new)\n",
        "print(f\"Prediction for X={X_new}: {y_pred}\")"
      ],
      "metadata": {
        "id": "W_PrXP20sv04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Write a Python script to perform linear regression with categorical features using one-hot encoding. Use\n",
        "the Seaborn 'tips' dataset."
      ],
      "metadata": {
        "id": "OxWksaPYs1pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the 'tips' dataset from seaborn\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Define the features (X) and the target variable (y)\n",
        "X = tips.drop('total_bill', axis=1)  # All columns except 'total_bill'\n",
        "y = tips['total_bill']  # Target variable\n",
        "\n",
        "# Define the column transformer with one-hot encoding for categorical variables\n",
        "column_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(), ['sex', 'smoker', 'day', 'time'])  # Apply one-hot encoding to categorical columns\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep the numeric columns as they are\n",
        ")\n",
        "\n",
        "# Create a pipeline that applies the column transformer and then fits a linear regression model\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', column_transformer),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's R-squared score and coefficients\n",
        "print(f\"R-squared score: {r2}\")\n",
        "\n",
        "# Coefficients are available from the regressor step of the pipeline\n",
        "coefficients = pipeline.named_steps['regressor'].coef_\n",
        "\n",
        "# Print the coefficients along with the feature names\n",
        "encoded_columns = pipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out(input_features=['sex', 'smoker', 'day', 'time'])\n",
        "all_columns = list(encoded_columns) + ['size']  # Add 'size' which is already numerical\n",
        "coeff_dict = dict(zip(all_columns, coefficients))\n",
        "\n",
        "print(f\"Model coefficients:\")\n",
        "for feature, coef in coeff_dict.items():\n",
        "    print(f\"{feature}: {coef}\")"
      ],
      "metadata": {
        "id": "QGAy2iO-s5XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and Rsquared score."
      ],
      "metadata": {
        "id": "CzH-spT3tB34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10  # 100 samples, 3 features (values between 0 and 10)\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + 5 + np.random.randn(100) * 2  # Linear relationship with noise\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score for Linear Regression\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "# Ridge Regression model (using alpha=1.0 for regularization)\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score for Ridge Regression\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "# Print coefficients and R-squared scores\n",
        "print(f\"Linear Regression Coefficients: {lr_model.coef_}\")\n",
        "print(f\"Linear Regression Intercept: {lr_model.intercept_}\")\n",
        "print(f\"Linear Regression R-squared: {r2_lr}\\n\")\n",
        "\n",
        "print(f\"Ridge Regression Coefficients: {ridge_model.coef_}\")\n",
        "print(f\"Ridge Regression Intercept: {ridge_model.intercept_}\")\n",
        "print(f\"Ridge Regression R-squared: {r2_ridge}\")"
      ],
      "metadata": {
        "id": "vAAbvrmttGj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic\n",
        "dataset"
      ],
      "metadata": {
        "id": "Xi2_aaeatLf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "# Print the cross-validation results\n",
        "print(f\"Cross-validation R-squared scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared: {cv_scores.mean()}\")\n",
        "print(f\"Standard deviation of R-squared: {cv_scores.std()}\")"
      ],
      "metadata": {
        "id": "-bWYM5AgtRcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. . Write a Python script that compares polynomial regression models of different degrees and prints the Rsquared score for each."
      ],
      "metadata": {
        "id": "k9jGyOq1tYIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data with a non-linear relationship\n",
        "np.random.seed(42)\n",
        "X = np.sort(np.random.rand(100, 1) * 10, axis=0)  # 100 data points between 0 and 10\n",
        "y = 0.5 * X**3 - X**2 + 2 + np.random.randn(100, 1) * 20  # Non-linear relation with some noise\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to perform polynomial regression for different degrees\n",
        "def polynomial_regression(degree):\n",
        "    # Create a polynomial feature transformer\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "\n",
        "    # Transform the features to include polynomial terms\n",
        "    X_poly_train = poly.fit_transform(X_train)\n",
        "    X_poly_test = poly.transform(X_test)\n",
        "\n",
        "    # Create a Linear Regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_poly_train, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = model.predict(X_poly_test)\n",
        "\n",
        "    # Calculate the R-squared score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return r2, model.coef_, model.intercept_\n",
        "\n",
        "# List of polynomial degrees to compare\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Compare the models\n",
        "for degree in degrees:\n",
        "    r2, coef, intercept = polynomial_regression(degree)\n",
        "    print(f\"Polynomial degree {degree}:\")\n",
        "    print(f\"  R-squared: {r2}\")\n",
        "    print(f\"  Coefficients: {coef}\")\n",
        "    print(f\"  Intercept: {intercept}\\n\")\n",
        "\n",
        "    # Plotting the polynomial regression curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(X, y, color='blue', label='Data points')\n",
        "    X_line = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "    poly_line = PolynomialFeatures(degree=degree).fit_transform(X_line)\n",
        "    y_line = model.predict(poly_line)\n",
        "    plt.plot(X_line, y_line, label=f'Polynomial degree {degree}')\n",
        "    plt.title(f\"Polynomial Regression (Degree {degree})\")\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3rcpZzm2tbke"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}