{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM//O0VYgfIjXIH75nm0yx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "543d6nKMHVaW"
      },
      "outputs": [],
      "source": [
        "What is a Decision Tree, and how does it work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a machine learning algorithm used for both classification and regression tasks. It works by recursively splitting the data into subsets based on the feature values to create a tree-like structure. The goal is to create branches that split the data into increasingly homogenous groups (or classes).\n",
        "\n",
        "How it works:\n",
        "Root Node: The tree starts at the root node, which contains the entire dataset.\n",
        "\n",
        "Splitting: At each node, the data is split into two or more subsets based on a feature and a threshold value that maximizes a certain criterion (e.g., Gini index for classification, Mean Squared Error for regression).\n",
        "\n",
        "Branching: The splitting process continues recursively for each child node, which results in the creation of branches.\n",
        "\n",
        "Leaf Nodes: Once a node reaches a certain condition (like max depth or minimum samples), it becomes a leaf node, which gives the predicted value or class label.\n",
        "\n",
        "Prediction: For a given input, you follow the branches from the root node to the appropriate leaf, and the prediction is based on the majority class or average value in the leaf."
      ],
      "metadata": {
        "id": "a5K6PdYbHfbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees\n",
        "Impurity measures in Decision Trees evaluate how well a node split separates the data. Common measures include:\n",
        "\n",
        "Gini Impurity: Measures class purity. Lower values are better.\n",
        "\n",
        "Entropy: Measures disorder. Lower values indicate better splits.\n",
        "\n",
        "Mean Squared Error (MSE): Used in regression, measures prediction error. Lower MSE is better.\n",
        "\n"
      ],
      "metadata": {
        "id": "A02l_Wk5HgKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical formula for Gini Impurity\n"
      ],
      "metadata": {
        "id": "Ej7kVMnIHtyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Gini Impurity Formula\n",
        "Gini = 1 - sum(p_i^2)"
      ],
      "metadata": {
        "id": "SBX7HaaUIC_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "pi\n",
        "is the proportion of each class\n",
        "i in the node.\n",
        "\n",
        "The sum is over all the unique classes in the node."
      ],
      "metadata": {
        "id": "Ri9LgCzRIFlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the mathematical formula for Entropy"
      ],
      "metadata": {
        "id": "XSnqjuOdILMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entropy Formula\n",
        "Entropy = -sum(p_i * log2(p_i))\n"
      ],
      "metadata": {
        "id": "rog7_6H0IP4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "pi\n",
        "is the proportion of each class\n",
        "i in the node.\n",
        "\n",
        "The sum is over all unique classes in the node"
      ],
      "metadata": {
        "id": "sLlLr3FGIQvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Information Gain, and how is it used in Decision Trees\n",
        "Information Gain is a measure used to decide which feature to split on in a Decision Tree. It quantifies the reduction in entropy (or disorder) after a dataset is split based on a feature. The goal is to choose the feature that provides the highest information gain, leading to a more informative and homogeneous split.\n",
        "Use in Decision Trees:\n",
        "The algorithm splits the data on the feature with the highest information gain, reducing uncertainty at each step."
      ],
      "metadata": {
        "id": "OAGMqVW0IXCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the difference between Gini Impurity and Entropy\n",
        "Gini Impurity and Entropy are both measures of impurity used in Decision Trees, but they differ in calculation and interpretation:\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "Simpler and faster to compute.\n",
        "\n",
        "Measures the probability of a wrong classification.\n",
        "Entropy:\n",
        "\n",
        "Measures the disorder or uncertainty in a node."
      ],
      "metadata": {
        "id": "Ab7C11eFIlRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees\n",
        "In Decision Trees, the goal is to recursively split the dataset into subsets that are as pure as possible (i.e., contain instances of the same class or similar values). The process is based on an impurity measure (like Gini Impurity or Entropy) to decide the best feature and threshold for each split.\n",
        "\n",
        "Mathematical Explanation:\n",
        "Root Node:\n",
        "\n",
        "Start with the entire dataset.\n",
        "\n",
        "The root node represents the whole dataset.\n",
        "\n",
        "Splitting:\n",
        "\n",
        "At each node, the algorithm chooses the feature and threshold that best splits the data into two child nodes. This is done by minimizing the impurity (e.g., Gini Impurity, Entropy, or MSE for regression)."
      ],
      "metadata": {
        "id": "mu8WTnN3Ivic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Pre-Pruning in Decision Trees\n",
        "Pre-Pruning (also known as Early Stopping) is a technique used to prevent overfitting in Decision Trees by limiting the growth of the tree during training. It involves setting conditions or constraints that stop the tree from growing further before it reaches full potential.\n",
        "\n",
        "Key Pre-Pruning Criteria:\n",
        "Maximum Depth: Limit the depth of the tree to prevent it from growing too complex.\n",
        "\n",
        "Minimum Samples per Leaf: Set a threshold for the minimum number of samples that must be in a leaf node.\n",
        "\n",
        "Minimum Samples per Split: Set a threshold for the minimum number of samples required to split a node.\n",
        "\n",
        "Maximum Features: Limit the number of features to consider when looking for the best split.\n",
        "\n",
        "Maximum Number of Leaf Nodes: Restrict the tree to a certain number of leaf nodes.\n",
        "\n",
        "Why Use Pre-Pruning:\n",
        "It reduces overfitting by limiting the complexity of the tree.\n",
        "\n",
        "Helps in creating more generalizable models that perform better on unseen data.\n",
        "\n",
        "In short, pre-pruning aims to stop tree growth before it becomes too tailored to the training data."
      ],
      "metadata": {
        "id": "oyUXyOf1Hr8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Post-Pruning in Decision Trees\n",
        "Post-Pruning (also known as Cost Complexity Pruning or Weakest Link Pruning) is a technique used in Decision Trees to reduce overfitting after the tree has been fully grown. Unlike pre-pruning, which stops the tree from growing, post-pruning removes branches from a fully grown tree.\n",
        "\n",
        "Key Steps in Post-Pruning:\n",
        "Build the Full Tree: First, a fully grown tree is built without any pruning restrictions.\n",
        "\n",
        "Evaluate Subtrees: Each node is evaluated by considering what would happen if that subtree were removed.\n",
        "\n",
        "Prune the Weakest Branches: Branches or subtrees that provide the least benefit in terms of reducing impurity or improving accuracy are pruned.\n",
        "\n",
        "Use Cross-Validation: A common approach is to use cross-validation to evaluate how each subtree affects the model's generalization performance. Nodes that increase error when removed are kept, while those that reduce performance are pruned.\n",
        "\n",
        "Why Use Post-Pruning:\n",
        "Helps remove unnecessary complexity from a fully grown tree.\n",
        "\n",
        "Reduces overfitting by eliminating nodes that capture noise in the training data.\n",
        "\n",
        "Results in a simpler, more generalizable model."
      ],
      "metadata": {
        "id": "hb7l7s53JX3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning\n",
        "The key differences between Pre-Pruning and Post-Pruning in Decision Trees are:\n",
        "\n",
        "When They Occur:\n",
        "\n",
        "Pre-Pruning: Happens during the tree construction process. The tree is stopped from growing further based on certain conditions (e.g., max depth, minimum samples).\n",
        "\n",
        "Post-Pruning: Happens after the tree is fully grown. It involves removing branches from the already constructed tree.\n",
        "\n",
        "How They Work:\n",
        "\n",
        "Pre-Pruning: Limits tree growth by setting constraints, preventing it from becoming too complex in the first place.\n",
        "\n",
        "Post-Pruning: Starts with a full tree and prunes branches that do not contribute significantly to improving the model’s performance.\n",
        "\n",
        "Overfitting Control:\n",
        "\n",
        "Pre-Pruning: Controls overfitting by limiting the size and depth of the tree upfront.\n",
        "\n",
        "Post-Pruning: Addresses overfitting by simplifying the tree after it has been trained, ensuring the model generalizes better.\n",
        "\n",
        "Flexibility:\n",
        "\n",
        "Pre-Pruning: More rigid, as you set rules to stop the tree from growing.\n",
        "\n",
        "Post-Pruning: More flexible, as it removes branches based on post-construction evaluation."
      ],
      "metadata": {
        "id": "j73zOyJpKn4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is a Decision Tree Regressor\n",
        "A Decision Tree Regressor is a type of decision tree algorithm used for regression tasks, where the goal is to predict a continuous target variable. It works by recursively splitting the data based on feature values, aiming to minimize the variance (or other measures of impurity) within each split.\n",
        "\n",
        "How it works:\n",
        "Splitting: The data is split at each node based on the feature that best reduces the variance or error within each subset of data.\n",
        "\n",
        "Leaf Nodes: At the leaf nodes, the prediction is the average of the target values of the data points in that node.\n",
        "\n",
        "Recursive Process: This process continues recursively, creating more splits at each node, until a stopping condition is met (e.g., max depth, minimum samples per leaf).\n",
        "\n",
        "Key Points:\n",
        "Output: Continuous values (e.g., predicting house prices or stock prices).\n",
        "\n",
        "Split Criterion: Typically uses Mean Squared Error (MSE) or Variance to determine the best splits.\n",
        "\n",
        "Interpretability: Easy to interpret and visualize since it follows a tree structure."
      ],
      "metadata": {
        "id": "ZtW4LfvEKxgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees\n",
        "Advantages of Decision Trees:\n",
        "Easy to Interpret: Decision trees are simple to understand and visualize, making them highly interpretable.\n",
        "\n",
        "Non-Linear Relationships: They can model non-linear relationships without requiring any transformation of the features.\n",
        "\n",
        "Handles Both Numerical and Categorical Data: Decision trees can work with both types of data without needing scaling or encoding.\n",
        "\n",
        "No Need for Feature Scaling: Unlike some models, decision trees don’t require normalization or standardization of data.\n",
        "\n",
        "Feature Selection: Decision trees inherently perform feature selection as they split on the most important features.\n",
        "\n",
        "Works Well with Missing Data: Decision trees can handle missing values during training by either using surrogate splits or ignoring them.\n",
        "\n",
        "Disadvantages of Decision Trees:\n",
        "Overfitting: Decision trees are prone to overfitting, especially when they grow deep, leading to a model that performs well on training data but poorly on unseen data.\n",
        "\n",
        "Instability: Small changes in the data can lead to large changes in the structure of the tree.\n",
        "\n",
        "Bias Toward Features with More Categories: Decision trees may favor features with more categories or distinct values, leading to biased splits.\n",
        "\n",
        "Complexity: For complex datasets, decision trees can become too large and complex, reducing interpretability and generalization.\n",
        "\n",
        "Greedy Algorithm: The algorithm follows a greedy approach, making locally optimal choices that may not lead to the globally optimal tree.\n",
        "\n",
        "Poor Performance on Linear Data: Decision trees do not perform well on data that has a linear relationship unless the tree is very deep."
      ],
      "metadata": {
        "id": "yC0LGifdK4ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. How does a Decision Tree handle missing values\n",
        "A Decision Tree can handle missing values in several ways during training and prediction:\n",
        "\n",
        "1. Ignoring Missing Values:\n",
        "During Training: The tree can ignore missing values in the data by only using the available data to split nodes. If a feature has missing values, it can still be used for splitting based on non-missing values.\n",
        "\n",
        "During Prediction: If a value is missing for a particular instance, the decision tree can make a prediction by moving down the tree using the non-missing features and deciding based on the available information.\n",
        "\n",
        "2. Surrogate Splits:\n",
        "In case of missing values, the tree can use surrogate splits. These are backup splits based on other features that are highly correlated with the primary splitting feature.\n",
        "\n",
        "The decision tree will first attempt to split based on the main feature, and if the value is missing, it will use a surrogate split to decide the branch.\n",
        "\n",
        "3. Imputation:\n",
        "Preprocessing Step: One way to handle missing values is to impute missing values before training the tree. This can be done by filling missing values with the mean, median, or mode of the column, or by using more advanced imputation techniques like KNN imputation.\n",
        "\n",
        "This helps the model by ensuring there are no missing values during training.\n",
        "\n",
        "4. Assigning Probabilities:\n",
        "For missing values at a particular node during training, some algorithms assign a probability distribution for the missing value based on the distribution of the available data.\n",
        "\n",
        "In summary, decision trees have built-in ways to handle missing values during training and prediction, either by ignoring the missing data, using surrogate splits, or imputing values before model training."
      ],
      "metadata": {
        "id": "H9eqJn_YK_7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  How does a Decision Tree handle categorical features\n",
        "A Decision Tree handles categorical features by:\n",
        "\n",
        "Splitting: It divides data based on distinct categories of the feature (e.g., \"Red\", \"Blue\", \"Green\").\n",
        "\n",
        "Best Split: It selects the best split based on metrics like Gini Impurity or Entropy.\n",
        "\n",
        "Ordinal Data: It can handle ordinal features (like \"Low\", \"Medium\", \"High\") but often treats them as nominal.\n",
        "\n",
        "No Scaling Needed: Categorical features do not require encoding or scaling.\n",
        "\n",
        "Efficient with High Cardinality: Handles features with many categories without extra preprocessing."
      ],
      "metadata": {
        "id": "1xSBH7trLIKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What are some real-world applications of Decision Trees?\n",
        "Some real-world applications of Decision Trees include:\n",
        "\n",
        "Healthcare: Diagnosing diseases based on patient symptoms and medical history.\n",
        "\n",
        "Finance: Credit scoring to assess loan eligibility.\n",
        "\n",
        "Marketing: Customer segmentation for targeted marketing campaigns.\n",
        "\n",
        "E-commerce: Recommending products based on user behavior.\n",
        "\n",
        "Fraud Detection: Identifying fraudulent transactions in banking and finance.\n",
        "\n",
        "Manufacturing: Predicting equipment failure or optimizing supply chain logistics.\n",
        "\n",
        "Risk Analysis: Predicting risks in insurance and investment sectors."
      ],
      "metadata": {
        "id": "qiTskO9sLUjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy*"
      ],
      "metadata": {
        "id": "-J5M1tpoLfqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import Decision\n"
      ],
      "metadata": {
        "id": "g4OMhIcOLiCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances*"
      ],
      "metadata": {
        "id": "68cx5XJFLk12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "id": "Mv4Z-cXRLm52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy"
      ],
      "metadata": {
        "id": "acCmRyYtLs9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Entropy as the criterion\n",
        "model = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "GnKJdCLYLw9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)*\n"
      ],
      "metadata": {
        "id": "loURv291Lzi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n"
      ],
      "metadata": {
        "id": "T2nvhrdpL1oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz"
      ],
      "metadata": {
        "id": "rTtbct-4L5K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from graphviz import Source\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the tree using Graphviz\n",
        "dot_data = export_graphviz(model, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "graph = Source(dot_data)\n",
        "graph.render(\"iris_decision_tree\", view=True)  # Save and view the tree\n"
      ],
      "metadata": {
        "id": "myB_c53BL84-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree"
      ],
      "metadata": {
        "id": "OhJz01MKMBtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with a max depth of 3\n",
        "tree_max_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_max_depth_3.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and calculate accuracy for max depth 3\n",
        "y_pred_max_depth_3 = tree_max_depth_3.predict(X_test)\n",
        "accuracy_max_depth_3 = accuracy_score(y_test, y_pred_max_depth_3)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier without limiting depth (fully grown tree)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and calculate accuracy for the fully grown tree\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy of Decision Tree with max depth 3: {accuracy_max_depth_3:.2f}\")\n",
        "print(f\"Accuracy of Decision Tree with fully grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "N2yIQbw8MD8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree"
      ],
      "metadata": {
        "id": "-rGktEorMISl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with min_samples_split=5\n",
        "tree_min_samples_split_5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "tree_min_samples_split_5.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and calculate accuracy for min_samples_split=5\n",
        "y_pred_min_samples_split_5 = tree_min_samples_split_5.predict(X_test)\n",
        "accuracy_min_samples_split_5 = accuracy_score(y_test, y_pred_min_samples_split_5)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with default parameters\n",
        "tree_default = DecisionTreeClassifier(random_state=42)\n",
        "tree_default.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and calculate accuracy for the default tree\n",
        "y_pred_default = tree_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy of Decision Tree with min_samples_split=5: {accuracy_min_samples_split_5:.2f}\")\n",
        "print(f\"Accuracy of Decision Tree with default parameters: {accuracy_default:.2f}\")\n"
      ],
      "metadata": {
        "id": "eN9rIbZ7MKjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data*"
      ],
      "metadata": {
        "id": "_6sRuUjMMPYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data (feature scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Decision Tree Classifier on unscaled data\n",
        "tree_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "tree_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy for unscaled data\n",
        "y_pred_unscaled = tree_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Train the Decision Tree Classifier on scaled data\n",
        "tree_scaled = DecisionTreeClassifier(random_state=42)\n",
        "tree_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and calculate accuracy for scaled data\n",
        "y_pred_scaled = tree_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy of Decision Tree with unscaled data: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy of Decision Tree with scaled data: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "oON2Ei_tMSr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification*"
      ],
      "metadata": {
        "id": "HMB_DT9RMXOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Wrap the classifier with OneVsRestClassifier (OvR strategy)\n",
        "ovr_classifier = OneVsRestClassifier(tree)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Decision Tree Classifier using One-vs-Rest (OvR) strategy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "XxCcUP1HMabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores"
      ],
      "metadata": {
        "id": "ewDd2WDVMexD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert to DataFrame for better visualization of feature importance\n",
        "X_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Get and display the feature importance scores\n",
        "feature_importance = tree.feature_importances_\n",
        "\n",
        "# Print the feature importance scores along with feature names\n",
        "for feature, importance in zip(X_df.columns, feature_importance):\n",
        "    print(f\"Feature: {feature}, Importance: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "h_IJIHMjMg_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. * Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree*"
      ],
      "metadata": {
        "id": "5nf38I9kMlgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor with max_depth=5\n",
        "tree_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train Decision Tree Regressor with no maximum depth (unrestricted tree)\n",
        "tree_unrestricted = DecisionTreeRegressor(random_state=42)\n",
        "tree_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "y_pred_unrestricted = tree_unrestricted.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for both models\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Print MSE for both models\n",
        "print(f\"Mean Squared Error with max_depth=5: {mse_limited:.4f}\")\n",
        "print(f\"Mean Squared Error with unrestricted tree: {mse_unrestricted:.4f}\")\n"
      ],
      "metadata": {
        "id": "1v9aDrx6MogN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy*"
      ],
      "metadata": {
        "id": "5RkLlSmGMsEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier without pruning\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy without pruning\n",
        "y_pred = tree.predict(X_test)\n",
        "accuracy_before_pruning = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Apply Cost Complexity Pruning (CCP)\n",
        "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# List to store accuracies after pruning for different alphas\n",
        "accuracies_after_pruning = []\n",
        "\n",
        "# Train and evaluate decision trees for each value of alpha (CCP)\n",
        "for alpha in ccp_alphas:\n",
        "    tree_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    tree_pruned.fit(X_train, y_train)\n",
        "    y_pred_pruned = tree_pruned.predict(X_test)\n",
        "    accuracies_after_pruning.append(accuracy_score(y_test, y_pred_pruned\n"
      ],
      "metadata": {
        "id": "8rNYPKmKMuZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "2dLRqrUHMyDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "7lui85G7M0eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.  Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "w0pxjAJGM5P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sMzF_m2dM7Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split"
      ],
      "metadata": {
        "id": "EioPislWNAH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "xJSugPDlNCym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}