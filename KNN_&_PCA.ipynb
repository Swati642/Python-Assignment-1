{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0qJaQHe3jHR/8iy6c5qqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/KNN_%26_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work\n",
        "KNN is a simple, non-parametric algorithm used for classification and regression. It works by finding the *k* nearest data points (neighbors) to a query point and predicting the label based on majority voting (classification) or averaging (regression). It relies on distance metrics like Euclidean distance. No training phase is needed—just memorization of data."
      ],
      "metadata": {
        "id": "Daz5eAIwLYS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between KNN Classification and KNN Regression\n",
        "KNN Classification predicts the class label by majority vote of *k* nearest neighbors.  \n",
        "KNN Regression predicts the target value by averaging the values of *k* nearest neighbors.  \n",
        "Classification deals with discrete labels; regression deals with continuous values.  \n",
        "Both use distance metrics to find neighbors."
      ],
      "metadata": {
        "id": "Yd_1yu6lLpOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the role of the distance metric in KNN\n",
        "The distance metric measures similarity between data points.  \n",
        "It helps KNN find the closest neighbors to a query point.  \n",
        "Common metrics: Euclidean (default), Manhattan, Minkowski.  \n",
        "The choice of metric affects accuracy and performance."
      ],
      "metadata": {
        "id": "1XbiAHI7Ly4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the Curse of Dimensionality in KNN\n",
        "In high dimensions, data becomes sparse and distances lose meaning.  \n",
        "KNN struggles to find true nearest neighbors.  \n",
        "Accuracy drops as noise increases.  \n",
        "Feature selection or dimensionality reduction is needed."
      ],
      "metadata": {
        "id": "KtfiYsAJL5YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How can we choose the best value of K in KNN\n",
        "Use cross-validation to test different K values.  \n",
        "Plot accuracy vs K and pick the one with best performance.  \n",
        "Avoid very low K (overfitting) and very high K (underfitting).  \n",
        "Odd K helps in binary classification."
      ],
      "metadata": {
        "id": "xK6GaaIjL_ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What are KD Tree and Ball Tree in KNN\n",
        "KD Tree and Ball Tree are data structures used to speed up nearest neighbor search in KNN.  \n",
        "- **KD Tree**: Best for low-dimensional data. Splits data along axes.  \n",
        "- **Ball Tree**: Works better for high-dimensional data. Uses hyperspheres.  \n",
        "They reduce computation time for distance queries."
      ],
      "metadata": {
        "id": "ptSbyaOhMH7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should you use KD Tree vs. Ball Tree\n",
        "Use **KD Tree** when data is **low-dimensional (d < 20)** and balanced.  \n",
        "Use **Ball Tree** when data is **high-dimensional**, sparse, or unbalanced.  \n",
        "Ball Tree handles complex structures better; KD Tree is faster for simpler data."
      ],
      "metadata": {
        "id": "CWeqrRmAMTZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What are the disadvantages of KNN\n",
        "- Slow for large datasets (no training, all computation at prediction).  \n",
        "- Sensitive to noisy or irrelevant features.  \n",
        "- Struggles in high-dimensional spaces (curse of dimensionality).  \n",
        "- Requires feature scaling.  \n",
        "- Memory-intensive (stores entire dataset)."
      ],
      "metadata": {
        "id": "50FP72gVMcbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does feature scaling affect KNN\n",
        "Feature scaling is crucial for KNN because it uses distance metrics. Without scaling, features with larger ranges dominate, leading to biased results. Scaling (like StandardScaler or MinMaxScaler) ensures fair distance computation."
      ],
      "metadata": {
        "id": "-_bYv0rvMkjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is PCA (Principal Component Analysis)\n",
        "PCA (Principal Component Analysis) is a dimensionality reduction technique. It transforms data into a new coordinate system, where the greatest variance lies on the first axis (principal component), the second greatest on the second axis, and so on. This helps reduce the number of features while preserving as much variance as possible."
      ],
      "metadata": {
        "id": "uSt2_bQ7Mr0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  How does PCA work\n",
        "PCA works by identifying the directions (principal components) in which the data varies the most. Steps:\n",
        "1. **Standardize the data** (mean = 0, variance = 1).\n",
        "2. **Calculate the covariance matrix** to understand relationships between features.\n",
        "3. **Compute the eigenvalues and eigenvectors** of the covariance matrix.\n",
        "4. **Sort eigenvalues** in descending order; select top 'k' eigenvectors.\n",
        "5. **Project data** onto the selected eigenvectors (principal components)."
      ],
      "metadata": {
        "id": "n40O-i32M0cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the geometric intuition behind PCA\n",
        "PCA can be understood geometrically as projecting data onto a lower-dimensional subspace that captures the maximum variance.\n",
        "\n",
        "1. **High variance direction**: PCA identifies the axis (principal component) along which the data varies the most.\n",
        "2. **Projection**: It then projects the original data points onto this axis, reducing dimensions.\n",
        "3. **Maximizing variance**: The first principal component captures the maximum variance, and each subsequent component captures the remaining variance orthogonally to the previous ones.\n",
        "   \n",
        "This helps in finding the best representation of the data in fewer dimensions."
      ],
      "metadata": {
        "id": "tnvJV6s-M98f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gyu1VS2eM39K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the difference between Feature Selection and Feature Extraction\n",
        "**Feature Selection**:\n",
        "- **Goal**: Select the most important features from the original set, keeping the data in its original form.\n",
        "- **Method**: Techniques like filtering, wrapper, or embedded methods are used to remove irrelevant or redundant features.\n",
        "- **Outcome**: Reduced feature set, but original features are retained.\n",
        "\n",
        "**Feature Extraction**:\n",
        "- **Goal**: Transform the original features into a new set of features, often of lower dimensionality.\n",
        "- **Method**: Techniques like PCA, LDA, or autoencoders are used to combine or transform features into new ones.\n",
        "- **Outcome**: A new set of features (e.g., principal components) that captures the essential information.\n",
        "\n",
        "In short, feature selection reduces the number of features, while feature extraction creates new features."
      ],
      "metadata": {
        "id": "bi1JDkuuNGdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are Eigenvalues and Eigenvectors in PCA\n",
        "**Eigenvalues**:\n",
        "- Represent the magnitude of the variance captured by each principal component in PCA.\n",
        "- Larger eigenvalues indicate more significant components that capture more data variability.\n",
        "\n",
        "**Eigenvectors**:\n",
        "- Represent the direction of the principal components in the feature space.\n",
        "- They define the new axes onto which the data will be projected, capturing the directions of maximum variance.\n",
        "\n",
        "In PCA, eigenvalues and eigenvectors are derived from the covariance matrix and are used to determine the principal components."
      ],
      "metadata": {
        "id": "lZuHOURgNSvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How do you decide the number of components to keep in PCA\n",
        "To decide the number of components to keep in PCA:\n",
        "\n",
        "1. **Cumulative Explained Variance**: Choose components that explain a significant portion of the variance (typically 80-95%). Plot the cumulative explained variance and select the number of components where the curve plateaus.\n",
        "\n",
        "2. **Scree Plot**: Look for an \"elbow\" in the plot of eigenvalues, where the explained variance starts to diminish significantly.\n",
        "\n",
        "3. **Domain Knowledge**: Sometimes, domain knowledge or business requirements can guide the selection of the number of components."
      ],
      "metadata": {
        "id": "Tf_iOiiLNfjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Can PCA be used for classification\n",
        "PCA itself is not a classification method, but it can be used as a preprocessing step. By reducing the dimensionality of the data, PCA helps to:\n",
        "\n",
        "1. **Reduce noise** and irrelevant features, improving the performance of classification algorithms.\n",
        "2. **Speed up computation**, especially with large datasets.\n",
        "3. **Improve visualization** by projecting data onto 2D or 3D space for easier interpretation.\n",
        "\n",
        "After applying PCA, you can use a classification algorithm (e.g., SVM, Logistic Regression) on the reduced data."
      ],
      "metadata": {
        "id": "HIDt39cGNlcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the limitations of PCA\n",
        "Limitations of PCA:\n",
        "\n",
        "1. **Assumes linearity** – can't capture complex non-linear relationships.  \n",
        "2. **Loses interpretability** – transformed features (PCs) are not easily interpretable.  \n",
        "3. **Sensitive to scaling** – requires feature scaling for meaningful results.  \n",
        "4. **Affected by outliers** – outliers can distort the direction of principal components.  \n",
        "5. **Only captures variance** – may ignore features important for classification if they have low variance."
      ],
      "metadata": {
        "id": "eZwyNN6oNuRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. 5 How do KNN and PCA complement each other\n",
        "KNN and PCA complement each other well:\n",
        "\n",
        "1. **PCA reduces dimensionality**, which helps **mitigate the Curse of Dimensionality** in KNN.  \n",
        "2. PCA removes noise and redundant features, improving KNN’s accuracy.  \n",
        "3. **Feature scaling in PCA** aligns with KNN's distance-based approach.  \n",
        "4. PCA speeds up KNN by reducing computation on fewer features.  \n",
        "5. Together, they enhance both performance and efficiency."
      ],
      "metadata": {
        "id": "__aMNyX6gKjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How does KNN handle missing values in a dataset\n",
        "KNN can handle missing values by:\n",
        "\n",
        "1. **Imputing missing values** using the mean, median, or mode of the k-nearest neighbors.  \n",
        "2. Some libraries support **KNN imputation** directly.  \n",
        "3. It’s not built to handle missing data inherently—**imputation is done before training**.  \n",
        "4. Best to scale features after imputation for better results."
      ],
      "metadata": {
        "id": "LT02IFvIg54I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "**Key differences between PCA and LDA**:\n",
        "\n",
        "1. **PCA** is unsupervised; **LDA** is supervised.  \n",
        "2. **PCA** maximizes variance; **LDA** maximizes class separability.  \n",
        "3. **PCA** doesn’t use class labels; **LDA** uses them.  \n",
        "4. **PCA** works for any data; **LDA** mainly for classification tasks.  \n",
        "5. **PCA** components are orthogonal; **LDA** components may not be."
      ],
      "metadata": {
        "id": "VhXWEFpZhDo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a KNN Classifier on the Iris dataset and print model accuracy\n"
      ],
      "metadata": {
        "id": "vWXjdhH4hMOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "gH4lqccUh6mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
        "\n"
      ],
      "metadata": {
        "id": "SOZpin4whXqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Regressor\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "09PQzy58h1VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n"
      ],
      "metadata": {
        "id": "TsKumgGohe8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Euclidean (default)\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test))\n",
        "\n",
        "# Manhattan\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Accuracy:\", acc_manhattan)\n"
      ],
      "metadata": {
        "id": "xDwYFDTyhwMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a KNN Classifier with different values of K and visualize decision boundaried\n"
      ],
      "metadata": {
        "id": "ZpbHbxhYhnPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Euclidean (default)\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test))\n",
        "\n",
        "# Manhattan\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Accuracy:\", acc_manhattan)\n"
      ],
      "metadata": {
        "id": "oXmhXTVuhqwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Apply Feature Scaling before training a KNN model and compare results with unscaled data"
      ],
      "metadata": {
        "id": "IR5hV6ItiGYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without Scaling\n",
        "knn_raw = KNeighborsClassifier()\n",
        "knn_raw.fit(X_train, y_train)\n",
        "acc_raw = accuracy_score(y_test, knn_raw.predict(X_test))\n",
        "\n",
        "# With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier()\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_raw)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)"
      ],
      "metadata": {
        "id": "Eyx0pjbbiM4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. 5 Train a PCA model on synthetic data and print the explained variance ratio for each component5"
      ],
      "metadata": {
        "id": "Lp2Mw_OXiSv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Create synthetic data\n",
        "X, _ = make_classification(n_samples=100, n_features=5, random_state=42)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"Explained variance ratio per component:\")\n",
        "print(pd.Series(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "b2DQLS_WiV5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA"
      ],
      "metadata": {
        "id": "4ggAysTBiah4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split & scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN without PCA\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_no_pca = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "# KNN with PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "knn_pca = KNeighborsClassifier()\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "print(f\"Accuracy without PCA: {acc_no_pca:.2f}\")\n",
        "print(f\"Accuracy with PCA: {acc_pca:.2f}\")\n"
      ],
      "metadata": {
        "id": "ah_N2EAtidoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV5"
      ],
      "metadata": {
        "id": "r3ihEalLijEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model performance\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Accuracy of best model: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "LJUoSqr-ioFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a KNN Classifier and check the number of misclassified samples5"
      ],
      "metadata": {
        "id": "K1lEKvIqivDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model performance\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Accuracy of best model: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "J1U62zNliy9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train a PCA model and visualize the cumulative explained variance."
      ],
      "metadata": {
        "id": "qHpS3H5Mi57T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Cumulative explained variance\n",
        "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', color='b', linestyle='--')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5te85KUFi_SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare\n",
        "accuracy"
      ],
      "metadata": {
        "id": "bW2l7ssnjGla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Classifier with uniform weights\n",
        "knn_uniform = KNeighborsClassifier(weights='uniform')\n",
        "knn_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# Train KNN Classifier with distance weights\n",
        "knn_distance = KNeighborsClassifier(weights='distance')\n",
        "knn_distance.fit(X_train, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test)\n",
        "accuracy_distance = accuracy_score(y_test, y_pred_distance)\n",
        "\n",
        "# Print results\n",
        "print(f'Accuracy with uniform weights: {accuracy_uniform:.4f}')\n",
        "print(f'Accuracy with distance weights: {accuracy_distance:.4f}')\n"
      ],
      "metadata": {
        "id": "nG7d7O5kjOGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a KNN Regressor and analyze the effect of different K values on performance"
      ],
      "metadata": {
        "id": "VcS-wWGajWlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of K values to test\n",
        "k_values = [1, 3, 5, 7, 9]\n",
        "mse_scores = []\n",
        "\n",
        "# Train KNN Regressor with different K values\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Print results\n",
        "for k, mse in zip(k_values, mse_scores):\n",
        "    print(f'K = {k}, Mean Squared Error: {mse:.4f}')\n"
      ],
      "metadata": {
        "id": "A95xo7nwjasK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Implement KNN Imputation for handling missing values in a dataset"
      ],
      "metadata": {
        "id": "asosCjDQjlDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of K values to test\n",
        "k_values = [1, 3, 5, 7, 9]\n",
        "mse_scores = []\n",
        "\n",
        "# Train KNN Regressor with different K values\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Print results\n",
        "for k, mse in zip(k_values, mse_scores):\n",
        "    print(f'K = {k}, Mean Squared Error: {mse:.4f}')\n"
      ],
      "metadata": {
        "id": "5uogjyNAjqui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a PCA model and visualize the data projection onto the first two principal components"
      ],
      "metadata": {
        "id": "8O3NESf9jz7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of K values to test\n",
        "k_values = [1, 3, 5, 7, 9]\n",
        "mse_scores = []\n",
        "\n",
        "# Train KNN Regressor with different K values\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Print results\n",
        "for k, mse in zip(k_values, mse_scores):\n",
        "    print(f'K = {k}, Mean Squared Error: {mse:.4f}')\n"
      ],
      "metadata": {
        "id": "a9doN1eXj6JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance"
      ],
      "metadata": {
        "id": "zXnt5v61j-xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Classifier using KD Tree\n",
        "knn_kd_tree = KNeighborsClassifier(n_neighbors=3, algorithm='kd_tree')\n",
        "knn_kd_tree.fit(X_train, y_train)\n",
        "y_pred_kd_tree = knn_kd_tree.predict(X_test)\n",
        "accuracy_kd_tree = accuracy_score(y_test, y_pred_kd_tree)\n",
        "\n",
        "# Train KNN Classifier using Ball Tree\n",
        "knn_ball_tree = KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree')\n",
        "knn_ball_tree.fit(X_train, y_train)\n",
        "y_pred_ball_tree = knn_ball_tree.predict(X_test)\n",
        "accuracy_ball_tree = accuracy_score(y_test, y_pred_ball_tree)\n",
        "\n",
        "# Compare performance\n",
        "print(f'Accuracy with KD Tree: {accuracy_kd_tree:.4f}')\n",
        "print(f'Accuracy with Ball Tree: {accuracy_ball_tree:.4f}')\n"
      ],
      "metadata": {
        "id": "FqAhN0v9kB48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot"
      ],
      "metadata": {
        "id": "wAZyLnQMlzJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a high-dimensional synthetic dataset (e.g., 100 features)\n",
        "X, _ = make_classification(n_samples=100, n_features=100, random_state=42)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Plot Scree plot (Explained variance ratio)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KKgndmUEl0pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score"
      ],
      "metadata": {
        "id": "j0F_NKILl7ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "dDG2_i-9mChK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a PCA model and analyze the effect of different numbers of components on accuracy"
      ],
      "metadata": {
        "id": "Y6thNbl2mDc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Analyze the effect of different numbers of PCA components\n",
        "accuracies = []\n",
        "components = [1, 2, 3, 4]\n",
        "\n",
        "for n_components in components:\n",
        "    # Apply PCA with n_components\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    # Train a KNN Classifier on transformed data\n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Print the results\n",
        "for n_components, accuracy in zip(components, accuracies):\n",
        "    print(f\"Accuracy with {n_components} PCA components: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "BFQr7ailmG3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a KNN Classifier with different leaf_size values and compare accuracy"
      ],
      "metadata": {
        "id": "LbHkwbpWmLhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of different leaf_size values\n",
        "leaf_sizes = [10, 20, 30, 40, 50]\n",
        "accuracies = []\n",
        "\n",
        "# Train KNN Classifier with different leaf_size values\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=3, leaf_size=leaf_size)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Print the results\n",
        "for leaf_size, accuracy in zip(leaf_sizes, accuracies):\n",
        "    print(f\"Accuracy with leaf_size={leaf_size}: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "XlWoERXvmOTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a PCA model and visualize how data points are transformed before and after PCA"
      ],
      "metadata": {
        "id": "5gtGFQjgmVWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Visualize data before PCA\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot data before PCA (First two features)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis')\n",
        "plt.title(\"Data Before PCA\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot data after PCA\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
        "plt.title(\"Data After PCA\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "riZ3dJcNmYC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report"
      ],
      "metadata": {
        "id": "r77dEoOFmeTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "xO5QUmT_mokz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42.  Train a KNN Regressor and analyze the effect of different distance metrics on prediction error"
      ],
      "metadata": {
        "id": "Sy3za3UUmpnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define distance metrics\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "# Initialize a dictionary to store errors\n",
        "errors = {}\n",
        "\n",
        "# Train KNN regressor with different distance metrics and calculate error\n",
        "for metric in metrics:\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    errors[metric] = mse\n",
        "\n",
        "# Print the errors\n",
        "for metric, error in errors.items():\n",
        "    print(f\"Mean Squared Error using {metric} distance: {error}\")\n"
      ],
      "metadata": {
        "id": "6m4abo5omsYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.  Train a KNN Classifier and evaluate using ROC-AUC score"
      ],
      "metadata": {
        "id": "73uXO3fWmwbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Binarize the output labels for ROC-AUC calculation\n",
        "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_pred_prob = knn.predict_proba(X_test)\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
        "\n",
        "print(f\"ROC-AUC score: {roc_auc}\")\n"
      ],
      "metadata": {
        "id": "5V-JWJKhmzWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44.  Train a PCA model and visualize the variance captured by each principal component"
      ],
      "metadata": {
        "id": "9n0k8lSTm5jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Get the explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot the variance captured by each principal component\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance)\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.title('Variance Captured by Each Principal Component')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DW_sLa3nm8Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a KNN Classifier and perform feature selection before training"
      ],
      "metadata": {
        "id": "w7GwaTynnAC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Perform feature selection using SelectKBest\n",
        "selector = SelectKBest(f_classif, k=2)  # Select the top 2 features\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate performance\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy of KNN Classifier after Feature Selection: {accuracy}')\n"
      ],
      "metadata": {
        "id": "xe03pb69nDtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46.  Train a PCA model and visualize the data reconstruction error after reducing dimensions"
      ],
      "metadata": {
        "id": "gyMqOF6CnIGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Reconstruct the data from the reduced dimensions\n",
        "X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "# Compute reconstruction error (Mean Squared Error)\n",
        "reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
        "\n",
        "# Plot original vs reconstructed data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Data\")\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=data.target, cmap='viridis')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Reconstructed Data\")\n",
        "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], c=data.target, cmap='viridis')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f'Reconstruction Error: {reconstruction_error}')\n"
      ],
      "metadata": {
        "id": "XhV0MHd3nMsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. Train a KNN Classifier and visualize the decision boundary"
      ],
      "metadata": {
        "id": "NFnCVYV9nR4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Reconstruct the data from the reduced dimensions\n",
        "X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "# Compute reconstruction error (Mean Squared Error)\n",
        "reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
        "\n",
        "# Plot original vs reconstructed data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Data\")\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=data.target, cmap='viridis')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Reconstructed Data\")\n",
        "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], c=data.target, cmap='viridis')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f'Reconstruction Error: {reconstruction_error}')\n"
      ],
      "metadata": {
        "id": "eHQ6Z1EGnVZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48.  Train a PCA model and analyze the effect of different numbers of components on data variance."
      ],
      "metadata": {
        "id": "GkLxdSTXncct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA model and analyze variance for different numbers of components\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Plot the explained variance ratio for each component\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.title(\"Cumulative Explained Variance vs Number of Components\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print the cumulative explained variance for each component\n",
        "print(f'Cumulative Explained Variance: {np.cumsum(pca.explained_variance_ratio_)}')\n"
      ],
      "metadata": {
        "id": "kpgbNAfFnfuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}