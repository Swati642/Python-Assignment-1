{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvPdMZ2R3mYqZ2DTW24TVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. - What is Simple Linear Regression\n",
        "Ans. Simple Linear Regression\n",
        "\n",
        "Definition:\n",
        "A method to model the linear relationship between one independent variable (X) and one dependent variable (Y).\n",
        "\n",
        "Equation:\n",
        "Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ\n",
        "\n",
        "Œ≤‚ÇÄ: Intercept\n",
        "\n",
        "Œ≤‚ÇÅ: Slope\n",
        "\n",
        "Œµ: Error term\n",
        "\n",
        "Goal:\n",
        "Predict Y based on X using a straight line.\n",
        "\n",
        "Example:\n",
        "Predict house price (Y) using size in sq ft (X)."
      ],
      "metadata": {
        "id": "B9eM0yaQCTK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression\n",
        "Assumptions of Simple Linear Regression\n",
        "\n",
        "Linearity\n",
        "\n",
        "The relationship between X and Y is linear.\n",
        "\n",
        "Independence\n",
        "\n",
        "Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "The variance of residuals (errors) is constant across all levels of X.\n",
        "\n",
        "Normality of Residuals\n",
        "\n",
        "The residuals (errors) are normally distributed.\n",
        "\n",
        "No or Little Multicollinearity (only relevant in multiple regression)\n",
        "\n",
        "Not applicable in simple linear regression as there's only one predictor."
      ],
      "metadata": {
        "id": "cUDrJMkTCp1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c\n",
        "ans. Meaning of\n",
        "ùëö\n",
        "m (Slope):\n",
        "\n",
        "It tells you how much Y changes for a one-unit increase in X.\n",
        "\n",
        "In other words:\n",
        "\n",
        "ùëö\n",
        "=\n",
        "Change in Y/Change in X\n",
        "m=\n",
        " Example: If ùëö=2\n",
        "it means:\n",
        "\n",
        "For every 1 unit increase in X, Y increases by 2 units"
      ],
      "metadata": {
        "id": "oyJp2rFTGLSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c\n",
        "Ans. Meaning of\n",
        "ùëê\n",
        "c (Intercept):\n",
        "\n",
        "It‚Äôs the point where the line crosses the Y-axis.\n",
        "\n",
        "It shows the baseline value of Y when there is no influence from X."
      ],
      "metadata": {
        "id": "NB1svufKCwWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How do we calculate the slope m in Simple Linear Regression\n"
      ],
      "metadata": {
        "id": "0XqKgD_lG8co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Formula to calculate slope (m) in Simple Linear Regression\n",
        "\n",
        "m = (n * sum(X * Y) - sum(X) * sum(Y)) / (n * sum(X**2) - (sum(X))**2)\n",
        "\n",
        "# or using covariance and variance:\n",
        "\n",
        "m = covariance(X, Y) / variance(X)"
      ],
      "metadata": {
        "id": "9k3nTqq9I15x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "Purpose of Least Squares Method in Simple Linear Regression:\n",
        "\n",
        "Minimize the error between the observed values (actual data points) and the predicted values (values on the regression line).\n",
        "\n",
        "It does this by minimizing the sum of squared residuals (errors).\n"
      ],
      "metadata": {
        "id": "AYLGAgBGI2gY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression.\n"
      ],
      "metadata": {
        "id": "SOFS2P1sI-q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretation of R-squared (Coefficient of Determination) in Simple Linear Regression\n",
        "\n",
        "# R-squared (R¬≤) measures the proportion of variance in the dependent variable (Y)\n",
        "# that is explained by the independent variable (X).\n",
        "\n",
        "# Value Range:\n",
        "# 0 <= R¬≤ <= 1\n",
        "# R¬≤ = 0: The model explains 0% of the variance in Y (no relationship).\n",
        "# R¬≤ = 1: The model explains 100% of the variance in Y (perfect relationship).\n",
        "\n",
        "# In simple terms:\n",
        "# - Higher R¬≤ means a better fit, i.e., more of the variance in Y is explained by X.\n",
        "# - Lower R¬≤ means the model doesn't explain much of the variability in Y."
      ],
      "metadata": {
        "id": "WxLhkxocJXJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression\u001d"
      ],
      "metadata": {
        "id": "MS19DDY6JX84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLR models the relationship between two or more independent variables (X)\n",
        "# and a dependent variable (Y).\n",
        "\n",
        "# Equation:\n",
        "# Y = Œ≤‚ÇÄ + Œ≤‚ÇÅ X‚ÇÅ + Œ≤‚ÇÇ X‚ÇÇ + ... + Œ≤n Xn + Œµ\n",
        "\n",
        "# Where:\n",
        "# Y = Dependent variable (response)\n",
        "# X‚ÇÅ, X‚ÇÇ, ..., Xn = Independent variables (predictors)\n",
        "# Œ≤‚ÇÄ = Intercept (value of Y when all X's are 0)\n",
        "# Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤n = Coefficients (how each X affects Y)\n",
        "# Œµ = Error term (residuals)"
      ],
      "metadata": {
        "id": "yUnUbmuBK5IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is the main difference between Simple and Multiple Linear Regression\n",
        "The main difference between Simple and Multiple Linear Regression is the number of independent variables used to predict the dependent variable:\n",
        "\n",
        "Simple Linear Regression: This involves only one independent variable (predictor) to model the relationship with the dependent variable. It's used when you want to understand how one factor affects another.\n",
        "\n",
        "Multiple Linear Regression: This involves two or more independent variables. It's used when you want to understand how multiple factors simultaneously affect the dependent variable. This model helps capture more complex relationships compared to simple linear regression.\n",
        "\n",
        "In essence, Simple Linear Regression is focused on a single predictor, while Multiple Linear Regression considers multiple predictors to make predictions."
      ],
      "metadata": {
        "id": "WdCSS3X3K9Nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression\n",
        "he key assumptions of Multiple Linear Regression are as follows:\n",
        "\n",
        "Linearity: There is a linear relationship between the independent variables and the dependent variable. This means that the relationship between predictors and the outcome is additive and linear.\n",
        "\n",
        "Independence of Errors: The residuals (errors) should be independent of each other. This assumption is crucial for the validity of hypothesis tests and confidence intervals.\n",
        "\n",
        "Homoscedasticity: The variance of the errors (residuals) should be constant across all levels of the independent variables. In other words, the spread of residuals should be roughly the same for all predicted values of the dependent variable.\n",
        "\n",
        "No Multicollinearity: The independent variables should not be highly correlated with each other. High correlation between predictors can lead to multicollinearity, which makes it difficult to estimate the individual effects of each predictor accurately.\n",
        "\n",
        "Normality of Errors: The residuals (errors) should be approximately normally distributed. This assumption is important for performing hypothesis tests and calculating confidence intervals for the coefficients.\n",
        "\n",
        "No Autocorrelation: The residuals should not show patterns or correlations with each other. This assumption is especially important in time series data, where errors might be correlated across time.\n",
        "\n",
        "Additivity: The effect of each independent variable on the dependent variable is assumed to be additive. This means the relationship is considered independently for each predictor variable, without interaction effects unless specified"
      ],
      "metadata": {
        "id": "dd4DCn6HM9cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "Heteroscedasticity refers to the condition where the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables.\n",
        "\n",
        "Effects on Multiple Linear Regression:\n",
        "Biased Standard Errors: Leads to inaccurate confidence intervals and hypothesis tests.\n",
        "\n",
        "Inefficient Estimates: Coefficients become less precise, though still unbiased.\n",
        "\n",
        "Violation of Assumptions: Heteroscedasticity violates the homoscedasticity assumption, affecting model validity.\n",
        "\n",
        "Model Misinterpretation: Residual plots may show patterns, indicating the model misses key data features.\n",
        "\n",
        "Solutions:\n",
        "Visualize Residuals: Plot residuals vs. predicted values.\n",
        "\n",
        "Use Tests: Apply Breusch-Pagan or White tests for detection.\n",
        "\n",
        "Transform Dependent Variable: Apply log or square root transformations.\n",
        "\n",
        "Use Robust Standard Errors: Correct for heteroscedasticity in statistical tests."
      ],
      "metadata": {
        "id": "o4uTGpGiNViO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "To improve a Multiple Linear Regression model with high multicollinearity, here are some strategies you can use:\n",
        "\n",
        "Remove Highly Correlated Predictors:\n",
        "\n",
        "Identify and remove one of the correlated variables to reduce multicollinearity. Use correlation matrices or Variance Inflation Factor (VIF) to detect high correlations between predictors.\n",
        "\n",
        "Combine Correlated Variables:\n",
        "\n",
        "If the correlated predictors are measuring similar concepts, consider combining them into a single composite variable (e.g., by averaging or summing the values).\n",
        "\n",
        "Principal Component Analysis (PCA):\n",
        "\n",
        "Use PCA to transform the correlated predictors into a smaller set of uncorrelated components that still explain most of the variance in the data.\n",
        "\n",
        "Ridge Regression (L2 Regularization):\n",
        "\n",
        "Apply ridge regression, which adds a penalty to the regression coefficients, shrinking them and reducing the impact of multicollinearity. This helps in stabilizing the model.\n",
        "\n",
        "Lasso Regression (L1 Regularization):\n",
        "\n",
        "Use lasso regression to perform both variable selection and regularization. It can zero out coefficients of less important predictors, helping reduce multicollinearity.\n",
        "\n",
        "Elastic Net Regression:\n",
        "\n",
        "Combine the advantages of ridge and lasso regression using elastic net. It applies both L1 and L2 regularization to handle multicollinearity while selecting relevant predictors.\n",
        "\n",
        "Centering or Standardizing Variables:\n",
        "\n",
        "If multicollinearity is caused by variables with different scales, standardize or center the variables (subtract the mean) to reduce the issue.\n",
        "\n",
        "By applying these techniques, you can mitigate the effects of multicollinearity and improve the performance and interpretability of your multiple linear regression model."
      ],
      "metadata": {
        "id": "vpMcL_kENw_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What are some common techniques for transforming categorical variables for use in regression models\n",
        "Here are common techniques for transforming categorical variables for regression models:\n",
        "\n",
        "One-Hot Encoding: Converts each category into a binary column (0 or 1). Best for nominal variables.\n",
        "\n",
        "Label Encoding: Assigns a unique integer to each category. Useful for ordinal variables.\n",
        "\n",
        "Ordinal Encoding: Similar to label encoding but used for variables with a natural order (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "Binary Encoding: Converts category integers into binary code, reducing dimensionality. Useful for high-cardinality variables.\n",
        "\n",
        "Frequency/Count Encoding: Replaces categories with their frequency or count in the dataset.\n",
        "\n",
        "Target Encoding: Replaces categories with the mean of the target variable for that category. Useful for high-cardinality categories.\n",
        "\n",
        "Hashing (Feature Hashing): Maps categories to a fixed number of columns using hash functions. Useful for large categories.\n",
        "\n",
        "Custom Encoding: Domain-specific transformations based on the nature of the data."
      ],
      "metadata": {
        "id": "EHLGax5uOBBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression\n",
        "Ans. Interaction terms in Multiple Linear Regression represent the combined effect of two or more independent variables on the dependent variable. These terms allow you to capture non-additive relationships between predictors, meaning that the effect of one predictor may depend on the level of another predictor.\n",
        "\n",
        "Role of Interaction Terms:\n",
        "Capture Synergy or Combined Effects:\n",
        "\n",
        "Interaction terms help to model situations where the effect of one predictor on the outcome is different depending on the value of another predictor.\n",
        "\n",
        "For example, the effect of education on income might vary depending on work experience. The interaction term between education and experience allows the model to account for this combined effect.\n",
        "\n",
        "Improve Model Fit:\n",
        "\n",
        "By including interaction terms, you can improve the accuracy of the model by better fitting complex relationships between variables. This leads to more precise predictions.\n",
        "\n",
        "Reveal Hidden Relationships:\n",
        "\n",
        "Sometimes, the relationship between predictors and the dependent variable is not purely linear. Interaction terms can uncover these hidden relationships that a simple linear model might miss.\n",
        "\n",
        "Increase Model Complexity:\n",
        "\n",
        "While adding interaction terms increases model complexity, it helps to explain more nuanced behaviors that a standard linear relationship might not capture. However, too many interactions can lead to overfitting, so it‚Äôs essential to add only meaningful interactions."
      ],
      "metadata": {
        "id": "I04AY7mAOwc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "# Simple Linear Regression\n",
        "# Model: Y = Œ≤0 + Œ≤1 * X + Œµ\n",
        "# Interpretation:\n",
        "# Œ≤0 is the predicted value of Y when X = 0.\n",
        "\n",
        "# Example:\n",
        "# If X = years of experience, and Y = salary,\n",
        "# then Œ≤0 is the salary when years of experience is 0.\n",
        "\n",
        "# --------------------------------------------------\n",
        "\n",
        "# Multiple Linear Regression\n",
        "# Model: Y = Œ≤0 + Œ≤1 * X1 + Œ≤2 * X2 + ... + Œ≤n * Xn + Œµ\n",
        "# Interpretation:\n",
        "# Œ≤0 is the predicted value of Y when all X1, X2, ..., Xn = 0.\n",
        "\n",
        "# Example:\n",
        "# If X1 = years of experience, X2 = education level,\n",
        "# then Œ≤0 is the salary when both experience and education are 0.\n",
        "\n",
        "# Note:\n",
        "# In multiple regression, Œ≤0 might not always have a practical meaning,\n",
        "# especially if the combination of all predictors being 0 is unrealistic."
      ],
      "metadata": {
        "id": "LuF5KuxRPO55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions"
      ],
      "metadata": {
        "id": "oG6LHP43Q_Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope in regression analysis represents the relationship between an independent variable and the dependent variable."
      ],
      "metadata": {
        "id": "BVMNUOGiRGli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General Regression Model\n",
        "Y = Œ≤0 + Œ≤1 * X + Œµ"
      ],
      "metadata": {
        "id": "AeP4EthKRrpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Œ≤1 (slope) tells you how much Y is expected to change when X increases by one unit, assuming all other variables are held constant (in multiple regression).\n",
        "\n",
        " How It Affects Predictions:\n",
        "Positive Slope (Œ≤1 > 0):\n",
        "\n",
        "As X increases, Y increases.\n",
        "\n",
        "Example: More hours studied ‚Üí higher exam score.\n",
        "\n",
        "Negative Slope (Œ≤1 < 0):\n",
        "\n",
        "As X increases, Y decreases.\n",
        "\n",
        "Example: More distance driven ‚Üí less fuel in tank.\n",
        "\n",
        "Zero Slope (Œ≤1 = 0):\n",
        "\n",
        "No relationship between X and Y.\n",
        "\n",
        "Changing X does not affect Y."
      ],
      "metadata": {
        "id": "8Ftdt-0ARsuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.How does the intercept in a regression model provide context for the relationship between variables\n",
        "The intercept in a regression model provides the baseline value of the dependent variable when all independent variables are zero. It helps set the starting point of the regression line and gives context to how other variables affect the outcome."
      ],
      "metadata": {
        "id": "eYu8RKT6R2yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tVTjKaKHR76R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "Y = Œ≤0 + Œ≤1 * X1 + Œ≤2 * X2 + ... + Œ≤n * Xn + Œµ"
      ],
      "metadata": {
        "id": "j4p5dupxSBlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Œ≤0 (intercept) is the predicted value of Y when all predictors (X1, X2, ..., Xn) are zero.\n",
        "\n",
        " Why It Matters:\n",
        "\n",
        "It shows where the regression line crosses the Y-axis.\n",
        "\n",
        "Helps understand how the outcome starts before the influence of any predictors.\n",
        "\n",
        "Interpretation Context:\n",
        "\n",
        "Provides a foundation for interpreting the slopes.\n",
        "\n",
        "Predictions are made relative to the intercept.\n",
        "\n",
        "Practical Insight:\n",
        "\n",
        "In some cases, the intercept has real meaning (e.g., starting salary with 0 years of experience).\n",
        "\n",
        "In others, it may be meaningless if \"all predictors = 0\" is not realistic (e.g., 0 age, 0 income, etc.)"
      ],
      "metadata": {
        "id": "j3fRHJ42SEOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R¬≤ as a sole measure of model performance\n",
        "Using R¬≤ (R-squared) as the sole measure of model performance has several limitations.\n",
        "Limitations of R¬≤:\n",
        "Does Not Indicate Causation\n",
        "\n",
        "A high R¬≤ shows correlation, not causation between variables.\n",
        "\n",
        "Sensitive to Overfitting\n",
        "\n",
        "R¬≤ always increases when more variables are added‚Äîeven if they are irrelevant.\n",
        "\n",
        "Ignores Model Complexity\n",
        "\n",
        "It doesn‚Äôt penalize for adding too many predictors. A complex model may look better just because of higher R¬≤.\n",
        "\n",
        "Not Useful for Nonlinear Models\n",
        "\n",
        "In nonlinear regression or other complex models, R¬≤ may mislead or be meaningless.\n",
        "\n",
        "No Info on Residuals\n",
        "\n",
        "It doesn‚Äôt show if the residuals are randomly distributed, which is key for a good model.\n",
        "\n",
        "Doesn‚Äôt Assess Predictive Power on New Data\n",
        "\n",
        "A high R¬≤ on training data doesn‚Äôt guarantee good generalization to unseen data.\n",
        "\n",
        "Can Be Misleading in Low-Variance Data\n",
        "\n",
        "If the variance in the target is small, even a bad model can have a high R¬≤."
      ],
      "metadata": {
        "id": "xWqCwMXaSMBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.- How would you interpret a large standard error for a regression coefficient\n",
        "A large standard error for a regression coefficient indicates low precision in the estimate of that coefficient. Here's a crisp interpretation:\n",
        "\n",
        "Interpretation of Large Standard Error:\n",
        "Uncertainty in Coefficient\n",
        "\n",
        "The coefficient is not reliably estimated; it could vary a lot with different data samples.\n",
        "\n",
        "Low Statistical Significance\n",
        "\n",
        "A large standard error increases the p-value, making it less likely that the coefficient is significantly different from zero.\n",
        "\n",
        "Possible Multicollinearity\n",
        "\n",
        "Often caused by high correlation between independent variables, making it hard to isolate each variable‚Äôs effect.\n",
        "\n",
        "Weak Predictor\n",
        "\n",
        "The variable may have little or no real influence on the dependent variable.\n",
        "\n",
        "Impact on Confidence Interval\n",
        "\n",
        "Leads to wide confidence intervals, meaning less confidence in the estimated effect size.\n"
      ],
      "metadata": {
        "id": "jisbc6BCSj06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XA3zNDdWCU18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. - How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "How to Identify Heteroscedasticity in Residual Plots:\n",
        "In a residual vs. fitted values plot, heteroscedasticity appears as:\n",
        "\n",
        "Funnel Shape: Residuals spread out or narrow as fitted values increase.\n",
        "\n",
        "Patterned Spread: Residuals show increasing or decreasing variance instead of random scatter.\n",
        "\n",
        "Non-constant Spread: Residuals don‚Äôt have a uniform \"cloud\" around the zero line.\n",
        "\n",
        "Why It's Important to Address Heteroscedasticity:\n",
        "Violates Regression Assumptions\n",
        "\n",
        "Multiple Linear Regression assumes constant variance of residuals (homoscedasticity). Violation affects model reliability.\n",
        "\n",
        "Biased Standard Errors\n",
        "\n",
        "Leads to inaccurate p-values and confidence intervals, causing misleading conclusions about coefficient significance.\n",
        "\n",
        "Impacts Predictive Accuracy\n",
        "\n",
        "Model may perform poorly on new or extreme values.\n",
        "\n",
        "Increases Risk of Type I/II Errors\n",
        "\n",
        "Can lead to false positives or negatives in hypothesis testing."
      ],
      "metadata": {
        "id": "9TznMWHXTcpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤\n",
        "When a Multiple Linear Regression model shows a high R¬≤ but a low Adjusted R¬≤, it suggests that although the model appears to explain a large portion of the variance in the dependent variable, this explanation might not be as meaningful as it seems. R¬≤ always increases or remains the same when more predictors are added to the model, even if those predictors do not actually contribute useful information. This can give a false impression that the model is improving, simply because it has more variables.\n",
        "\n",
        "However, Adjusted R¬≤ is designed to counter this by adjusting for the number of predictors in the model. It increases only if the new predictors genuinely improve the model's explanatory power. Therefore, if Adjusted R¬≤ is significantly lower than R¬≤, it usually means that some of the predictors included are not relevant and may be introducing noise rather than insight. This is a sign of overfitting ‚Äî where the model fits the training data very well but may not generalize effectively to new or unseen data.\n",
        "\n",
        "In essence, a high R¬≤ alongside a low Adjusted R¬≤ indicates that the model might be overly complex without providing meaningful improvements in prediction. It serves as a warning to simplify the model by removing redundant or non-informative predictors, ensuring that only truly impactful variables remain. This helps create a more robust and generalizable model."
      ],
      "metadata": {
        "id": "vCja7pp3T32O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression\n",
        "Scaling variables in Multiple Linear Regression is important because it ensures that all predictors contribute to the model fairly, especially when they are on different scales. In regression, the algorithm calculates coefficients to minimize the difference between predicted and actual values, and the scale of the variables can influence this optimization process.\n",
        "\n",
        "If one predictor has values in the thousands (like income) and another has values between 0 and 1 (like probability or ratios), the larger-scaled variable might dominate the learning process. This doesn‚Äôt necessarily mean it has more predictive power, but it can distort the interpretation of the coefficients and the model‚Äôs behavior, especially in the presence of regularization techniques like Ridge or Lasso regression. These methods penalize large coefficients, and unscaled data can lead to biased penalization simply due to differences in units.\n",
        "\n",
        "Scaling also makes the coefficients more interpretable when comparing the relative importance of predictors. Without scaling, a large coefficient might just reflect the large unit size of a variable, not its actual influence on the dependent variable.\n",
        "\n",
        "In summary, scaling brings all variables to a common range, reduces bias caused by differing magnitudes, and improves both the interpretability and numerical stability of the model."
      ],
      "metadata": {
        "id": "OveC4YUNU-XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression"
      ],
      "metadata": {
        "id": "TZBCb_MVVHUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "Unlike linear regression, which assumes a straight-line (linear) relationship, polynomial regression can model non-linear curves by including higher-order terms"
      ],
      "metadata": {
        "id": "PWkqVD31VQLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = Œ≤‚ÇÄ + Œ≤‚ÇÅ * X + Œµ"
      ],
      "metadata": {
        "id": "qApsY7CWVbug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression"
      ],
      "metadata": {
        "id": "_rR8osbNVfll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Relationship Type:\n",
        "Linear Regression assumes a straight-line relationship between the independent variable(s) and the dependent variable:\n",
        "\n",
        "Polynomial Regression models a non-linear relationship by including higher-order terms\n",
        "Model Flexibility:\n",
        "Linear Regression is less flexible because it can only fit straight lines (i.e., it can‚Äôt model curvatures or bends).\n",
        "\n",
        "Polynomial Regression is more flexible and can fit curved or nonlinear patterns, making it more suitable for datasets with non-linear trends.\n",
        "\n",
        "3. Complexity:\n",
        "Linear Regression is simpler and has fewer parameters to estimate.\n",
        "\n",
        "Polynomial Regression introduces more parameters (higher-degree terms), increasing its complexity. This allows for a more nuanced fit but may also lead to overfitting if the degree of the polynomial is too high.\n",
        "\n",
        "25. When is polynomial regression used\n",
        "Polynomial regression is used when the relationship between the independent and dependent variables is non-linear, but you still want to use a regression model to capture this relationship. It is particularly useful when the data shows patterns or trends that cannot be accurately modeled by a simple straight line (linear regression). Here are some specific situations when polynomial regression is commonly applied:\n",
        "\n"
      ],
      "metadata": {
        "id": "rlwN9D3WVj9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression\n",
        "For single predictor"
      ],
      "metadata": {
        "id": "bdVDmQYPV7YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = Œ≤0 + Œ≤1 * X + Œ≤2 * X^2 + Œ≤3 * X^3 + ... + Œ≤n * X^n + Œµ"
      ],
      "metadata": {
        "id": "g8E5taJ3WMNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For multiple predictors"
      ],
      "metadata": {
        "id": "vkSOwc9BWO7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = Œ≤0 + Œ≤1 * X1 + Œ≤2 * X2 + ... + Œ≤p * Xp\n",
        "    + Œ≤(p+1) * X1^2 + Œ≤(p+2) * X1 * X2 + ... + Œ≤n * Xp^2 + Œµ"
      ],
      "metadata": {
        "id": "jX4K_mlfWUFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "27. Can polynomial regression be applied to multiple variables\n",
        "Yes, polynomial regression can be applied to multiple variables. In this case, the model not only includes higher powers of individual variables (like\n",
        "but also interaction terms between the variables This allows the model to capture more complex, non-linear relationships between the predictors and the dependent variable. However, it's important to be cautious of overfitting when using higher-degree polynomials, especially with many variables."
      ],
      "metadata": {
        "id": "KDhp83XLWXlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression"
      ],
      "metadata": {
        "id": "cYvl4s70W2M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression, while powerful for modeling non-linear relationships, has several limitations:\n",
        "\n",
        "Overfitting: As the degree of the polynomial increases, the model becomes more flexible and can fit the training data very well, but this may lead to overfitting. The model may perform poorly on unseen data because it becomes too sensitive to noise in the training set.\n",
        "\n",
        "Complexity: Higher-degree polynomials can quickly become very complex, making the model harder to interpret. It might capture intricate patterns that are not meaningful in real-world contexts.\n",
        "\n",
        "Extrapolation Issues: Polynomial regression can perform poorly when making predictions for values of the independent variable outside the range of the training data, especially for high-degree polynomials, where predictions can become unreasonable or erratic.\n",
        "\n",
        "Multicollinearity: Adding higher-order terms can introduce multicollinearity (correlation between predictors), which may affect the stability and interpretation of the model coefficients.\n",
        "\n",
        "Sensitive to Outliers: Polynomial regression can be highly sensitive to outliers, especially in higher-degree models, as the curve can be distorted to accommodate extreme values.\n",
        "\n",
        "Model Selection: Choosing the optimal degree for the polynomial is challenging. Too high a degree can lead to overfitting, while too low a degree might not capture the underlying pattern in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "vF15iWSXW5WO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "When selecting the degree of a polynomial for regression, it's important to evaluate the model fit to ensure that the chosen degree balances fit accuracy and model complexity. Here are some methods to help evaluate model fit:\n",
        "\n",
        "Cross-Validation:\n",
        "\n",
        "K-fold cross-validation helps assess how well the model generalizes to unseen data by dividing the dataset into K subsets. The model is trained on K-1 subsets and tested on the remaining subset, repeated for all K subsets.\n",
        "\n",
        "This helps prevent overfitting by showing how the model performs on data not seen during training.\n",
        "\n",
        "Adjusted R¬≤:\n",
        "\n",
        "While R¬≤ measures how much variance in the dependent variable is explained by the independent variables, Adjusted R¬≤ accounts for the number of predictors in the model. As you increase the polynomial degree, R¬≤ naturally increases, but Adjusted R¬≤ penalizes the addition of unnecessary variables, helping to avoid overfitting.\n",
        "\n",
        "A decrease in Adjusted R¬≤ when increasing the polynomial degree indicates that the added complexity is not improving the model.\n",
        "\n",
        "AIC (Akaike Information Criterion) / BIC (Bayesian Information Criterion):\n",
        "\n",
        "Both AIC and BIC are used to compare models while penalizing for added complexity (higher degrees of polynomials).\n",
        "\n",
        "A lower AIC or BIC indicates a better model, balancing goodness-of-fit and model simplicity.\n",
        "\n",
        "Residual Analysis:\n",
        "\n",
        "Residual plots can help check for overfitting. A well-fitting model will have residuals that are randomly scattered around zero. If residuals show a clear pattern, it might indicate underfitting or overfitting.\n",
        "\n",
        "Checking residuals for heteroscedasticity (unequal variance) is also important."
      ],
      "metadata": {
        "id": "-PKLrgmIXA8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression\n",
        "Visualization plays a crucial role in polynomial regression for several reasons:\n",
        "\n",
        "Understanding the Data and Model Fit:\n",
        "\n",
        "Visualization helps to see how well the polynomial regression model fits the data. By plotting the actual data points and the fitted polynomial curve, you can quickly identify whether the model is capturing the underlying pattern accurately. This is especially important in polynomial regression, where higher-degree polynomials can introduce excessive flexibility that may lead to overfitting.\n",
        "\n",
        "Identifying Overfitting or Underfitting:\n",
        "\n",
        "Visualizing the model‚Äôs fit allows you to detect issues like overfitting (where the model fits the noise in the data) or underfitting (where the model does not capture the true pattern). Overfitting may show the curve becoming too wiggly, while underfitting would show a straight line or too simplistic a curve that fails to capture key trends.\n",
        "\n",
        "Selecting the Degree of the Polynomial:\n",
        "\n",
        "Visualization makes it easier to choose the right polynomial degree. As you increase the degree of the polynomial, you can observe how the curve evolves and whether it begins to overfit or still captures the main trend in the data. A degree that fits the data well without excessive complexity can be identified through visual inspection.\n",
        "\n",
        "Visualizing Residuals:\n",
        "\n",
        "Plotting residuals (the differences between predicted and actual values) is an important diagnostic tool. A good model will have residuals that are randomly scattered around zero. Visualization of residuals can help identify problems like heteroscedasticity (non-constant variance) or model misspecification."
      ],
      "metadata": {
        "id": "mpEFsLuBaNYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python\n",
        "Implementing polynomial regression in Python typically involves using libraries like NumPy for numerical computations and scikit-learn for building the regression model.\n",
        "\n",
        "Steps for Implementing Polynomial Regression:\n",
        "Import Required Libraries:\n",
        "\n",
        "Use NumPy for handling arrays and creating polynomial features.\n",
        "\n",
        "Use scikit-learn for fitting the polynomial regression model.\n",
        "\n",
        "Matplotlib can be used for visualization.\n",
        "\n",
        "Prepare the Data:\n",
        "\n",
        "You need a dataset with both independent variables (predictors) and a dependent variable (target).\n",
        "\n",
        "Create Polynomial Features:\n",
        "\n",
        "Use PolynomialFeatures from sklearn.preprocessing to transform the input data into polynomial features\n",
        "\n",
        "Fit the Polynomial Regression Model:\n",
        "\n",
        "Use LinearRegression from sklearn.linear_model to fit the polynomial features.\n",
        "\n",
        "Visualize the Results (optional but recommended):\n",
        "\n",
        "Use Matplotlib to plot the original data and the fitted polynomial curve."
      ],
      "metadata": {
        "id": "HKA_Nys5aZWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Step 2: Prepare the dataset (Example: Simple dataset)\n",
        "X = np.array([[1], [2], [3], [4], [5]])  # Independent variable\n",
        "y = np.array([1, 4, 9, 16, 25])  # Dependent variable (y = x^2)\n",
        "\n",
        "# Step 3: Create polynomial features (degree 2 for quadratic)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)  # Transform X to include X^2\n",
        "\n",
        "# Step 4: Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Step 5: Predict values using the polynomial regression model\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Step 6: Visualize the results\n",
        "plt.scatter(X, y, color='blue')  # Original data points\n",
        "plt.plot(X, y_pred, color='red')  # Polynomial regression curve\n",
        "plt.title('Polynomial Regression (Degree = 2)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g7mY4pwvao-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lv1gCNBVapu1"
      }
    }
  ]
}