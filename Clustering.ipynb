{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOtPhMN0cW+yKTTL2mQDg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is unsupervised learning in the context of machine learning\n",
        "Unsupervised learning is a type of machine learning where the model is trained on data that doesn't have labeled responses. The goal is to identify underlying patterns, structures, or relationships in the data. Common techniques include clustering (e.g., K-means) and dimensionality reduction (e.g., PCA). The model tries to find similarities or groupings within the data without specific guidance on what the output should be."
      ],
      "metadata": {
        "id": "9EdbIM8ho6Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. K-Means clustering works by partitioning data into a predefined number of clusters (K) based on feature similarity. Here's how it works:\n",
        "\n",
        "1. **Initialization**: Select K random data points as cluster centroids.\n",
        "2. **Assignment Step**: Assign each data point to the nearest centroid based on distance (e.g., Euclidean distance).\n",
        "3. **Update Step**: Calculate new centroids by averaging all points assigned to each centroid's cluster.\n",
        "4. **Repeat**: Repeat the assignment and update steps until the centroids no longer change (convergence).\n",
        "\n",
        "This algorithm minimizes the variance within clusters, making the data points in each cluster as similar as possible."
      ],
      "metadata": {
        "id": "NthEJtPYpGWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of a dendrogram in hierarchical clustering\n",
        "A **dendrogram** in hierarchical clustering is a tree-like diagram that visually represents the merging process of clusters. It shows how data points are grouped into clusters step-by-step based on their similarity.\n",
        "\n",
        "- **X-axis**: Represents the data points or clusters.\n",
        "- **Y-axis**: Represents the dissimilarity or distance at which clusters are merged.\n",
        "\n",
        "### Key Points:\n",
        "1. **Bottom**: Individual data points start as separate clusters.\n",
        "2. **Branches**: Data points or clusters are progressively merged into larger clusters as the algorithm moves upwards.\n",
        "3. **Height**: The height of the branches indicates the distance (or dissimilarity) at which two clusters are joined.\n",
        "\n",
        "You can decide the number of clusters by \"cutting\" the dendrogram at a certain height. This helps identify how many clusters are formed based on a chosen level of similarity."
      ],
      "metadata": {
        "id": "b8VK1ipOpxJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the main difference between K-Means and Hierarchical Clustering\n",
        "The main difference between **K-Means** and **Hierarchical Clustering** lies in their approach to forming clusters:\n",
        "\n",
        "1. **K-Means Clustering**:\n",
        "   - **Type**: Partitional.\n",
        "   - **Approach**: Requires the number of clusters (K) to be predefined. It iteratively assigns data points to clusters and updates cluster centroids.\n",
        "   - **Merge/Split**: It doesn't form a hierarchy and doesn't provide a hierarchy of clusters.\n",
        "   - **Efficiency**: Works well with large datasets but sensitive to the initial choice of centroids.\n",
        "\n",
        "2. **Hierarchical Clustering**:\n",
        "   - **Type**: Agglomerative or Divisive.\n",
        "   - **Approach**: Builds a hierarchy of clusters either by merging smaller clusters (agglomerative) or splitting a larger cluster (divisive).\n",
        "   - **Merge/Split**: Produces a dendrogram that shows the hierarchical relationships between clusters.\n",
        "   - **Efficiency**: Computationally expensive, especially for large datasets.\n",
        "\n",
        "In summary, **K-Means** requires the number of clusters in advance and is fast, while **Hierarchical Clustering** produces a hierarchy and does not require predefined clusters, but is more computationally intensive."
      ],
      "metadata": {
        "id": "yLAhlUwPqD01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are the advantages of DBSCAN over K-Means\n",
        "**Advantages of DBSCAN over K-Means**:\n",
        "\n",
        "1. **No Need to Predefine Number of Clusters**: Unlike K-Means, DBSCAN does not require specifying the number of clusters (K) in advance. It can automatically determine the number of clusters based on the density of data points.\n",
        "\n",
        "2. **Can Handle Non-Spherical Clusters**: DBSCAN is capable of identifying clusters of arbitrary shapes, while K-Means typically works best with spherical (or circular) clusters.\n",
        "\n",
        "3. **Handles Noise and Outliers**: DBSCAN can identify noise points as outliers and does not force them into any cluster, unlike K-Means, which assigns every data point to a cluster, even if it doesn't fit well.\n",
        "\n",
        "4. **Works Well with Unevenly Sized Clusters**: DBSCAN can discover clusters of different densities, whereas K-Means struggles when clusters have varying sizes and densities.\n",
        "\n",
        "5. **No Need for Distance to Centroid**: K-Means uses a centroid-based approach, which can lead to problems if clusters are of irregular shape. DBSCAN uses density, making it more robust to such issues.\n",
        "\n",
        "In summary, DBSCAN is more flexible for complex data structures, particularly in the presence of noise and varying cluster shapes."
      ],
      "metadata": {
        "id": "xUPWivNFqYqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-77ZyNdaqGj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. When would you use Silhouette Score in clustering\n",
        "The **Silhouette Score** is used in clustering to evaluate the quality of the clusters. It is particularly useful in the following scenarios:\n",
        "\n",
        "1. **Determine Optimal Number of Clusters**: The Silhouette Score helps in selecting the best number of clusters (K) by evaluating how well-separated the clusters are. A higher silhouette score indicates better-defined clusters.\n",
        "\n",
        "2. **Assess Cluster Quality**: After performing clustering, the Silhouette Score can be used to measure how similar each data point is to its own cluster compared to other clusters. Scores close to +1 indicate well-separated clusters, while scores close to -1 indicate poor clustering.\n",
        "\n",
        "3. **Compare Different Clustering Algorithms**: It can be used to compare different clustering algorithms (e.g., K-Means vs DBSCAN) and their performance on the same dataset, helping to select the best model.\n",
        "\n",
        "4. **Identifying Potential Outliers**: Points with a low or negative silhouette score can be considered as outliers or misclassified, providing insight into the clustering quality.\n",
        "\n",
        "In summary, the Silhouette Score is a valuable metric for assessing cluster cohesion, separation, and identifying the optimal number of clusters."
      ],
      "metadata": {
        "id": "bKfjZGh9qjec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What are the limitations of Hierarchical Clustering\n",
        "The limitations of **Hierarchical Clustering** are:\n",
        "\n",
        "1. **Scalability**: Hierarchical clustering is computationally expensive with a time complexity of \\(O(n^2)\\), making it inefficient for large datasets.\n",
        "\n",
        "2. **Sensitivity to Noise**: It can be sensitive to outliers, which may distort the clustering structure and lead to incorrect results.\n",
        "\n",
        "3. **Difficulty in Deciding Number of Clusters**: Although the dendrogram provides a visual representation, selecting the right number of clusters is subjective and can be challenging.\n",
        "\n",
        "4. **Non-flexible Cluster Shapes**: It works best with spherical clusters, and struggles with non-convex or irregularly shaped clusters.\n",
        "\n",
        "5. **Once Merged, Cannot Be Unmerged**: Once two clusters are merged, they cannot be undone, which might limit the flexibility in adjusting clustering results.\n",
        "\n",
        "These factors can make hierarchical clustering less practical for large, noisy, or complex datasets."
      ],
      "metadata": {
        "id": "762Sapafqr8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Why is feature scaling important in clustering algorithms like K-Means\n",
        "Feature scaling is important in clustering algorithms like **K-Means** because:\n",
        "\n",
        "1. **Distance Sensitivity**: K-Means relies on calculating distances (typically Euclidean) between data points. Features with larger ranges or units dominate the distance calculation, which can distort the clustering results.\n",
        "\n",
        "2. **Equal Weightage**: Scaling ensures that all features contribute equally to the distance calculation, avoiding any single feature from disproportionately influencing the clusters.\n",
        "\n",
        "3. **Improved Convergence**: Scaled data can help the K-Means algorithm converge faster and more reliably, leading to more accurate clustering.\n",
        "\n",
        "Common techniques for feature scaling include **Standardization** (zero mean, unit variance) and **Normalization** (scaling to a fixed range)."
      ],
      "metadata": {
        "id": "WSqhx_OMq4q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does DBSCAN identify noise points\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifies noise points based on its density-based clustering approach. Here's how it works:\n",
        "\n",
        "1. **Core Points**: A point is considered a core point if it has a minimum number of neighbors (defined by the `min_samples` parameter) within a given radius (`eps`).\n",
        "\n",
        "2. **Border Points**: A point that is not a core point but lies within the `eps` distance of a core point is called a border point.\n",
        "\n",
        "3. **Noise Points**: Any point that is neither a core point nor a border point is labeled as **noise**. These points don't belong to any cluster and are considered outliers.\n",
        "\n",
        "Thus, DBSCAN effectively separates dense regions as clusters and labels sparsely populated regions as noise."
      ],
      "metadata": {
        "id": "tuvgdTq5q_jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Define inertia in the context of K-Means\n",
        "Apologies for the confusion! Here's the formula for **inertia** in K-Means clustering, written mathematically:\n",
        "\n",
        "\\[\n",
        "\\text{Inertia} = \\sum_{i=1}^{k} \\sum_{x_j \\in C_i} \\|x_j - \\mu_i\\|^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( k \\) is the number of clusters,\n",
        "- \\( C_i \\) is the set of points assigned to the \\( i \\)-th cluster,\n",
        "- \\( x_j \\) represents each point in the cluster \\( C_i \\),\n",
        "- \\( \\mu_i \\) is the centroid (mean) of the \\( i \\)-th cluster,\n",
        "- \\( \\|x_j - \\mu_i\\|^2 \\) is the squared Euclidean distance between the point \\( x_j \\) and its assigned centroid \\( \\mu_i \\).\n",
        "\n",
        "This formula computes the sum of squared distances between each point and its assigned cluster centroid, which is used to measure the compactness of the clusters."
      ],
      "metadata": {
        "id": "nS2VTGcrrbCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the elbow method in K-Means clustering\n",
        "The **Elbow Method** in K-Means clustering is used to determine the optimal number of clusters, \\( k \\), for a given dataset. Here's how it works:\n",
        "\n",
        "1. **Run K-Means clustering** for a range of values for \\( k \\) (e.g., from 1 to 10).\n",
        "2. **Compute the inertia (within-cluster sum of squares)** for each value of \\( k \\). This represents the compactness of the clusters.\n",
        "3. **Plot the inertia** against the number of clusters \\( k \\).\n",
        "4. **Look for the \"elbow\" point** in the plot, which is where the inertia starts decreasing at a slower rate. This point indicates the optimal number of clusters.\n",
        "\n",
        "The **elbow point** represents the value of \\( k \\) where adding more clusters no longer significantly improves the compactness of the clusters, thus balancing between the model's complexity and performance."
      ],
      "metadata": {
        "id": "-OpNBnm8seAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Describe the concept of \"density\" in DBSCAN\n",
        "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), **density** refers to the number of data points within a specific region around a given data point, determined by two parameters: **ε (epsilon)** and **MinPts**.\n",
        "\n",
        "1. **ε (epsilon)**: This is the radius that defines the neighborhood around a point. It determines the maximum distance between two points to be considered neighbors.\n",
        "\n",
        "2. **MinPts**: This is the minimum number of points required to form a dense region (cluster). A point is considered a core point if it has at least **MinPts** points (including itself) within its ε-neighborhood.\n",
        "\n",
        "DBSCAN classifies points based on their density:\n",
        "\n",
        "- **Core Points**: Points that have at least **MinPts** points within their ε-neighborhood. These are the central points of a cluster.\n",
        "  \n",
        "- **Border Points**: Points that are not core points but lie within the ε-neighborhood of a core point. They can belong to a cluster but are not the center of it.\n",
        "\n",
        "- **Noise Points (Outliers)**: Points that do not meet the density criteria (i.e., they don't have enough neighbors to be classified as core or border points). These points are considered noise and are excluded from any clusters.\n",
        "\n",
        "DBSCAN's ability to find clusters of arbitrary shapes and handle noise is a direct result of its density-based approach."
      ],
      "metadata": {
        "id": "980I5w_6smaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "13. Can hierarchical clustering be used on categorical data"
      ],
      "metadata": {
        "id": "na5EYug9spUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, **hierarchical clustering** can be used on categorical data, but it requires a suitable distance metric to handle categorical variables. Traditional distance metrics like **Euclidean distance** are not appropriate for categorical data. Instead, you can use specialized metrics such as:\n",
        "\n",
        "1. **Hamming Distance**: This is used to calculate the distance between two categorical data points by counting the number of mismatches between corresponding attributes.\n",
        "\n",
        "2. **Jaccard Index**: Measures the similarity between two sets. It is useful when clustering binary or categorical data, comparing the presence or absence of attributes.\n",
        "\n",
        "3. **Matching Coefficient**: Similar to Hamming distance, it calculates the proportion of matching attributes between two categorical data points.\n",
        "\n",
        "When using hierarchical clustering on categorical data, you can apply these distance metrics and then proceed with the usual agglomerative or divisive clustering techniques, such as **Single-Linkage**, **Complete-Linkage**, or **Average-Linkage**.\n",
        "\n",
        "By choosing an appropriate distance metric, hierarchical clustering can effectively group categorical data into meaningful clusters."
      ],
      "metadata": {
        "id": "5HdYPrmUs9Yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What does a negative Silhouette Score indicate\n",
        "A **negative Silhouette Score** indicates that a data point is **closer to points in another cluster** than to points in its own cluster. This suggests:\n",
        "\n",
        "- The point is likely **misclassified**.\n",
        "- The clusters may be **overlapping** or **not well-separated**.\n",
        "- The current clustering **may not be optimal**."
      ],
      "metadata": {
        "id": "lde6hQOEtmqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Explain the term \"linkage criteria\" in hierarchical clustering\n",
        "**Linkage criteria** determine how the distance between two clusters is calculated in hierarchical clustering. Common types:\n",
        "\n",
        "- **Single linkage**: Minimum distance between any two points in the clusters.  \n",
        "- **Complete linkage**: Maximum distance between points.  \n",
        "- **Average linkage**: Average of all pairwise distances.  \n",
        "- **Ward’s method**: Minimizes variance increase.  \n",
        "\n",
        "It influences the shape of the dendrogram and cluster formation."
      ],
      "metadata": {
        "id": "Vc8WIzvqt4gJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities\n",
        "K-Means assumes all clusters are spherical and equally sized. It performs poorly when:\n",
        "\n",
        "- Clusters have different sizes or densities.  \n",
        "- Clusters are not well-separated.  \n",
        "- It struggles with non-globular shapes.  \n",
        "- Sensitive to outliers and initialization."
      ],
      "metadata": {
        "id": "42MKSoSeuD4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the core parameters in DBSCAN, and how do they influence clustering\n",
        "Core parameters in DBSCAN:\n",
        "\n",
        "1. **eps (ε)**: Max distance for two points to be neighbors. Larger `eps` → larger clusters.  \n",
        "2. **min_samples**: Min points to form a dense region. Higher `min_samples` → fewer clusters.  \n",
        "They control density threshold to define core, border, and noise points."
      ],
      "metadata": {
        "id": "AIKxbCvZuei5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. How does K-Means++ improve upon standard K-Means initialization\n",
        "K-Means++ improves standard K-Means by choosing initial centroids more strategically:  \n",
        "1. First centroid is picked randomly.  \n",
        "2. Next centroids are chosen with higher probability from distant points.  \n",
        "This reduces chances of poor clustering and speeds up convergence."
      ],
      "metadata": {
        "id": "6CB3EKmWuoDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is agglomerative clustering\n",
        "Agglomerative clustering is a bottom-up hierarchical clustering method.  \n",
        "1. Each point starts as its own cluster.  \n",
        "2. Closest clusters are merged step by step.  \n",
        "3. This continues until all points are in one cluster or a stopping criterion is met.  \n",
        "It builds a dendrogram to show the merging process."
      ],
      "metadata": {
        "id": "fuic6Klvu06Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "Silhouette Score considers both **intra-cluster cohesion** and **inter-cluster separation**, unlike inertia which only measures how close points are to centroids.  \n",
        "It gives a **normalized score** (−1 to 1), making it easier to interpret and compare across models.  \n",
        "Useful for **non-spherical clusters** too.  \n",
        "Hence, it's more **comprehensive and reliable** than just using inertia."
      ],
      "metadata": {
        "id": "uDO89m19vCVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a\n",
        "scatter plot"
      ],
      "metadata": {
        "id": "doW2lNuTvKOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='red', marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering with 4 Centers\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-L5T4gOJvNmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10\n",
        "predicted labels\n"
      ],
      "metadata": {
        "id": "mWejarjwvSU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load Iris data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "model = AgglomerativeClustering(n_clusters=3)\n",
        "labels = model.fit_predict(X)\n",
        "\n",
        "# Display first 10 predicted labels\n",
        "print(\"First 10 labels:\", labels[:10])\n"
      ],
      "metadata": {
        "id": "13g57eV9vx1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot"
      ],
      "metadata": {
        "id": "CKNPuIWpvy7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_moons(n_samples=300, noise=0.1, random_state=0)\n",
        "\n",
        "# Apply DBSCAN\n",
        "db = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "labels = db.labels_\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k')\n",
        "plt.scatter(X[labels == -1, 0], X[labels == -1, 1], color='red', label='Outliers')\n",
        "plt.title(\"DBSCAN on make_moons\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Km_p3vRxv-2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each\n",
        "cluster"
      ],
      "metadata": {
        "id": "Azs3mMEOwEXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Print the size of each cluster\n",
        "cluster_sizes = np.bincount(kmeans.labels_)\n",
        "print(\"Cluster sizes:\", cluster_sizes)\n"
      ],
      "metadata": {
        "id": "_kBIGxdKwHYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result"
      ],
      "metadata": {
        "id": "3x6w8NrtwST-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data with make_circles\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.1)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot the result\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering on Synthetic Data (make_circles)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xQU58ccBwc96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.  Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster\n",
        "centroids"
      ],
      "metadata": {
        "id": "e80KPEXuwlTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering with 2 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Output the cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "print(\"Cluster Centroids:\\n\", centroids)\n"
      ],
      "metadata": {
        "id": "Mm6g4rdwwt48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with\n",
        "DBSCAN"
      ],
      "metadata": {
        "id": "aR4wfl6WxeOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data with varying cluster standard deviations\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.0, 0.5], random_state=42)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot the DBSCAN result\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title(\"DBSCAN Clustering on Synthetic Data with Varying Cluster Std\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8tvx907SxhII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28, Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means"
      ],
      "metadata": {
        "id": "BNoo-EOTyCIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce the data to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis')\n",
        "plt.title('K-Means Clusters on Digits Dataset (PCA-reduced)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E2J9I-uuyHVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart"
      ],
      "metadata": {
        "id": "aXYmWLmPyO98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Generate synthetic data using make_blobs\n",
        "X, _ = make_blobs(n_samples=500, centers=4, random_state=42)\n",
        "\n",
        "# List to store silhouette scores\n",
        "silhouette_scores = []\n",
        "\n",
        "# Evaluate silhouette scores for k = 2 to 5\n",
        "for k in range(2, 6):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Display the silhouette scores as a bar chart\n",
        "plt.bar(range(2, 6), silhouette_scores, color='skyblue')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores for Different Values of k')\n",
        "plt.xticks(range(2, 6))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iQqA66LHyVxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage"
      ],
      "metadata": {
        "id": "TFMs7iEmyqoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Perform hierarchical clustering using average linkage\n",
        "Z = linkage(X, method='average')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(Z)\n",
        "plt.title('Dendrogram for Iris Dataset (Average Linkage)')\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y8rIFtW4yvbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with\n",
        "decision boundaries"
      ],
      "metadata": {
        "id": "LMTR4ClHy7Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Generate synthetic data with overlapping clusters\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# Define a function to plot decision boundaries\n",
        "def plot_decision_boundaries(X, y, model, title=\"Decision Boundaries\"):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    cmap_background = ListedColormap(['#FFAAAA', '#AAAAFF', '#AAFFAA'])\n",
        "    cmap_points = ListedColormap(['#FF0000', '#0000FF', '#00FF00'])\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=cmap_background)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_points, edgecolors='k', s=30)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize decision boundaries\n",
        "plot_decision_boundaries(X, y_kmeans, kmeans, title=\"K-Means Clustering with Decision Boundaries\")\n"
      ],
      "metadata": {
        "id": "5gRoN6wUy_tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results"
      ],
      "metadata": {
        "id": "IGJEZbDHzHmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduce dimensions with t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_dbscan, cmap='viridis', edgecolors='k', s=50)\n",
        "plt.title('DBSCAN on Digits Dataset after t-SNE Dimensionality Reduction')\n",
        "plt.xlabel('t-SNE component 1')\n",
        "plt.ylabel('t-SNE component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "68JlXWOpzWwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33, Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot\n",
        "the result"
      ],
      "metadata": {
        "id": "Egi0SktfzcqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with complete linkage\n",
        "agg_clust = AgglomerativeClustering(linkage='complete', n_clusters=4)\n",
        "y_agg = agg_clust.fit_predict(X)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis', edgecolors='k', s=50)\n",
        "plt.title('Agglomerative Clustering with Complete Linkage')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0iQ4_PJ2zlDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a\n",
        "line plot"
      ],
      "metadata": {
        "id": "tHyLA5mFzmAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# List to store inertia values\n",
        "inertia_values = []\n",
        "\n",
        "# Apply K-Means for K = 2 to 6 and calculate inertia\n",
        "for k in range(2, 7):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the inertia values for different K\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(2, 7), inertia_values, marker='o', linestyle='-', color='b')\n",
        "plt.title('Inertia Values for Different K in K-Means')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xticks(range(2, 7))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t_LbtHTJzsWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with\n",
        "single linkage"
      ],
      "metadata": {
        "id": "vLEWffgLz7dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic data with concentric circles\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.1)\n",
        "\n",
        "# Apply Agglomerative Clustering with single linkage\n",
        "agg_clust = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg_clust.fit_predict(X)\n",
        "\n",
        "# Plot the result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering with Single Linkage on Concentric Circles')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0PwJP0Etz-4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding\n",
        "noise"
      ],
      "metadata": {
        "id": "EuzAhMZ90FPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Count the number of clusters (excluding noise)\n",
        "# Noise points are labeled as -1 by DBSCAN\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "print(f\"Number of clusters (excluding noise): {n_clusters}\")\n"
      ],
      "metadata": {
        "id": "ZlHbS3bk0OwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the\n",
        "data points"
      ],
      "metadata": {
        "id": "6H5o6mKT0UkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic data with 3 clusters\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', s=50)\n",
        "\n",
        "# Plot the cluster centers\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            marker='x', color='red', s=200, label='Centroids')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('K-Means Clustering with Cluster Centers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WUJo8JZ00XjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38.  Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise"
      ],
      "metadata": {
        "id": "KrPBxp1u0gQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan.fit(X_scaled)\n",
        "\n",
        "# Identify noise points (label -1 indicates noise)\n",
        "noise_samples = sum(dbscan.labels_ == -1)\n",
        "\n",
        "print(f'Number of noise samples: {noise_samples}')\n"
      ],
      "metadata": {
        "id": "gMwR1ZUE0k6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the\n",
        "clustering result"
      ],
      "metadata": {
        "id": "BChN5_rn0pop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic non-linearly separable data\n",
        "X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Plot the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', marker='o', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
        "plt.title('K-Means Clustering on make_moons Dataset')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qPblA84s0vNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.  Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D\n",
        "scatter plot"
      ],
      "metadata": {
        "id": "K_hTSF-R01D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_digits\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Apply PCA to reduce to 3 components\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 3D scatter plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y_kmeans, cmap='viridis', s=50)\n",
        "ax.set_title('K-Means Clustering on PCA-reduced Digits Dataset')\n",
        "ax.set_xlabel('PCA Component 1')\n",
        "ax.set_ylabel('PCA Component 2')\n",
        "ax.set_zlabel('PCA Component 3')\n",
        "\n",
        "# Add color bar\n",
        "plt.colorbar(scatter)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pX1-nvYY08k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the\n",
        "clustering"
      ],
      "metadata": {
        "id": "2alPXttz1Dy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Generate synthetic data with 5 centers\n",
        "X, y = make_blobs(n_samples=1000, centers=5, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Evaluate clustering performance using silhouette score\n",
        "sil_score = silhouette_score(X, y_kmeans)\n",
        "\n",
        "# Print silhouette score\n",
        "print(f'Silhouette Score: {sil_score:.3f}')\n",
        "\n",
        "# Visualize the clustering result\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
        "plt.title('KMeans Clustering with 5 Centers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CZp3Alxa1HKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering.\n",
        "Visualize in 2D"
      ],
      "metadata": {
        "id": "Hxwe_n701Myf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduce dimensionality to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clust = AgglomerativeClustering(n_clusters=2)  # Assuming 2 clusters\n",
        "y_pred = agg_clust.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the clustering result in 2D\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='viridis', s=50)\n",
        "plt.title('Agglomerative Clustering on PCA-reduced Breast Cancer Data')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5Q4N2GAe1Wzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN\n",
        "side-by-side"
      ],
      "metadata": {
        "id": "6hqWqvxf1Ynq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate noisy circular data using make_circles\n",
        "X, _ = make_circles(n_samples=300, noise=0.1, factor=0.5)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Create a figure to visualize both clustering results side-by-side\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# KMeans visualization\n",
        "axs[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels, cmap='viridis', s=50)\n",
        "axs[0].set_title('KMeans Clustering')\n",
        "axs[0].set_xlabel('Feature 1')\n",
        "axs[0].set_ylabel('Feature 2')\n",
        "\n",
        "# DBSCAN visualization\n",
        "axs[1].scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels, cmap='viridis', s=50)\n",
        "axs[1].set_title('DBSCAN Clustering')\n",
        "axs[1].set_xlabel('Feature 1')\n",
        "axs[1].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bp16hOgC1miY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering"
      ],
      "metadata": {
        "id": "9hkd9AlG1siJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply KMeans clustering with 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Compute the Silhouette Coefficient for each sample\n",
        "silhouette_vals = silhouette_samples(X_scaled, labels)\n",
        "\n",
        "# Plot the Silhouette Coefficients\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(range(len(silhouette_vals)), silhouette_vals, color='skyblue', edgecolor='black')\n",
        "plt.axhline(y=0, color='black', linestyle='--')\n",
        "plt.title('Silhouette Coefficients for Each Sample')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Silhouette Coefficient')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6vRIS_Qq1wUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage.\n",
        "Visualize clusters"
      ],
      "metadata": {
        "id": "88YQqIoA14H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic data using make_blobs\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with 'average' linkage\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Visualize the resulting clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o')\n",
        "plt.title('Agglomerative Clustering (Average Linkage)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T2hkyu5317wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4\n",
        "features)"
      ],
      "metadata": {
        "id": "ECFpfQvs2DIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data[:, :4]  # First 4 features\n",
        "y = wine.target  # Labels (not used in clustering, but can be used for comparison)\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Create a DataFrame for the features and cluster labels\n",
        "df = pd.DataFrame(X, columns=wine.feature_names)\n",
        "df['Cluster'] = cluster_labels\n",
        "\n",
        "# Visualize the cluster assignments using a pairplot\n",
        "sns.pairplot(df, hue='Cluster', palette='viridis')\n",
        "plt.suptitle('Wine Dataset - KMeans Clustering (First 4 Features)', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jDL1CZ-l2Mnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the\n",
        "count"
      ],
      "metadata": {
        "id": "rUQk_3Ja2NVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data with noisy blobs\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "X = np.vstack([X, np.random.uniform(low=-6, high=6, size=(50, 2))])  # Adding noise points\n",
        "\n",
        "# Apply DBSCAN\n",
        "db = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "# Count the number of clusters and noise points\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # -1 indicates noise\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "# Print the results\n",
        "print(f'Number of clusters: {n_clusters}')\n",
        "print(f'Number of noise points: {n_noise}')\n",
        "\n",
        "# Visualize the result\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o')\n",
        "plt.title(\"DBSCAN Clustering (Noise points are labeled -1)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-RVlS8Z02Rco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the\n",
        "clusters"
      ],
      "metadata": {
        "id": "ZqSviFdR2XFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce dimensionality using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg_clustering.fit_predict(X_tsne)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=50, marker='o')\n",
        "plt.title('Agglomerative Clustering on Digits Dataset (t-SNE reduced)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "An69jm562bqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}