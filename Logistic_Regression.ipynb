{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyt3Y/+qxOyuE9PElOaQFP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "Logistic Regression is a statistical model used for binary classification problems, where the target variable is categorical and typically takes values like 0 or 1 (e.g., yes/no, true/false). It estimates the probability that a given input belongs to a particular class. The model uses a logistic function (sigmoid) to output values between 0 and 1, which can be interpreted as probabilities.\n",
        "Differences-   \n",
        "Logistic Regression is a classification algorithm that outputs probabilities and is used for binary classification, while Linear Regression is a regression algorithm used for predicting continuous values.\n",
        "\n",
        "The key difference lies in the type of output and the model’s equatio"
      ],
      "metadata": {
        "id": "7ES-5R9_vp5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression.\n",
        "import numpy as np\n",
        "\n",
        "def logistic_regression(X, b):\n",
        "    \"\"\"\n",
        "    X: Input feature(s) as a numpy array or matrix\n",
        "    b: Coefficients (including intercept) as a numpy array or vector\n",
        "    Returns the predicted probability P(y=1|X)\n",
        "    \"\"\"\n",
        "    # Linear combination (b0 + b1*X1 + b2*X2 + ... + bn*Xn)\n",
        "    linear_combination = np.dot(X, b[1:]) + b[0]  # b[0] is intercept, b[1:] are coefficients for features\n",
        "    # Logistic (sigmoid) function\n",
        "    probability = 1 / (1 + np.exp(-linear_combination))\n",
        "    return probability\n",
        "\n",
        "# Example usage:\n",
        "# X = [feature1, feature2, ...] (input feature values)\n",
        "# b = [b0, b1, b2, ...] (model coefficients, where b0 is intercept)\n",
        "X = np.array([1.5, 2.3])  # Example feature values\n",
        "b = np.array([0.5, -0.8, 0.6])  # Example coefficients: b0 = 0.5, b1 = -0.8, b2 = 0.6\n",
        "\n",
        "# Predicted probability for y=1\n",
        "prob = logistic_regression(X, b)\n",
        "print(f\"Predicted probability for y=1: {prob}\")"
      ],
      "metadata": {
        "id": "lsGFwO49v-kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Why do we use the Sigmoid function in Logistic Regression."
      ],
      "metadata": {
        "id": "W4D3d5lpwaJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Sigmoid function to map input value z to a probability between 0 and 1.\n",
        "\n",
        "    z: input value (linear combination of features, b0 + b1*X1 + b2*X2 + ... + bn*Xn)\n",
        "\n",
        "    Returns:\n",
        "    Probability value between 0 and 1.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Example usage:\n",
        "z = 0.5  # Example input (linear combination)\n",
        "probability = sigmoid(z)\n",
        "print(f\"Probability for z={z}: {probability}\")"
      ],
      "metadata": {
        "id": "7pVQyMx0ww18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function sigmoid(z) implements the Sigmoid function\n",
        "The function returns a probability between 0 and 1, which can be interpreted as the predicted probability of class 1 for binary classification tasks."
      ],
      "metadata": {
        "id": "ywjneZEvwyH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression.\n",
        "The cost function for Logistic Regression is the log-loss (also called binary cross-entropy). It measures the difference between the predicted probabilities and the actual class labels. The goal of logistic regression is to minimize this cost function during the training process."
      ],
      "metadata": {
        "id": "6Zkjce5fw6RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def logistic_regression_cost(y, y_hat):\n",
        "    \"\"\"\n",
        "    Compute the cost function for logistic regression (log-loss).\n",
        "\n",
        "    y: Actual labels (true values) as a numpy array of shape (m,)\n",
        "    y_hat: Predicted probabilities as a numpy array of shape (m,)\n",
        "\n",
        "    Returns:\n",
        "    The cost (scalar value).\n",
        "    \"\"\"\n",
        "    m = len(y)  # Number of training examples\n",
        "\n",
        "    # Compute the log-loss cost function\n",
        "    cost = -1/m * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "    return cost\n",
        "\n",
        "# Example usage:\n",
        "y = np.array([1, 0, 1, 1, 0])  # Actual labels\n",
        "y_hat = np.array([0.9, 0.1, 0.8, 0.7, 0.2])  # Predicted probabilities\n",
        "\n",
        "# Calculate cost\n",
        "cost_value = logistic_regression_cost(y, y_hat)\n",
        "print(f\"Cost (Log-Loss): {cost_value}\")"
      ],
      "metadata": {
        "id": "dwn8uMjxxFcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "Regularization adds a penalty to large coefficients in logistic regression. It helps prevent overfitting and improves model generalization, especially when there are many or correlated features."
      ],
      "metadata": {
        "id": "-cs3OLkPxPf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "Lasso: Uses L1 regularization, can shrink some coefficients to zero (helps with feature selection).\n",
        "\n",
        "Ridge: Uses L2 regularization, shrinks coefficients but keeps all features.\n",
        "\n",
        "Elastic Net: Combines L1 and L2, balances between shrinking and selecting features."
      ],
      "metadata": {
        "id": "Wdv9NVLY_b9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "Use Elastic Net when you have many features and some are highly correlated. It gives a balance of Ridge's stability and Lasso's feature selection."
      ],
      "metadata": {
        "id": "2qyEHls-_jSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "The regularization parameter (λ) controls the strength of the penalty.\n",
        "\n",
        "High λ: More regularization, smaller coefficients, less overfitting.\n",
        "\n",
        "Low λ: Less regularization, model may overfit.\n",
        "It helps balance bias and variance."
      ],
      "metadata": {
        "id": "5IIw1Ipr_uzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n",
        "The outcome is binary.\n",
        "\n",
        "No strong multicollinearity among features.\n",
        "\n",
        "Linearity between features and log-odds.\n",
        "\n",
        "Large sample size for stable results.\n",
        "\n",
        "Independent observations."
      ],
      "metadata": {
        "id": "gZw6xTCm_0vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "Gradient Boosting (like XGBoost, LightGBM)"
      ],
      "metadata": {
        "id": "IwIOfNu5_8tY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics.\n",
        "Accuracy: Overall correct predictions\n",
        "\n",
        "Precision: Correct positives out of predicted positives\n",
        "\n",
        "Recall: Correct positives out of actual positives\n",
        "\n",
        "F1 Score: Balance of precision and recall\n",
        "\n",
        "ROC-AUC: Measures model's ability to distinguish classes\n",
        "\n",
        "Confusion Matrix: Shows TP, FP, FN, TN"
      ],
      "metadata": {
        "id": "CYJha3ZmADnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 12. How does class imbalance affect Logistic Regression.\n",
        " Class imbalance can cause logistic regression to favor the majority class, leading to poor recall for the minority class and misleading accuracy. It affects the model's ability to learn patterns from the underrepresented class."
      ],
      "metadata": {
        "id": "yOWwW0hDAOA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "Hyperparameter tuning in logistic regression involves adjusting parameters like C (regularization strength) and solver (optimization algorithm) to improve model performance. Techniques like grid search or random search are used to find the best values."
      ],
      "metadata": {
        "id": "mLsuLudYAWpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "Common solvers in logistic regression:\n",
        "\n",
        "'liblinear': Good for small datasets and L1 regularization.\n",
        "\n",
        "'newton-cg': Best for large datasets with L2 regularization.\n",
        "\n",
        "'lbfgs': Works well for large datasets, supports L2 regularization.\n",
        "\n",
        "'saga': Suitable for large datasets and both L1 and L2 regularization."
      ],
      "metadata": {
        "id": "tybEwrijAu34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "Logistic regression is extended for multiclass classification using two main strategies:\n",
        "\n",
        "One-vs-Rest (OvR): Treats each class as a binary classification problem, with the model learning to separate one class from all others.\n",
        "\n",
        "Softmax Regression: Generalizes logistic regression by using the softmax function to predict probabilities for multiple classes simultaneously."
      ],
      "metadata": {
        "id": "NhAzNeA-BZOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "Advantages:\n",
        "\n",
        "Simple and easy to implement.\n",
        "\n",
        "Works well with linearly separable data.\n",
        "\n",
        "Efficient for binary and multiclass classification.\n",
        "\n",
        "Outputs probabilities for class predictions.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Assumes linearity between features and log-odds.\n",
        "\n",
        "Sensitive to outliers.\n",
        "\n",
        "May underperform with complex relationships or non-linear data.\n",
        "\n",
        "Struggles with high-dimensional data without regularization."
      ],
      "metadata": {
        "id": "ZSwcqXDHBf_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression.\n",
        "Spam Detection: Classifying emails as spam or not spam.\n",
        "\n",
        "Customer Churn Prediction: Predicting if a customer will leave a service.\n",
        "\n",
        "Medical Diagnosis: Predicting the presence or absence of a disease.\n",
        "\n",
        "Credit Scoring: Assessing whether a loan applicant will default.\n",
        "\n",
        "Marketing: Predicting whether a customer will click on an ad."
      ],
      "metadata": {
        "id": "fSBehWjvBo0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "Logistic Regression: Used for binary classification, predicts the probability of one class (0 or 1) using the sigmoid function.\n",
        "\n",
        "Softmax Regression: Extends logistic regression to handle multiple classes, predicts probabilities for each class using the softmax function, and assigns the class with the highest probability."
      ],
      "metadata": {
        "id": "8PlqKxmGBycW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "One-vs-Rest (OvR): Choose when you have a small to medium dataset, or when model simplicity is needed. It works well for binary classifiers extended to multiclass but can struggle with highly imbalanced classes.\n",
        "\n",
        "Softmax: Choose for more complex problems or when classes are mutually exclusive. It directly predicts probabilities for each class and is more efficient for multiclass problems. Preferred when you have more data and want better model performance."
      ],
      "metadata": {
        "id": "oL_-cnc9B6RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.How do we interpret coefficients in Logistic Regression?\n",
        "In logistic regression, coefficients represent the change in the log-odds of the target variable for a one-unit change in the predictor variable, keeping other variables constant.\n",
        "\n",
        "Positive coefficient: Increases the probability of the target class (class 1).\n",
        "\n",
        "Negative coefficient: Decreases the probability of the target class.\n",
        "\n",
        "Magnitude: The larger the absolute value of the coefficient, the stronger the influence of that feature on the model's prediction."
      ],
      "metadata": {
        "id": "uDuOv7dFCpmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracyC"
      ],
      "metadata": {
        "id": "6y6-ww5RC0Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (using Iris dataset for illustration)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "8NAiEPNtC2yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracyC"
      ],
      "metadata": {
        "id": "5l4OZkN3C6Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (using Iris dataset for illustration)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Use only binary classification for simplicity (setosa vs non-setosa)\n",
        "y = (y == 0).astype(int)  # 1 for setosa, 0 for non-setosa\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "h7wiGQLXC9Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficient"
      ],
      "metadata": {
        "id": "B1DMbU0eDGDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (using Iris dataset for illustration)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Use only binary classification for simplicity (setosa vs non-setosa)\n",
        "y = (y == 0).astype(int)  # 1 for setosa, 0 for non-setosa\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and coefficients\n",
        "print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy:.2f}\")\n",
        "print(\"Model Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "id": "bG0ALPkBDJxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')"
      ],
      "metadata": {
        "id": "UANIcfG5DNgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (using Iris dataset for illustration)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Use only binary classification for simplicity (setosa vs non-setosa)\n",
        "y = (y == 0).astype(int)  # 1 for setosa, 0 for non-setosa\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with Elastic Net Regularization\n",
        "# ElasticNet requires both l1_ratio (balance between L1 and L2) and solver\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and coefficients\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "nkZoTsktDSti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr"
      ],
      "metadata": {
        "id": "jf0z6H-cDVFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (using Iris dataset for multiclass classification)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and model coefficients\n",
        "print(f\"Model Accuracy with One-vs-Rest Strategy: {accuracy:.2f}\")\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "DgqSuKRKDa3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "N9FzUDBaDcdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using Iris dataset for classification)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']  # Regularization types (Lasso and Ridge)\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from GridSearchCV\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "2oK2zn07Dg4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracyC"
      ],
      "metadata": {
        "id": "7Ufoy2ScDjOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using Iris dataset for classification)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize StratifiedKFold with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy scores for each fold\n",
        "accuracy_scores = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy and append to the list\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calculate and print average accuracy\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "print(f\"Average Accuracy from Stratified K-Fold Cross-Validation: {average_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "7oV1ZtLkDlxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy"
      ],
      "metadata": {
        "id": "c312qK_DDqy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from a CSV file\n",
        "# Replace 'your_dataset.csv' with the path to your dataset file\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Assuming the last column is the target variable and the rest are features\n",
        "X = data.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "y = data.iloc[:, -1]   # Target variable (the last column)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Pqk0A8yTDtb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "fMj-C11WDxwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using Iris dataset for classification)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Regularization strength (log scale)\n",
        "    'penalty': ['l1', 'l2'],  # Regularization types (Lasso and Ridge)\n",
        "    'solver': ['liblinear', 'saga', 'lbfgs']  # Solvers for Logistic Regression\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, random_state=42)\n",
        "\n",
        "# Fit the randomized search to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from RandomizedSearchCV\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "1NE-1wzID0jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
      ],
      "metadata": {
        "id": "ha4nzje1D39n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using Iris dataset for classification)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (target)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Apply One-vs-One (OvO) multiclass strategy\n",
        "ovo_classifier = OneVsOneClassifier(model)\n",
        "\n",
        "# Train the model\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Model Accuracy (One-vs-One): {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "R_YVBOUeEBbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "OYlPJzYQEB72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix for Binary Classification')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "06LJ8hFdEE9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "j0ffjL7YEJAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "_NNsqn0fEPbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performanceM"
      ],
      "metadata": {
        "id": "IxUA1o4uERhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model with class weights to handle the imbalance\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "uU6tfBi_Eb5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance"
      ],
      "metadata": {
        "id": "E2ugQIB2EckO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Handle missing values by filling with the median or mode (for categorical features)\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical columns\n",
        "data['Age'] = SimpleImputer(strategy='mean').fit_transform(data[['Age']])  # For numerical columns\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]  # Features\n",
        "y = data['Survived']  # Target variable\n",
        "\n",
        "# Convert categorical variable 'Sex' into numeric (0, 1)\n",
        "X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a logistic regression model pipeline with imputation and scaling\n",
        "model = make_pipeline(SimpleImputer(strategy='most_frequent'), StandardScaler(), LogisticRegression())\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "kCmgJXmNEjbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling\n"
      ],
      "metadata": {
        "id": "MA_wC48xEkZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Logistic Regression with scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print the comparison of accuracy\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.2f}\")\n"
      ],
      "metadata": {
        "id": "Y9s_6VMUEqoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score"
      ],
      "metadata": {
        "id": "sG2nTyPSEuVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use class 0 and class 1 for binary classification\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set (probabilities for ROC-AUC)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Get the probabilities for the positive class (class 1)\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mTg9jjxfE24e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracyM"
      ],
      "metadata": {
        "id": "bsxfs7ypE3du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with a custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "323HvV2GE6sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients"
      ],
      "metadata": {
        "id": "fZjncXthE-2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the coefficients of the model\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Identify the feature names\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create a DataFrame to show the coefficients alongside feature names\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Abs_Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort the features based on the absolute value of the coefficients\n",
        "coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Print the features and their corresponding coefficients\n",
        "print(\"Important Features Based on Coefficients:\")\n",
        "print(coef_df)\n",
        "\n",
        "# Optionally, print the model accuracy\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "LYvWpbanFHr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score"
      ],
      "metadata": {
        "id": "x9J3WoJFFIPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print the Cohen's Kappa score\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "bt_xyaczFKiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:"
      ],
      "metadata": {
        "id": "ntxYK4n3FQaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision and recall values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sN6pdbpWFSnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QGqUivWCBjc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracyM"
      ],
      "metadata": {
        "id": "h_6uy1ocFV6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different solvers\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "# Train Logistic Regression with different solvers and evaluate accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "\n",
        "# Print the accuracies for each solver\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy with {solver} solver: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "2q0EgFFEFY5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)M"
      ],
      "metadata": {
        "id": "tlm6JBDHFxZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "kjX3ip23F3SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scalingM"
      ],
      "metadata": {
        "id": "UEezRpoAF8Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "QUQdXeegF_BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation"
      ],
      "metadata": {
        "id": "RttFBAKgGChQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the range of hyperparameters (C values)\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Use GridSearchCV to find the optimal C using cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best C value and corresponding accuracy\n",
        "print(f\"Optimal C: {grid_search.best_params_['C']}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the model with the optimal C on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(f\"Test set accuracy with optimal C: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "4ph5anaIGFPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions"
      ],
      "metadata": {
        "id": "q9qLiT6hGLHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# We will only use two classes for binary classification (class 0 and class 1)\n",
        "X = X[y != 2]  # Remove class 2 for binary classification\n",
        "y = y[y != 2]  # Keep only class 0 and class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "tp0AVGGtGNez"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}