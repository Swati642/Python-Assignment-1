{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPELMf/u3YUOfoq8fav1vz0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/Boosting_Technique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning\n",
        "Ans. Boosting is an ensemble technique in machine learning that combines multiple weak learners (usually decision trees) sequentially, where each new model focuses on correcting the errors of the previous ones. It reduces bias and improves accuracy.\n",
        "\n",
        "Popular algorithms:\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost\n",
        "\n",
        "LightGBM\n",
        "\n",
        "CatBoost\n"
      ],
      "metadata": {
        "id": "FRD_z6LWeE0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does Boosting differ from Bagging\n",
        "Bagging:\n",
        "Models are trained at the same time on random parts of the data. Final result is the average or majority vote.\n",
        "(Example: Random Forest)\n",
        "\n",
        "Boosting:\n",
        "Models are trained one after another. Each new model fixes the mistakes of the previous one.\n",
        "(Example: AdaBoost, XGBoost)\n",
        "\n"
      ],
      "metadata": {
        "id": "8EEMDDzNfyfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the key idea behind AdaBoost\n",
        "AdaBoost builds a strong model by combining many weak models (usually small decision trees) one after another.\n",
        "\n",
        "Each new model focuses more on the data points that were misclassified by previous models. It gives more weight to the hard examples and less weight to the correctly predicted ones.\n",
        "\n",
        "At the end, all models are combined using weighted voting to make the final prediction."
      ],
      "metadata": {
        "id": "CYf4wp8DgKBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the working of AdaBoost with an example\n",
        "Ans. AdaBoost (Adaptive Boosting) works by combining several weak models (like small decision trees) to form a strong model. It trains the models one after another, and each new model focuses more on the mistakes made by the previous ones.\n",
        "\n",
        "At the start, all data points are given equal importance. After each model is trained, the weights of wrongly predicted examples are increased, so the next model tries harder to get them right. This way, AdaBoost keeps improving\n",
        "Example of AdaBoost:\n",
        "Imagine you want to classify fruits as apple or orange based on their color and size using AdaBoost.\n",
        "\n",
        "Step 1: Train the first weak model\n",
        "You train the first model (a small decision tree). Let's say it misclassifies an orange fruit as an apple.\n",
        "\n",
        "Now, AdaBoost gives higher weight to that misclassified orange so the next model pays more attention to it.\n",
        "\n",
        "Step 2: Train the second model\n",
        "The second model focuses more on the misclassified orange fruit. But this model might now misclassify a different fruit, say a red apple that was previously correct.\n",
        "\n",
        "AdaBoost increases the weight of the misclassified red apple for the next model.\n",
        "\n",
        "Step 3: Train the third model\n",
        "The third model now works to correct the mistakes made by the first two models.\n",
        "\n",
        "This model might get the orange and apple fruits right, improving the accuracy of the system.\n",
        "\n",
        "Step 4: Combine all models\n",
        "In the final step, the predictions of all three models are combined. Each model’s prediction is weighted based on its accuracy.\n",
        "\n",
        "If the first model was highly accurate, its prediction will count more. If the second was less accurate, its prediction will count less.\n",
        "\n",
        "Final Prediction:\n",
        "\n",
        "The final prediction is the result of weighted voting from all the models, where the harder-to-classify fruits are given more importance.\n",
        "\n",
        "AdaBoost, after correcting its mistakes, will make a more accurate classification of apples and oranges."
      ],
      "metadata": {
        "id": "Vrf6OO0ngXtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Gradient Boosting, and how is it different from AdaBoost\n",
        "Ans. Gradient Boosting is an ensemble learning technique that builds strong predictive models by combining multiple weak models (typically decision trees) sequentially, similar to AdaBoost. The key difference is that Gradient Boosting minimizes a loss function (such as Mean Squared Error for regression or Log-Loss for classification) using gradient descent.\n",
        "\n",
        "How it works:\n",
        "Each new model is trained to predict the residual errors (the difference between the predicted values and the actual values) of the previous model.\n",
        "\n",
        "Key concept:\n",
        "The new model \"gradually corrects\" the errors made by the previous models using gradient descent to minimize the overall error. This is why it's called Gradient Boosting.\n",
        "\n",
        "Gradient Boosting uses gradient descent to minimize errors and focuses on residuals.\n",
        "\n",
        "AdaBoost adjusts weights of misclassified points and combines models based on accuracy."
      ],
      "metadata": {
        "id": "wFLXNtBmhE9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the loss function in Gradient Boosting\n",
        "In Gradient Boosting, the loss function plays a key role in guiding the algorithm to minimize prediction errors. The loss function measures the difference between the predicted values and the actual values, and the goal is to minimize this difference during the training process."
      ],
      "metadata": {
        "id": "aa56K4MmhRPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L(y, ŷ) = (1/n) * Σ(y_i - ŷ_i)^2\n"
      ],
      "metadata": {
        "id": "Y261_3_fhhue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How does XGBoost improve over traditional Gradient Boosting\n",
        "XGBoost Improvements Over Gradient Boosting:\n",
        "\n",
        "Regularization: Includes L1 and L2 regularization to prevent overfitting.\n",
        "\n",
        "Parallelization: Faster training by parallelizing tasks.\n",
        "\n",
        "Missing Data: Handles missing values directly without imputation.\n",
        "\n",
        "Tree Pruning: Uses post-pruning for better accuracy.\n",
        "\n",
        "Faster Splits: Uses approximate algorithms for faster computation.\n",
        "\n",
        "Scalability: Handles large datasets efficiently.\n",
        "\n",
        "Early Stopping: Stops training early if performance plateaus.\n",
        "\n",
        "Imbalanced Data: Better at handling imbalanced datasets."
      ],
      "metadata": {
        "id": "E7PBK7QJhqAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the difference between XGBoost and CatBoost\n",
        "1. Categorical Feature Handling:\n",
        "XGBoost: Requires manual encoding of categorical features (e.g., one-hot encoding).\n",
        "\n",
        "CatBoost: Automatically handles categorical features without the need for encoding.\n",
        "\n",
        "2. Speed:\n",
        "XGBoost: Fast for general tasks, but can be slower on datasets with many categorical features due to manual encoding.\n",
        "\n",
        "CatBoost: Often faster when dealing with many categorical features, as it optimizes categorical handling.\n",
        "\n",
        "3. Regularization:\n",
        "XGBoost: Supports L1 (Lasso) and L2 (Ridge) regularization.\n",
        "\n",
        "CatBoost: Uses built-in sophisticated regularization to avoid overfitting, but does not expose L1 or L2 in the same way.\n",
        "\n",
        "4. Hyperparameter Tuning:\n",
        "XGBoost: Requires extensive tuning of hyperparameters for optimal performance.\n",
        "\n",
        "CatBoost: Performs well with fewer hyperparameters and often requires less fine-tuning.\n",
        "\n",
        "5. Missing Data:\n",
        "XGBoost: Can handle missing data, but requires special handling.\n",
        "\n",
        "CatBoost: Natively handles missing data without additional steps.\n",
        "\n",
        "6. Parallelization:\n",
        "XGBoost: Parallelizes tree building but may not be as efficient with categorical data.\n",
        "\n",
        "CatBoost: Uses advanced parallelization techniques, especially effective with categorical features.\n"
      ],
      "metadata": {
        "id": "a8eFKGd_hz_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. 1. Finance\n",
        "Credit scoring and fraud detection in banking.\n",
        "\n",
        "2. Healthcare\n",
        "Disease prediction (e.g., cancer, diabetes) and medical image classification.\n",
        "\n",
        "3. E-commerce\n",
        "Customer segmentation and product recommendations.\n",
        "\n",
        "4. Marketing\n",
        "Churn prediction and targeted advertising.\n",
        "\n",
        "5. Autonomous Vehicles\n",
        "Object detection for self-driving cars.\n",
        "\n",
        "6. Search Engines\n",
        "Improving search rankings.\n",
        "\n",
        "7. NLP\n",
        "Sentiment analysis and text classification.\n",
        "\n",
        "8. Image & Speech Recognition\n",
        "Face detection and speech-to-text.\n",
        "\n",
        "Boosting techniques are used to improve predictions in these real-world applications, particularly for classification and regression tasks."
      ],
      "metadata": {
        "id": "wMnYVsh3h9Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How does regularization help in XGBoost\n",
        "Ans. Regularization in XGBoost helps to prevent overfitting by controlling model complexity, leading to better generalization. It adds penalties to the loss function to discourage overly complex models.\n",
        "\n",
        "How Regularization Works in XGBoost:\n",
        "L1 Regularization (Lasso):\n",
        "\n",
        "Purpose: Reduces the number of features by pushing some feature weights to zero.\n",
        "\n",
        "Effect: Helps in feature selection and sparsity.\n",
        "\n",
        "Formula: Adds λ * ||w||_1 to the loss, where λ is the regularization strength.\n",
        "\n",
        "L2 Regularization (Ridge):\n",
        "\n",
        "Purpose: Penalizes large weights and makes the model smoother.\n",
        "\n",
        "Effect: Helps reduce variance and overfitting.\n",
        "\n",
        "Formula: Adds α * ||w||_2^2 to the loss, where α is the regularization strength.\n",
        "\n",
        "Benefits of Regularization in XGBoost:\n",
        "Prevents overfitting by reducing model complexity.\n",
        "\n",
        "Improves generalization to unseen data.\n",
        "\n",
        "Controls large feature spaces by pushing irrelevant features towards zero (L1) or penalizing large coefficients (L2)."
      ],
      "metadata": {
        "id": "NhuPpNEniLSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are some hyperparameters to tune in Gradient Boosting models\n",
        "Ans. Here are 5 important hyperparameters to tune in Gradient Boosting models:\n",
        "\n",
        "n_estimators – Number of boosting rounds (trees). More trees can improve accuracy but may lead to overfitting.\n",
        "\n",
        "learning_rate – Shrinks the contribution of each tree. Lower values need more trees but improve generalization.\n",
        "\n",
        "max_depth – Maximum depth of each tree. Controls model complexity; deeper trees can capture more patterns but may overfit.\n",
        "\n",
        "subsample – Fraction of training data used for fitting each tree. Helps prevent overfitting by introducing randomness.\n",
        "\n",
        "min_samples_split – Minimum number of samples required to split a node. Higher values prevent the model from learning too fine details."
      ],
      "metadata": {
        "id": "NKBuStocmenk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the concept of Feature Importance in Boosting\n",
        "Ans. Feature Importance in Boosting shows which features are most useful for making predictions. It's calculated based on how often and how effectively a feature is used in tree splits. It helps in selecting key features and understanding the model better."
      ],
      "metadata": {
        "id": "L5ZVKAAxu2xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Why is CatBoost efficient for categorical data?\n",
        "CatBoost is efficient for categorical data because:\n",
        "\n",
        "Automatic Handling: It automatically detects and processes categorical features — no need for manual encoding.\n",
        "\n",
        "Ordered Target Statistics: Uses a special technique to convert categories into numbers without data leakage, improving accuracy.\n",
        "\n",
        "Efficient Encoding: It uses efficient encoding methods instead of one-hot encoding, which saves memory and speeds up training.\n",
        "\n",
        "Less Preprocessing: Reduces preprocessing steps, making it faster and easier to use on real-world data"
      ],
      "metadata": {
        "id": "VbdzJ5UqvPZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Train an AdaBoost Classifier on a sample dataset and print model accuracy4"
      ],
      "metadata": {
        "id": "xtzwsqswvCEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "YW8MI4COvFwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
        "Ans. from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "model = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)"
      ],
      "metadata": {
        "id": "QPrtwgp7vJ6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance"
      ],
      "metadata": {
        "id": "kC8V35Euv5vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "id": "orEJP7Twv_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score4"
      ],
      "metadata": {
        "id": "cQwPpgvawDLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-Squared Score:\", r2)"
      ],
      "metadata": {
        "id": "mEbJM6uNwF-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting"
      ],
      "metadata": {
        "id": "bWk9OgQywKPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_pred = gb_model.predict(X_test)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Gradient Boosting Classifier Accuracy: {gb_accuracy:.4f}\")\n",
        "print(f\"XGBoost Classifier Accuracy: {xgb_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Gs-KY8toycub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Train a CatBoost Classifier and evaluate using F1-Score"
      ],
      "metadata": {
        "id": "Tjkwybqfzu06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, random_seed=42, silent=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "5hsHzleozz5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "O5bqBDXnz0xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate using MSE\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "id": "dEqQ6ezpz95c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train an AdaBoost Classifier and visualize feature importance"
      ],
      "metadata": {
        "id": "_KhtvfFB0AFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier with a base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)  # Stump as base estimator\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, importances, color='skyblue')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"AdaBoost Classifier - Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xhHqCZUS0J-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Gradient Boosting Regressor and plot learning curves"
      ],
      "metadata": {
        "id": "TUHfGEzw0KyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=0.3, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Generate learning curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    model, X_train, y_train, cv=5, scoring='neg_mean_squared_error',\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)\n",
        "\n",
        "# Calculate the mean and standard deviation of the training and testing scores\n",
        "train_mean = -np.mean(train_scores, axis=1)\n",
        "test_mean = -np.mean(test_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_sizes, train_mean, label=\"Training Error\", color=\"blue\")\n",
        "plt.plot(train_sizes, test_mean, label=\"Validation Error\", color=\"red\")\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\")\n",
        "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"red\")\n",
        "plt.title(\"Learning Curves (Gradient Boosting Regressor)\")\n",
        "plt.xlabel(\"Training Size\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gRKwrFs70ScL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train an XGBoost Classifier and visualize feature importance"
      ],
      "metadata": {
        "id": "ejVri3BU0TDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "xgb.plot_importance(model, importance_type='weight', max_num_features=10,\n",
        "                    title=\"Feature Importance (XGBoost)\", height=0.8)"
      ],
      "metadata": {
        "id": "nBNSdoyT0i2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a CatBoost Classifier and plot the confusion matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, random_seed=42, silent=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the trained model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix (CatBoost Classifier)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BDt_xst_0jcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy"
      ],
      "metadata": {
        "id": "_SjTpOMy014y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different numbers of estimators\n",
        "n_estimators_list = [50, 100, 150, 200, 250]\n",
        "\n",
        "# Store accuracy for each number of estimators\n",
        "accuracies = []\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    # Train AdaBoost Classifier with different numbers of estimators\n",
        "    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot the accuracy vs. number of estimators\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(n_estimators_list, accuracies, marker='o', linestyle='-', color='b')\n",
        "plt.title(\"AdaBoost Classifier Accuracy vs Number of Estimators\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy for each number of estimators\n",
        "for n_estimators, accuracy in zip(n_estimators_list, accuracies):\n",
        "    print(f\"Number of Estimators: {n_estimators}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "nVMYBLzB0_f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "26. Train a Gradient Boosting Classifier and visualize the ROC curve"
      ],
      "metadata": {
        "id": "gIn1PznU1Aw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC curve (probability of class 1)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_awFlJh1bUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=0.3, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],  # Trying different learning rates\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune learning rate\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best learning rate from GridSearchCV\n",
        "best_lr = grid_search.best_params_['learning_rate']\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "\n",
        "# Train the model with the best learning rate\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate performance\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
      ],
      "metadata": {
        "id": "o1X3w0qg1fbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting"
      ],
      "metadata": {
        "id": "XqinrV_S1oLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Generate an imbalanced dataset (2 classes, with highly imbalanced target)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                           n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier without class weighting\n",
        "model_no_weight = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, random_seed=42, silent=True)\n",
        "model_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = model_no_weight.predict(X_test)\n",
        "\n",
        "# Train CatBoost Classifier with class weighting\n",
        "model_with_weight = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, random_seed=42, class_weights=[1, 10], silent=True)\n",
        "model_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = model_with_weight.predict(X_test)\n",
        "\n",
        "# Compare performance using classification report\n",
        "print(\"Performance without class weighting:\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "print(\"Performance with class weighting:\")\n",
        "print(classification_report(y_test, y_pred_with_weight))"
      ],
      "metadata": {
        "id": "yImWougw1ukE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train an AdaBoost Classifier and analyze the effect of different learning rates4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Generate an imbalanced dataset (2 classes, with highly imbalanced target)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                           n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier without class weighting\n",
        "model_no_weight = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, random_seed=42, silent=True)\n",
        "model_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = model_no_weight.predict(X_test)\n",
        "\n",
        "# Train CatBoost Classifier with class weighting\n",
        "model_with_weight = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, random_seed=42, class_weights=[1, 10], silent=True)\n",
        "model_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = model_with_weight.predict(X_test)\n",
        "\n",
        "# Compare performance using classification report\n",
        "print(\"Performance without class weighting:\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "print(\"Performance with class weighting:\")\n",
        "print(classification_report(y_test, y_pred_with_weight))"
      ],
      "metadata": {
        "id": "fD7hd5q51vjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss."
      ],
      "metadata": {
        "id": "zXr8CBT92BCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Iris dataset for multi-class classification\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier for multi-class classification\n",
        "model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for multi-class\n",
        "y_pred_prob = model.predict_proba(X_test)\n",
        "\n",
        "# Evaluate using log-loss\n",
        "loss = log_loss(y_test, y_pred_prob)\n",
        "print(f\"Log-Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "SHuNVtdy3Blx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OiRE2I551JEF"
      }
    }
  ]
}