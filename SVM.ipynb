{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkHaIOGkfcqTnYJz1fXETY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swati642/Python-Assignment-1/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)\n",
        "SVM is a supervised ML algorithm used for classification and regression. It finds the best boundary (hyperplane) that separates data into classes."
      ],
      "metadata": {
        "id": "mcwpjpDGOYOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM\n",
        "Hard Margin: No misclassification allowed, works only with linearly separable data.\n",
        "Soft Margin: Allows some misclassifications, works better with noisy or overlapping data."
      ],
      "metadata": {
        "id": "ZSR2xZ1OOlZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM\n",
        "SVM finds the **hyperplane** that best separates classes with the **maximum margin** (distance between the hyperplane and closest points from each class)."
      ],
      "metadata": {
        "id": "LwC8wudNOuMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM\n",
        "They help convert the constrained optimization problem of SVM into a solvable form (dual form) by incorporating constraints into the objective function."
      ],
      "metadata": {
        "id": "9Vbs67BRO42c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Support Vectors in SVM\n",
        "Support Vectors are the data points closest to the decision boundary; they define the margin and affect the optimal hyperplane."
      ],
      "metadata": {
        "id": "IjjgIL8LPChv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)\n",
        "Support Vector Classifier (SVC) is an SVM used for classification. It finds the best hyperplane to separate classes with the maximum margin."
      ],
      "metadata": {
        "id": "3VaPI32cPK_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)\n",
        "Support Vector Regressor (SVR) is the regression version of SVM. It fits the best line within a margin, ignoring small errors while penalizing larger ones."
      ],
      "metadata": {
        "id": "oR4b3XUAPUNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM\n",
        "The **kernel trick** enables SVM to efficiently perform classification in high-dimensional space without the need for explicitly transforming the data. By applying kernel functions like **linear**, **polynomial**, or **Radial Basis Function (RBF)**, SVM can compute the dot product between data points in a higher-dimensional space, allowing it to find nonlinear decision boundaries. This trick helps in handling complex data where the decision boundary isn't linear, making SVM powerful for various tasks."
      ],
      "metadata": {
        "id": "LO8lwdlmPjd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. : Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "**Linear Kernel**:\n",
        "- Simple and fast.\n",
        "- Suitable for linearly separable data.\n",
        "- Decision boundary is a straight line or hyperplane.\n",
        "\n",
        "**Polynomial Kernel**:\n",
        "- Suitable for datasets where relationships are polynomial (non-linear).\n",
        "- Can model more complex decision boundaries.\n",
        "- Higher degree can capture more intricate patterns but may lead to overfitting.\n",
        "\n",
        "**RBF Kernel (Radial Basis Function)**:\n",
        "- Commonly used kernel for complex, non-linear datasets.\n",
        "- Creates smooth decision boundaries and can handle highly complex patterns.\n",
        "- Sensitive to the choice of parameters like **gamma**, which determines the spread of the kernel."
      ],
      "metadata": {
        "id": "xmI3CwSiP7ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the effect of the C parameter in SVM\n",
        "The **C parameter** in SVM controls the trade-off between maximizing the margin and minimizing classification errors:\n",
        "\n",
        "- **High C**: Prioritizes minimizing classification errors, leading to a smaller margin and potentially overfitting.\n",
        "- **Low C**: Allows for a larger margin but may accept more misclassifications, leading to better generalization."
      ],
      "metadata": {
        "id": "Fgyp9qcKQFf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "The **Gamma** parameter in RBF Kernel SVM controls the influence of a single training point:\n",
        "\n",
        "- **High Gamma**: Each point has a smaller influence, leading to a more complex, overfitted model with a highly flexible decision boundary.\n",
        "- **Low Gamma**: Each point has a broader influence, resulting in a smoother, simpler decision boundary and better generalization."
      ],
      "metadata": {
        "id": "uR-poxfZQLvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"4\n",
        "The **Naïve Bayes classifier** is a probabilistic machine learning model based on **Bayes' Theorem**, which assumes that the features are independent given the class. It calculates the probability of each class given the features and assigns the class with the highest probability.\n",
        "\n",
        "It's called **\"Naïve\"** because of the strong (and often unrealistic) assumption that all features are independent, which may not hold true in real-world data. Despite this assumption, it often performs surprisingly well."
      ],
      "metadata": {
        "id": "g8XZybOkQVjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes’ Theorem\n",
        "Bayes' Theorem is a mathematical formula used to update the probability of a hypothesis based on new evidence. It is defined as:"
      ],
      "metadata": {
        "id": "FeOaylmuQfCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P(A|B) = (P(B|A) * P(A)) / P(B)\n"
      ],
      "metadata": {
        "id": "JeiI9_nHQxkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "\n",
        "P(A|B): The probability of hypothesis A given the evidence B (posterior probability).\n",
        "\n",
        "P(B|A): The probability of observing the evidence B given that hypothesis A is true (likelihood).\n",
        "\n",
        "P(A): The prior probability of hypothesis A (before observing B).\n",
        "\n",
        "P(B): The probability of evidence B.\n",
        "\n",
        "In Naïve Bayes, this is applied to classify data by calculating the posterior probability for each class based on feature probabilities."
      ],
      "metadata": {
        "id": "2Qf8FKsqQzrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "Here’s a brief explanation of the differences between the three types of Naïve Bayes classifiers:\n",
        "\n",
        "1. **Gaussian Naïve Bayes**:\n",
        "   - Assumes that the features follow a **Gaussian (Normal) distribution**.\n",
        "   - Used for **continuous** data where the features are real-valued.\n",
        "   - Each feature's probability is modeled using the **mean and variance** of that feature.\n",
        "\n",
        "2. **Multinomial Naïve Bayes**:\n",
        "   - Assumes that the features are **discrete counts** (e.g., word counts in text classification).\n",
        "   - Used for **count-based data** like **document classification** (e.g., in text mining).\n",
        "   - Models the distribution of the features as **multinomial** (based on the number of times an event happens).\n",
        "\n",
        "3. **Bernoulli Naïve Bayes**:\n",
        "   - Assumes that the features are **binary** (e.g., presence or absence of a word in a document).\n",
        "   - Used for **binary/Boolean** data (0 or 1).\n",
        "   - Models the probability of a feature being present (1) or absent (0).\n",
        "\n",
        "In summary:\n",
        "- **Gaussian** is for continuous data (with normal distribution).\n",
        "- **Multinomial** is for count data (like word counts).\n",
        "- **Bernoulli** is for binary/Boolean data."
      ],
      "metadata": {
        "id": "8db6MuWkQ2he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Naïve Bayes over other variants\n",
        "Use **Gaussian Naïve Bayes** when:\n",
        "\n",
        "1. **Continuous Data**: Your features are continuous and assumed to follow a **normal distribution** (Gaussian distribution).\n",
        "2. **Real-Valued Features**: The data is real-valued (e.g., measurements like height, weight, temperature).\n",
        "3. **Data Distribution**: You believe that the features are approximately normally distributed for each class.\n",
        "4. **Performance**: You want a quick and simple model for classification with continuous features and are okay with assuming normality.\n",
        "\n",
        "Avoid it if the features are discrete (count data, binary data) or if their distribution doesn’t resemble a normal distribution."
      ],
      "metadata": {
        "id": "72u4UggtRKp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Naïve Bayes\n",
        "Key assumptions of **Naïve Bayes**:\n",
        "\n",
        "1. **Conditional Independence**: Features are assumed to be independent given the class label.\n",
        "2. **Feature Distribution**: Each feature follows a specific distribution (e.g., Gaussian, Multinomial, Bernoulli) depending on the variant.\n",
        "3. **Class-Conditional Independence**: The features contribute independently to the prediction, and their effect is solely based on their relationship with the class label."
      ],
      "metadata": {
        "id": "yIhAoGS9RRrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Naïve Bayes\n",
        "**Advantages of Naïve Bayes**:\n",
        "\n",
        "1. **Simple & Fast**: Easy to implement and computationally efficient.\n",
        "2. **Scalable**: Works well with large datasets.\n",
        "3. **Good for Text Classification**: Often used in spam detection, sentiment analysis, etc.\n",
        "4. **Handles Missing Data**: Can handle missing data through marginal probabilities.\n",
        "5. **Low Overfitting Risk**: Less prone to overfitting, especially with small data.\n",
        "\n",
        "**Disadvantages of Naïve Bayes**:\n",
        "\n",
        "1. **Independence Assumption**: The assumption of feature independence is often unrealistic.\n",
        "2. **Poor Performance with Correlated Features**: Struggles when features are highly correlated.\n",
        "3. **Requires Domain Knowledge**: The model can be inaccurate if the feature distribution assumptions (Gaussian, Multinomial) are not met."
      ],
      "metadata": {
        "id": "20cWb5VeR37s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2TCBAA-VOofG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Why is Naïve Bayes a good choice for text classification\n",
        "Naïve Bayes is a good choice for text classification because:\n",
        "\n",
        "1. **Independence Assumption**: It assumes that features (words) are conditionally independent, which is often a reasonable approximation in text data (e.g., words occurring independently in a document).\n",
        "2. **Efficient**: It is computationally efficient and works well with high-dimensional data, like text, where the number of features (words) can be large.\n",
        "3. **Fast Training and Prediction**: It requires less training time and can make predictions quickly, even with large datasets.\n",
        "4. **Performs Well with Small Data**: It handles smaller datasets effectively compared to more complex models.\n",
        "5. **Works Well with Probabilistic Outputs**: Text classification often benefits from probabilistic outputs, and Naïve Bayes naturally provides these, which can be useful in ranking or prioritizing classifications."
      ],
      "metadata": {
        "id": "SkNDSNTbSBMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Compare SVM and Naïve Bayes for classification tasks:\n",
        "Here’s a comparison between SVM and Naïve Bayes for classification tasks:\n",
        "\n",
        "1. **Assumptions**:\n",
        "   - **SVM**: Does not make any strong assumptions about data distribution. Focuses on maximizing the margin between classes.\n",
        "   - **Naïve Bayes**: Assumes features are conditionally independent given the class, which is often an oversimplification.\n",
        "\n",
        "2. **Model Complexity**:\n",
        "   - **SVM**: More complex, especially with non-linear kernels. Can handle high-dimensional data effectively but may require tuning (e.g., kernel, C, gamma).\n",
        "   - **Naïve Bayes**: Simple, fast, and easy to implement. Works well with less computational cost and requires fewer resources.\n",
        "\n",
        "3. **Handling Linearity**:\n",
        "   - **SVM**: Effective in both linear and non-linear classification tasks, especially with the use of kernels.\n",
        "   - **Naïve Bayes**: Best suited for problems where feature independence holds. Performs well with text classification tasks where words are often treated as independent.\n",
        "\n",
        "4. **Data Distribution**:\n",
        "   - **SVM**: Works well with small to medium-sized datasets and is more sensitive to outliers.\n",
        "   - **Naïve Bayes**: Performs well with large datasets, especially when features are categorical or probabilistic (e.g., text).\n",
        "\n",
        "5. **Interpretability**:\n",
        "   - **SVM**: Less interpretable, especially when using non-linear kernels.\n",
        "   - **Naïve Bayes**: Easier to interpret, as it provides probabilistic predictions for each class.\n",
        "\n",
        "6. **Performance**:\n",
        "   - **SVM**: Generally performs well on a wide range of classification tasks, especially when the classes are well-separated.\n",
        "   - **Naïve Bayes**: Performs well when the feature independence assumption holds. It can be less accurate when the features are correlated.\n",
        "\n",
        "7. **Training Time**:\n",
        "   - **SVM**: Slower training time, particularly for larger datasets or complex kernels.\n",
        "   - **Naïve Bayes**: Fast to train and requires less computational power.\n",
        "\n",
        "In summary, SVM is powerful for complex, high-dimensional problems, whereas Naïve Bayes is a faster and simpler choice, especially for text classification and problems with independent features."
      ],
      "metadata": {
        "id": "pxaZhuNaSKeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.How does Laplace Smoothing help in Naïve Bayes?\n",
        "Laplace smoothing helps in Naïve Bayes by handling zero probabilities. When a certain feature value doesn’t appear in the training data for a given class, the probability of that feature given the class becomes zero, which would result in a zero probability for the entire class.\n",
        "\n",
        "Laplace smoothing adds a small constant (usually 1) to all feature counts, ensuring that no probability is ever zero. This helps in avoiding the issue of zero probabilities, especially when dealing with rare or unseen events."
      ],
      "metadata": {
        "id": "S8bJwbtJSdS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:"
      ],
      "metadata": {
        "id": "KZslWVz3SrL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "Y-cTJXcsSsie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies"
      ],
      "metadata": {
        "id": "d79mJKLLS4_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifiers with Linear and RBF kernels\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "# Train the SVM classifiers\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f'Accuracy of Linear Kernel SVM: {accuracy_linear * 100:.2f}%')\n",
        "print(f'Accuracy of RBF Kernel SVM: {accuracy_rbf * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "R7ypQ4nVTAIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE)"
      ],
      "metadata": {
        "id": "e6oMaBmgTBEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a synthetic housing dataset (for demonstration purposes)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points with one feature (e.g., square footage)\n",
        "y = X**2 + np.random.normal(0, 2, (100, 1))  # Simulated target (price, for example)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the SVR model\n",
        "svr = SVR(kernel='rbf')\n",
        "\n",
        "# Train the model\n",
        "svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the result\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n"
      ],
      "metadata": {
        "id": "JOUHA30NTEeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "boundary:\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a synthetic 2D classification dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the SVM Classifier with Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "svm_poly.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Visualize the decision boundary\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict over the grid to create the decision boundary\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.75, cmap='coolwarm')\n",
        "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap='coolwarm', marker='o', edgecolor='k', s=50)\n",
        "plt.title('SVM Classifier with Polynomial Kernel')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "accuracy = svm_poly.score(X_test_scaled, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "aikWtpkHTKJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy"
      ],
      "metadata": {
        "id": "9kgsbY1CTVDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "pLeVlBIzTYHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset."
      ],
      "metadata": {
        "id": "ZSaswum2TbeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "\n",
        "# Extract features (text data) and labels\n",
        "X = newsgroups.data  # Text data\n",
        "y = newsgroups.target  # Labels\n",
        "\n",
        "# Convert the text data to a bag-of-words representation (features)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_vect = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Multinomial Naïve Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "01QrK601Te-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "boundaries visually"
      ],
      "metadata": {
        "id": "r6jHiIVLTir8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Using only the first two features for easier visualization\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features for better performance with SVM\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a list of C values to try\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "# Create a meshgrid for plotting decision boundaries\n",
        "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Plot decision boundaries for different C values\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, C_value in enumerate(C_values, 1):\n",
        "    # Train the SVM model with the current C value\n",
        "    model = SVC(C=C_value, kernel='linear_\n"
      ],
      "metadata": {
        "id": "rr-y11zOTpns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "binary features"
      ],
      "metadata": {
        "id": "BiB1rHyKTtus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Generating a synthetic binary dataset with binary features\n",
        "# Let's create a simple dataset where features are binary\n",
        "data = {\n",
        "    'Feature1': np.random.randint(0, 2, 100),\n",
        "    'Feature2': np.random.randint(0, 2, 100),\n",
        "    'Feature3': np.random.randint(0, 2, 100),\n",
        "    'Feature4': np.random.randint(0, 2, 100),\n",
        "    'Target': np.random.randint(0, 2, 100)\n",
        "}\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Bernoulli Naïve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target values on the test set\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy and other metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "print('Classification Report:')\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "cCFny6d7TyMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "unscaled data"
      ],
      "metadata": {
        "id": "35EKfeilUFrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the SVM on unscaled data\n",
        "svm_unscaled = SVC(kernel='linear')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy on unscaled data\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling (standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM on scaled data\n",
        "svm_scaled = SVC(kernel='linear')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy on scaled data\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f'Accuracy on unscaled data: {accuracy_unscaled}')\n",
        "print(f'Accuracy on scaled data: {accuracy_scaled}')\n"
      ],
      "metadata": {
        "id": "Whh7jidHUMn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
        "after Laplace Smoothing"
      ],
      "metadata": {
        "id": "JHAMTxeTUR9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the SVM on unscaled data\n",
        "svm_unscaled = SVC(kernel='linear')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy on unscaled data\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling (standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM on scaled data\n",
        "svm_scaled = SVC(kernel='linear')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy on scaled data\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f'Accuracy on unscaled data: {accuracy_unscaled}')\n",
        "print(f'Accuracy on scaled data: {accuracy_scaled}')\n"
      ],
      "metadata": {
        "id": "cXu_QcAlUUfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "gamma, kernel)"
      ],
      "metadata": {
        "id": "URBxYKpwUcL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with the SVM model and the parameter grid\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Predict on the test data using the best model\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the best model:\", accuracy\n"
      ],
      "metadata": {
        "id": "CX3RiaFwUfDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy"
      ],
      "metadata": {
        "id": "u1OjsSIwUlyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Generate an imbalanced dataset (binary classification)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compute class weights to handle imbalance\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
        "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
        "\n",
        "# Initialize the SVM model with class weights\n",
        "svm = SVC(class_weight=class_weight_dict)\n",
        "\n",
        "# Train the model\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy with class weighting:\", accuracy)\n",
        "\n",
        "# Train the model without class weighting for comparison\n",
        "svm_no_weight = SVC()\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set without class weighting\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy without class weighting\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(\"Accuracy without class weighting:\", accuracy_no_weight)\n"
      ],
      "metadata": {
        "id": "Au1eyOSFUpEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data"
      ],
      "metadata": {
        "id": "_V6w64bQUvGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Sample dataset (this is a small example, you can replace with actual dataset)\n",
        "data = {\n",
        "    'email': [\n",
        "        \"Congratulations! You've won a free vacation to Bahamas!\",\n",
        "        \"Dear user, your account has been compromised. Please reset your password immediately.\",\n",
        "        \"Get a 50% discount on our latest tech gadgets.\",\n",
        "        \"Dear friend, I miss you! Let's catch up soon.\",\n",
        "        \"You have been selected to receive a free iPhone. Act now!\",\n",
        "        \"Important: Your account is at risk. Immediate action required.\",\n",
        "        \"Your Amazon order has been shipped! Check it out.\"\n",
        "    ],\n",
        "    'label': ['spam', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham']  # spam or ham (non-spam)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df['email']\n",
        "y = df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text data into numerical data using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train the Naïve Bayes classifier (MultinomialNB for text classification)\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vect, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = nb_classifier.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print detailed classification report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Ulu6SwozU1em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "compare their accuracy"
      ],
      "metadata": {
        "id": "Q65eoTn7U65d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset (replace with a real dataset)\n",
        "data = {\n",
        "    'email': [\n",
        "        \"Congratulations! You've won a free vacation to Bahamas!\",\n",
        "        \"Dear user, your account has been compromised. Please reset your password immediately.\",\n",
        "        \"Get a 50% discount on our latest tech gadgets.\",\n",
        "        \"Dear friend, I miss you! Let's catch up soon.\",\n",
        "        \"You have been selected to receive a free iPhone. Act now!\",\n",
        "        \"Important: Your account is at risk. Immediate action required.\",\n",
        "        \"Your Amazon order has been shipped! Check it out.\"\n",
        "    ],\n",
        "    'label': ['spam', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham']  # spam or ham (non-spam)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "N7SJAxc0U-Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "results"
      ],
      "metadata": {
        "id": "1wUDzmLCVBg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Sample dataset (replace with real dataset)\n",
        "data = {\n",
        "    'email': [\n",
        "        \"Congratulations! You've won a free vacation to Bahamas!\",\n",
        "        \"Dear user, your account has been compromised. Please reset your password immediately.\",\n",
        "        \"Get a 50% discount on our latest tech gadgets.\",\n",
        "        \"Dear friend, I miss you! Let's catch up soon.\",\n",
        "        \"You have been selected to receive a free iPhone. Act now!\",\n",
        "        \"Important: Your account is at risk. Immediate action required.\",\n",
        "        \"Your Amazon order has been shipped! Check it out.\"\n",
        "    ],\n",
        "    'label': ['spam', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham']  # spam or ham (non-spam)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = df['email']\n",
        "y = df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text data into numerical data using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# --- Model 1: Naïve Bayes without Feature Selection ---\n",
        "\n",
        "# Initialize and train the Naïve Bayes classifier without feature selection\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vect, y_train)\n",
        "\n",
        "# Predict using Naïve Bayes classifier\n",
        "y_pred_nb = nb_classifier.predict(X_test_vect)\n",
        "\n",
        "# Calculate accuracy for model without feature selection\n",
        "accuracy_nb_no_fs = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "# --- Model 2: Naïve Bayes with Feature Selection ---\n",
        "\n",
        "# Apply SelectKBest with chi-square test to select top k features\n",
        "selector = SelectKBest(chi2, k=3)  # Select top 3 features based on Chi-Square test\n",
        "X_train_fs = selector.fit_transform(X_train_vect, y_train)\n",
        "X_test_fs = selector.transform(X_test_vect)\n",
        "\n",
        "# Initialize and train the Naïve Bayes classifier with feature selection\n",
        "nb_classifier_fs = MultinomialNB()\n",
        "nb_classifier_fs.fit(X_train_fs, y_train)\n",
        "\n",
        "# Predict using Naïve Bayes classifier with feature selection\n",
        "y_pred_nb_fs = nb_classifier_fs.predict(X_test_fs)\n",
        "\n",
        "# Calculate accuracy for model with feature selection\n",
        "accuracy_nb_fs = accuracy_score(y_test, y_pred_nb_fs)\n",
        "\n",
        "# Print results\n",
        "print(f'Accuracy without Feature Selection: {accuracy_nb_no_fs * 100:.2f}%')\n",
        "print(f'Accuracy with Feature Selection: {accuracy_nb_fs * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "aWmT0QBGVEic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "strategies on the Wine dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "Ex1JrpmhVUx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- One-vs-Rest Strategy ---\n",
        "# Train SVM Classifier with One-vs-Rest strategy\n",
        "svm_ovr = SVC(decision_function_shape='ovr', random_state=42)\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy for OvR strategy\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# --- One-vs-One Strategy ---\n",
        "# Train SVM Classifier with One-vs-One strategy\n",
        "svm_ovo = SVC(decision_function_shape='ovo', random_state=42)\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy for OvO strategy\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "# Print the results\n",
        "print(f'Accuracy using One-vs-Rest (OvR): {accuracy_ovr * 100:.2f}%')\n",
        "print(f'Accuracy using One-vs-One (OvO): {accuracy_ovo * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "qBxREkUlVXbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "Cancer dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "HGkVX6qhVcgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Linear Kernel ---\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# --- Polynomial Kernel ---\n",
        "svm_poly = SVC(kernel='poly', degree=3, random_state=42)  # degree=3 for Polynomial kernel\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "# --- RBF Kernel ---\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the results\n",
        "print(f'Accuracy using Linear Kernel: {accuracy_linear * 100:.2f}%')\n",
        "print(f'Accuracy using Polynomial Kernel: {accuracy_poly * 100:.2f}%')\n",
        "print(f'Accuracy using RBF Kernel: {accuracy_rbf * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "EHrhFCfhVfiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "average accuracy"
      ],
      "metadata": {
        "id": "Z1tJuk0QVjVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize SVM Classifier\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    # Split the data into training and testing sets for each fold\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = svm.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Compute average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f'Average Accuracy using Stratified K-Fold Cross-Validation: {average_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "OfHKwg6aVmCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "performance"
      ],
      "metadata": {
        "id": "O7-KX_t6VqMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different class priors\n",
        "priors_list = [\n",
        "    [0.3, 0.4, 0.3],  # Custom prior probabilities\n",
        "    [0.5, 0.2, 0.3],  # Another set of custom priors\n",
        "    [1/3, 1/3, 1/3]   # Uniform priors (equal probabilities)\n",
        "]\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate Naive Bayes with different priors\n",
        "for priors in priors_list:\n",
        "    # Initialize Gaussian Naive Bayes with custom priors\n",
        "    nb = GaussianNB(priors=priors)\n",
        "\n",
        "    # Train the model\n",
        "    nb.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = nb.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Print accuracy for each set of priors\n",
        "for i, accuracy in enumerate(accuracies):\n",
        "    print(f\"Accuracy with priors {priors_list[i]}: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "v8k7w3slVtD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "compare accuracy"
      ],
      "metadata": {
        "id": "Yg_WV93MVy_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM classifier without feature selection\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_no_rfe = svm.predict(X_test)\n",
        "accuracy_no_rfe = accuracy_score(y_test, y_pred_no_rfe)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE)\n",
        "rfe = RFE(estimator=SVC(kernel='linear'), n_features_to_select=2)  # Select top 2 features\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train SVM classifier with RFE-selected features\n",
        "svm_rfe = SVC(kernel='linear')\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy without RFE: {accuracy_no_rfe\n"
      ],
      "metadata": {
        "id": "YxnkKMpkV1FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "F1-Score instead of accuracy"
      ],
      "metadata": {
        "id": "y_9hwa2YV41c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "xh4MeWv0V7Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "(Cross-Entropy Loss)"
      ],
      "metadata": {
        "id": "I3EeAnv_V-2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions (probabilities) on the test set\n",
        "y_pred_proba = nb.predict_proba(X_test)\n",
        "\n",
        "# Binarize the labels (for multi-class classification)\n",
        "lb = LabelBinarizer()\n",
        "y_test_bin = lb.fit_transform(y_test)\n",
        "\n",
        "# Calculate Log Loss (Cross-Entropy Loss)\n",
        "logloss = log_loss(y_test_bin, y_pred_proba)\n",
        "\n",
        "# Print the Log Loss\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {logloss:.4f}\")\n"
      ],
      "metadata": {
        "id": "DGa8ntWYWCRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn"
      ],
      "metadata": {
        "id": "Md0kEEItWFqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "umR8x4VXWIDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "Error (MAE) instead of MSE"
      ],
      "metadata": {
        "id": "qvgi5yqHWMms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Boston housing dataset\n",
        "boston = datasets.load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVR model\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the result\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n"
      ],
      "metadata": {
        "id": "3B5p8WeJWXPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "score"
      ],
      "metadata": {
        "id": "3Sg4n107WX2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC evaluation\n",
        "y_pred_prob = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f'ROC-AUC Score: {roc_auc}')\n"
      ],
      "metadata": {
        "id": "qVHNlQtCWahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46.  Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve"
      ],
      "metadata": {
        "id": "qaReHAXnWgL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm = SVC(kernel='linear', probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for Precision-Recall curve\n",
        "y_pred_prob = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision and Recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Compute Average Precision (AP) score\n",
        "average_precision = average_precision_score(y_test, y_pred_prob)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='b', label=f'Precision-Recall curve (AP = {average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NugznS4CWnG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}